var tipuesearch = {"pages":[{"title":"About","text":"#About me Hello! I'm Stevie, a Belgian PhD student enrolled at the Catholic University of Leuven. Through this page I will briefly introduce myself as well as summarize the intent of my blog. Like many of my age demographic, online communication and digital media played an important role in my upbringing. As a logical extension of my interest in the technical aspects of the internet and computing cultivated while growing up, I enrolled in an undergraduate program in Applied Informatics 1 in 2011, graduating with a BA in 2014. Having reached the end of those studies and becoming more interested in matters of politics and society, however, I felt an increasing desire to develop and express myself beyond those technical skills. Admittedly under the internet-facilitated influence of Japanese pop-culture, I next enrolled in the Japanese Studies program at the Catholic University of Leuven, graduating with a BA in 2017 and MA in 2019. During that period, I received the wonderful opportunity to spend three academic semesters in Japan as a MEXT-funded research student of the department of Socio-Informatics at Chuo University. No doubt a life-changing experience, my time at Chuo University also served as an important synthesis of both my former, technical interests and the interests I had developed in media, politics and cultural sociology during my time at KU Leuven. Entering 2020, I began my PhD studies at KU Leuven, working on the topics of grassroots political usage of social media in Japan and political propaganda in subcultural online spaces. On a personal note, I am fairly passionate about music. I have spent the better part of my youth taking up music lessons or playing in bands, and although I had to make some sacrifices in order to pursue my academic ambitions, I enjoy spending free time on composing and playing piano. #About this blog This blog, Onoreto, 2 serves simultaneously as a kind of personal portfolio and as a creative outlet. That aside, I hope I am also able to provide some assistance to my fellow students (or anyone interested) regarding the technical aspects of learning Japanese or working academically on the topic of contemporary Japan. Technically, his blog was built as a static site with a highly customized template using the lightweight, python-based Pelican framework , and is hosted on GitHub's free web hosting service GitHub Pages . Due to its ease and sustainability, I have for the past half decade written most of my writings (including graduate and undergraduate thesis, term papers and course notes) in the markdown syntax, and continue doing so with Pelican.","tags":"pages","url":"https://steviepoppe.net/about/","loc":"https://steviepoppe.net/about/"},{"title":"Resources","text":"This page contains links and files that have proven to be particularly useful for myself and might be of use for those with similar interests. I'll update this spot as I encounter more tools or pages I feel are worthy of mention. Useful Links General Anki : Free, open-source, multi-platform flashcard application with powerful SRS 1 algorithm. Especially vital for language acquisition and probably the single most important (digital) tool for efficient memorization. Language Japanese Yomichan : a Firefox and Chrome pop-up dictionary and a digital-minded Japanese learner's best friend. Yomichan allows additional audio playback and full Anki integration. I've written a blog on how to do this. Rikaisama , although no longer by Firefox, also continues to be useful for mass-generating cards while reading HTML-formatted e-books. Animelon : for all you ‘learn Japanese through Anime' kids out there! Reminiscent of the classic Erin's Challenge videos: watch various popular anime with Japanese subtitles and integrated dictionary. Works in combination with Yomichan or Rikaisama as well. Tangorin : my favorite online Japanese dictionary with tons of example sentences for each definition, and an excellent Kanji dictionary as well. Goo , Weblio and Sanseido provide good alternatives. Waran Jiten : a Japanese-Dutch dictionary courtesy of Leuven University. Has a dutch Rikai addon. Sourceforge : contains a wide variety of software developed by fellow Japanese learners. Some, such as Jnovelformatter , Japanese Text Analysis Tool or OCR Manga Reader are particularly noteworthy, others not so much. KULeuven Japanology : the Leuven University Japan Studies homepage. Korean Toktogi : a Korean-English pop-up dictionary similar to yomichan, available both for Firefox and Chrome. It's still a work-in-progress and not as polished as Yomichan, but very usable nonetheless. Naver Dictionary : Naver Corporation, the highly popular South Korean web-hub, has a fairly complete English-Korean dictionary as well as an Android application (requires internet). Sanskrit spokensanskrit : the only decent online Sanskrit dictionary out there. Lexilogos : multilingual web keyboard that beats learning to type devanagari on a latin-script keyboard. Files Anki I use Anki to complement my other study methods for nearly all my classes. These are a bunch of Anki-sets I made I feel are stand-alone enough to be useful to new students of these classes as well. Inleiding tot de Europese Literatuur na 1750 Download : Europese Literatuur (±26MB) A full set to accompany the textbook ‘ Literaire Verbeelding 2 ': 2 contains important dates and summaries of all artists and their discussed works (in Dutch), sorted by period (from Romanticism till Post-modernism). Kanji 1 & Kanji 2 Download : Kanji 1 Kanji Vocab 1 (±9MB each) Two sets on the compulsory Kanji and vocabulary as seen in the Kanji classes in the first year of Japan Studies at KU Leuven. They contain stroke order, definitions, and different readings, both in recognition and reproduction form. They use the KanjiStrokeOrders 3 font and display hidden hiragana on touch/hover. I recommend doing these on a hand-held device to practice writing and stroke order using the touchscreen. Download : Kanji 2 Kanji Vocab 2 (±10MB each) Similar to the previous decks, two sets on the compulsory kanji and vocabulary for Kanji/goi classes in the second year of Japanese Studies, as seen in our handbook 4 lessons 1 - 20. They're tagged by chapter and contain respectively stroke order, radicals, definitions, and different readings for the Kanji set, and example sentences in cloze deletion 5 for vocab, both in recognition and reproduction form. Japanese Geography Download : Japanese Geography (±10MB) A modified Prefectures in Japan set, includes all Japanese regions and prefectures written in Kanji with furigana , and some extra maps. Sanskrit Download : Europese Literatuur (±5MB) A full set to accompany the introductory Sanskrit language course. Includes an introduction to the Devanagari alphabet, a recap of the taught grammar points (such as different conjugations and Sandi rules), three-way cards (English/Sanskrit, Sanskrit/English and audio) of all the vocabulary as seen in class and several exercises. Anki Plug-in: Japanese Definitions for Korean Vocabulary Download : Japanese Definitions for Korean Vocabulary An Anki add-on, based on the Sanseido Definitions add-on, for adding Japanese translations of Korean vocabulary. It's primarily meant for Korean learners who're proficient in, or simultaneously learning, Japanese. I wrote a small tutorial for those new to Anki plug-ins. Anki Plug-in: Remove Missing Audio References Download : Remove Missing Audio References An Anki plug-in to be used in combination with a Rikaisama / Anki set-up as prescribed in this blog . Replaces or removes references to missing audio-files. Spaced Repetition System : a method of studying optimally by repeating items over gradually increasing periods. Cards are calculated to appear shortly before the item slips one's mind. ↩ Ghesquiere, R. 2006 . Literaire verbeelding 2: een geschiedenis van de Europese literatuur en cultuur vanaf 1750. Leuven: Acco ↩ Available for free under a BSD style license at http://www.nihilist.org.uk/ . ↩ Japanese for International/Graduate Students vol.5: Kanji and Vocabulary （稲村真理子. 2007. 大学・大学院留学生の日本語. 5(漢字・語彙編) 5(漢字・語彙編). 東京: アルク） ↩ cloze deletion test : an exercise in which one fills in portions of the text left blank, based on the context of the text. ↩","tags":"pages","url":"https://steviepoppe.net/resources/","loc":"https://steviepoppe.net/resources/"},{"title":"A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 4: Natural Language Processing With MeCab, Neologd and NLTK","text":"This short series of blogs chronicles the bare-bones required to conduct a basic form of social media analysis on corpora of (Japanese) Tweets. It is primarily intended for undergraduate and graduate students whose topics of research include contemporary Japan or its online vox populi, and want to strengthen their existing research (such as an undergraduate thesis or term paper) with a social media-based quantitative angle. This fourth blog follows up on the MeCab + NEologd set-up described in the third part of this series, and introduces the reader to some of possibilities when working with python-based NLP tools such as NLTK. Concretely, we will: Import the python wrapper mecab-python, necessary to use MeCab in our python scripts, Learn about NLP tools like NLTK, Perform a basic sentiment analysis on a corpus of Japanese tweets. Set-up MeCab python binding Having set-up MeCab, we now require a python binding in order to use MeCab in our python scripts. As usual, install the required python library using pip in the command prompt: pip install mecab . 2 Upon installation, let's test our set-up by using the python command in the command prompt and copy-pasting the code examples below. Example 1: 1 2 3 4 5 import MeCab tagger = MeCab . Tagger () parsed = tagger . parse ( \"お母さんが作ってくれた魚フライ定食を食べてみたいです。\" ) print ( parsed ) The MeCab.Tagger function accepts several arguments we can pass to MeCab: -d : path to one or more tokenizer dictionaries. -E : Which escape sequence to use at the end of the parsed string (tabs: \\t, backspaces: \\b, newlines: \\n, etc; don't forget to escape the backslash with an additional backslash). -O : Quick-formatting options. Wakati (short for wakachi-gaki , 分かち書き), for example, is a tokenizer option for returning a string with only the surface form of each token, separated by spaces. 3 -F : Grants us more control over the output formatting. Adding -F%m:\\\\t\\\\t%f[0]\\\\n , for example, will format the outcome as seen in figure 4 : the token surface form and the first element of the ‘ features ‘ array (part-of-speech), separated by tab spaces. –unk-feature : set the return value of the part-of-speech column for unknown words (e.g. to unknown or to 未知語). Example 2: 1 2 3 4 5 import MeCab tagger = MeCab . Tagger ( \"-F%m: \\\\ t %f [0] \\\\ n -E \\\\ n --unk-feature 未知語\" ) text = \"うわー！母が作ってくれた魚フライ定食は素敵だったな～！？ウェーーーーイイイイイイイイィ！\" parsed = tagger . parse ( text ) print ( parsed ) Normalizing Japanese text with Neologdn (optional) This small library consists of several regular expressions helpful in normalizing common tendencies of Japanese text on social media, such as converting half-width characters to full-width and removing dramatizing hyphens. As always, install with pip : pip install neologdn , and try out the script below as example or read the documentation for further usage. 1 2 3 4 5 6 7 import MeCab import neologdn tagger = MeCab . Tagger ( \"-O wakati\" ) text = \" ﾊﾝｶｸｶﾅっ ウェーーーーイイイイイイイイィっ！？\" text = neologdn . normalize ( text , repeat = 2 ) parsed = tagger . parse ( text ) print ( parsed ) Using MeCab with NLTK Having set up the above, we can now further integrate this into one of the python NLP libraries. With its focus on speed and efficiency, SpaCy is popular for real-life applications while NLTK (Natural Language Toolkit) remains a popular for experimentation among students and researchers. The choice for NLTK was fairly arbitrary and the examples below are easily accomplished in both libraries. To install NLTK, use pip from the command line: pip install NLTK . Frequency distribution The script below combines the steps we have taken thus far with the common NLP practice of calculating and analyzing the frequency distribution of texts, something we have done using KH Coder in the previous article. For visualization, we will be using the external Python library matplotlib . As always, we will have to install such libraries with pip : pip install matplotlib . Running the script below will generate a graph with the 10 most frequent lemmas in the text provided, as seen in figure 1 . Figure 1: Frequency distribution (filtered). Figure 2: Frequency distribution (unfiltered). The script below takes several further steps to removing noise by filtering tokens based on their POS-tags (thus excluding particles, conjugations in the form of auxiliary verbs, pronouns, pre- and suffixes, symbols, exclamations, etc). Moreover, the script below uses, if available, the dictionary form of segmented words. Figure 2 is an illustration of the outcome when that step is not taken. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # coding=utf-8 import MeCab import neologdn #Using tabs to separate lemmas and then tokenizing by splitting on tabs retains proper nouns separated by half-width spaces tagger = MeCab . Tagger ( \"-F%m \\\\ t --unk-feature 未知語\" ) from nltk.probability import FreqDist from matplotlib import rcParams rcParams [ 'font.family' ] = 'sans-serif' rcParams [ 'font.sans-serif' ] = [ 'Hiragino Maru Gothic Pro' , 'Yu Gothic' , 'Meirio' , 'Takao' , 'IPAexGothic' , 'IPAPGothic' , 'VL PGothic' , 'Noto Sans CJK JP' ] CONTENT_WORD_POS = ( \"名詞\" , \"動詞\" , \"形容詞\" , \"副詞\" , \"未知語\" ) IGNORE = ( \"接尾\" , \"非自立\" , \"代名詞\" ) def is_content_word ( feature ): return feature . startswith ( CONTENT_WORD_POS ) and all ( f not in IGNORE for f in feature . split ( \",\" )[: 6 ]) if __name__ == '__main__' : text = \"１９２３年の関東大震災が起こった時、ベルギー人がためらわず、救援活動を行ったり資金集めをしたりしたのは、この友好的な日本との関係のためだと言えます。王族と、新たに設立されたベルギー国内委員会が協力して大規模な救援活動を実施しました。私の故郷アントワープを含むベルギーの全国各地でも教会と戦争の退役軍人が協力して「Japan Day」という催しを行いました。他にも、ベルギー人の芸術家達が作品を集めたり新作を発表したりして特別な貢献をしました。まずはブリュッセルで、次に日本で作品の展示発売や展覧会を開催し、売り上げや入場料の利益は全て救援活動のために寄付されました。日本の展覧会だけでも3万５千人もの観客が訪れたと報告されていて、その上、日本の皇太子様と皇后様が30点もの芸術品をご購入されたと聞いております。\" text = neologdn . normalize ( text , repeat = 2 ) #some more noise removal text = '' . join ([ i for i in text if i . isalpha () or i . isspace ()]) result = tagger . parseToNode ( text ) content_words = [] while result : if is_content_word ( result . feature ): lemma = result . feature . split ( \",\" )[ 6 ] if len ( result . feature . split ( \",\" )) > 6 and result . feature . split ( \",\" )[ 6 ] is not \"*\" else result . surface content_words . append (( lemma )) result = result . next fdist = FreqDist ( content_words ) fdist . plot ( 10 , cumulative = False ) Note When dealing with Japanese text directly within a python environment, a declaration of text-encoding is required as first or second line (as seen in line 1 ). Furthermore, Lines 10 & 11 are required to display Japanese text in the generated mathlib graph. Note 2 The text example above uses a paragraph from a Japanese speech I wrote several years ago, published on this blog. It refers to a particular historical event, \"Japan Day\", which is clearly split into two distinct lemmas. Furthermore, and although there are specific rules for that, ベルギー 人 ( じん ) and 芸術家 ( げいじゅつか ) are both split in two lemmas while 芸術品 ( げいじゅつひん ) is taken as one lemma. To make matters worse, 人 ( じん ) and 家 ( か ) are correctly seen as suffixes in this context and thus removed from our bag of words based on our filtering. In other words, if we want more coarse-grained results, manual normalization is required. Stop-words (optional) Stop-words refers to the most commonly used words in natural language; words we might want to filter out of our corpora depending on the context. NLTK has stop-word functionality and comes with lists of stop-words for 16 different languages. Those do not include Japanese, however, and we will have to add that manually. An NLTK stop-word list is merely a text-file of words separated by newlines and stored in nltk_data\\corpora\\stopwords by their language (without the .txt extension). 4 For now I recommend this one (the same one we have used in the previous guide). Save this in the NLTK stopwords folder as ‘Japanese', without .txt extension. figure 3 displays the result of running an edited python script filtering out stop-words. Figure 3: Frequency distribution (filtered with stop-word list). Figure 4: Comparison of English and Japanese stop-word lists. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # coding=utf-8 import MeCab import neologdn tagger = MeCab . Tagger ( \"-F%m \\\\ t --unk-feature 未知語\" ) from nltk.corpus import stopwords from nltk.probability import FreqDist from matplotlib import rcParams rcParams [ 'font.family' ] = 'sans-serif' rcParams [ 'font.sans-serif' ] = [ 'Hiragino Maru Gothic Pro' , 'Yu Gothic' , 'Meirio' , 'Takao' , 'IPAexGothic' , 'IPAPGothic' , 'VL PGothic' , 'Noto Sans CJK JP' ] CONTENT_WORD_POS = ( \"名詞\" , \"動詞\" , \"形容詞\" , \"副詞\" , \"未知語\" ) IGNORE = ( \"接尾\" , \"非自立\" , \"代名詞\" ) def is_content_word ( feature ): return feature . startswith ( CONTENT_WORD_POS ) and all ( f not in IGNORE for f in feature . split ( \",\" )[: 6 ]) if __name__ == '__main__' : text = \"１９２３年の関東大震災が起こった時、ベルギー人がためらわず、救援活動を行ったり資金集めをしたりしたのは、この友好的な日本との関係のためだと言えます。王族と、新たに設立されたベルギー国内委員会が協力して大規模な救援活動を実施しました。私の故郷アントワープを含むベルギーの全国各地でも教会と戦争の退役軍人が協力して「Japan Day」という催しを行いました。他にも、ベルギー人の芸術家達が作品を集めたり新作を発表したりして特別な貢献をしました。まずはブリュッセルで、次に日本で作品の展示発売や展覧会を開催し、売り上げや入場料の利益は全て救援活動のために寄付されました。日本の展覧会だけでも3万５千人もの観客が訪れたと報告されていて、その上、日本の皇太子様と皇后様が30点もの芸術品をご購入されたと聞いております。\" text = neologdn . normalize ( text , repeat = 2 ) text = '' . join ([ i for i in text if i . isalpha () or i . isspace ()]) result = tagger . parseToNode ( text ) stop_words = stopwords . words ( 'japanese' ) content_words = [] while result : if is_content_word ( result . feature ): lemma = result . feature . split ( \",\" )[ 6 ] if len ( result . feature . split ( \",\" )) > 6 and result . feature . split ( \",\" )[ 6 ] != \"*\" else result . surface if lemma not in stop_words : content_words . append (( lemma )) result = result . next fdist = FreqDist ( content_words ) fdist . plot ( 10 , cumulative = False ) Sentiment analysis [Coming soon: Sentiment analysis] Bringing it all together: processing tweets [Let's apply this on a real-world example] Wait! There is more! The next guide in this series will expand on the methods outlined in the second guide, paying particular attention to the most common actors related to a certain keyword as well as their network. A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 1: Twitter Data Collection A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 2: Basic Metrics & Graphs A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 3: Natural Language Processing With MeCab, Neologd and KH Coder A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 5: Advanced Metrics & Graphs On a final note, it is my aim to write tutorials like these in such a way that they provide enough detail and (technical) information on the applied methodology to be useful in extended contexts, while still being accessible to less IT-savvy students. If anything is unclear, however, please do not hesitate to leave questions in the comment section below. Still image from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under a Fair Use doctrine. ↩ For more information on the MeCab python library, see the pypi project page or the developer's page (Japanese). ↩ The other options are \"-O chasen\" (to display the POS tagged tokens in ChaSen format), \"-O yomi\" (for displaying the reading of the token) and -\"O dump\" (the default options, dumps all information). ↩ When unsure what the NLTK path is, simply run nltk.download() in python and the NLTK download manager will pop up, for me it was \"C:\\Users\\stevie\\AppData\\Roaming\\nltk_data\\corpora\\stopwords\". ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2020/07/a-quick-guide-to-data-mining-textual-analysis-of-japanese-twitter-part-4/","loc":"https://steviepoppe.net/blog/2020/07/a-quick-guide-to-data-mining-textual-analysis-of-japanese-twitter-part-4/"},{"title":"A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 3: Natural Language Processing With MeCab, Neologd and KH Coder","text":"This short series of blogs chronicles the bare-bones required to conduct a basic form of social media analysis on corpora of (Japanese) Tweets. It is primarily intended for undergraduate and graduate students whose topics of research include contemporary Japan or its online vox populi, and want to strengthen their existing research (such as an undergraduate thesis or term paper) with a social media-based quantitative angle. The purpose of this third blog and the next, fourth blog is to introduce the reader to the concept of natural language processing (NLP), to the techniques available for processing Japanese texts, and to some basic forms of quantitative content analysis that could be performed with that processed data. Concretely, in this third blog we will: Learn about natural language processing in a Japanese context, Set up the morphological analyzer MeCab and the neologism dictionary mecab-ipadic-NEologd , Install and set-up the quantitative content analysis tool KH Coder, Perform a rudimentary content analysis on a corpus of tweets collected with methods described in the previous two blogs. Although this blog thus assumes that the reader has read part one and two of this series, it should also serve as a solid stand-alone introduction to setting up MeCab and the IPADic NEologd dictionary (as of writing still pretty much the de-facto canonical tools in academic scholarship applying a form of Japanese computational linguistics) and combining them with KH Coder. Due to the nature of the field, parts of this tutorial will be rather technical and rely on having some basic experience working with the Windows command prompt or MacOS Terminal, but despite the complexity of the matter, there shouldn't be much of a technical difficulty gap compared to the previous tutorials. 2 Japanese Natural Language Processing ‘Natural language' refers to the usage of language that has developed naturally over time among humans, encompassing methods of communication such as speech, written text and signaling. Natural Language Processing, then, is the field of developing how computers can understand and process such use of language in a cognitive way. Applications of NLP are seen anywhere ranging from AI assistants (such as Siri, Alexa or Google Assistant), to automatic translation (e.g. Google Translate), targeted advertising, and even in health-care. 3 An important preliminary step in processing natural language computationally is to tokenize of language (also referred to as word segmentation ). In other words, to break down language into smaller segments (tokens), with morphemes being the smallest unit of language having grammatical meaning that can be broken down to. The following step in the NLP pipeline is then to identify the grammatical and semantic meaning of each morpheme within the context of the text it belongs to; a process called part-of-speech tagging (POS tagging). This is easier to do when handling languages with clear cut semantic borders such as punctuation and space boundaries, but more challenging when dealing with ambiguous, agglutinative and non-segmented languages (such as Japanese). Due to the nature of the Japanese language, those two steps commonly go hand in hand under the header of morphological analysis and are done at the hand of so-called dictionaries; corpora of texts with a part of speech tag-set processed by hand and/or through machine learning methods. There is some debate to the development of those dictionaries as well, depending on how coarse or fine-grained tokenization tags should be (i.e. should sakanafuraiteishoku 魚フライ定食 be seen as one token, as a combination of sakanafurai 魚フライ and teishoku 定食, or as sakana 魚, furai フライ and teishoku 定食?), as well as what rules are applied, and which sources should be used for new dictionary entries. 4 Depending on the selected dictionary, different Japanese morphological analysis tools will then apply different POS tagging techniques 5 – probabilistic methods being the most common 6 – in calculating boundaries and contextual tags of a language sequence. Having tokenized and POS-tagged texts (which can be further preprocessed with methods such as stop-word removal), we can now employ a variety of computer-assisted quantitative content and text analysis techniques suitable for social media, ranging from a very rudimentary keyword frequency analysis, to topic modeling based on coding tables, sentiment analysis and narrative network theory analysis (which will be demonstrated over the next few articles). Although there are several NLP frameworks available that offer limited support for the Japanese language (such as NLTK and SpaCy), it is crucial that the initial process of morphological analysis is tailored to our needs. This blog article will therefore expand on a set-up adequate for tackling social media content such as tweets. Japanese morphological analyzing Despite being discontinued for almost a decade, and although rapid developments have being made in Japanese NLP over the past few years, the morphological analyzer MeCab remains as of writing the fastest, most frequently used Japanese tokenizer | POS-tagger, and the only option we can use in combination with KH Coder. There are, however, several other highly promising Japanese NLP options in active 7 development that should not be neglected (and for which I will write a more complete comparison of in the near future): Janome is a pure Python MeCab alternative with a decent post-processing analyzer framework and English language documentation . 8 Ginza is an all-encompassing Japanese NLP library based on the NLP library spaCy , the Universal Dependencies cross-linguistic annotation convention, and the Japanese morphological analyzer SudachiPy (the latter, a python flavor of Sudachi , being the actual alternative to MeCab). 9 Juman++ , unlike MeCab and its predecessors, relies on recurrent neural networks ( RNN ) – a deep learning method – for its POS tagging rather than on token dictionaries. While much slower than Mecab + NEologd, it appears to have incredible promise in dealing with colloquial language and spelling inconsistencies, which will be particularly useful for processing social media content. MeCab Installation There are several options for installing MeCab, based on our set-up: Unix | Windows x86 : the official installers for Unix and 32-bit versions of Windows. Windows 64-bit : An unofficial 64-bit installer (select mecab-64-0.996.2.exe ). 10 Mac (JP): Haven't tested this myself but installation seems fairly straightforward using brew. Make sure to select UTF-8 as encoding option when installing MeCab; UTF-8 is required to guarantee compatibility with Python and the optional custom dictionary mecab-ipadic-NEologd . 11 Upon completion, we should also add two references to our global system variables. On Windows devices, open System Utility in the control panel and click system variables : First we should add a PATH reference to the folder containing the MeCab executables: select Path → edit → new → add the path to that folder (e.g. C:\\Program Files\\MeCab\\bin). Next, we should add a global variable MECABRC, with a value pointing to (our mecab folder)\\etc\\mecabrc (e.g. to C:\\Program Files\\MeCab\\etc\\mecabrc ). The Mecabrc file within contains configuration data for MeCab, including the path to the dictionary we will using. Figure 1: Example of MeCab + IPADIC POS-tagging of Japanese text Figure 2: Example of encoding issues As seen in Figure 1 , MeCab probes the text-input at the hand of the selected dictionary (in this case the default IPADIC dictionary) in order to segment the Japanese text and return the grammatical and semantic meaning of each morpheme. The dictionary files ( sys.dic and unk.dic ) were pre-compiled based on a list of CSV files containing that grammatical and semantic information for most common morphemes. A closer look at any of those CSV files reveals a data structure according to the following column scheme: Surface form (表層形), left context ID (左文脈ID), right context ID (右文脈ID), cost (コスト), part-of-speech (品詞), part-of-speech sub-classification 1 (品詞細分類1), part-of-speech sub-classification 2 (品詞細分類2), part-of-speech sub-classification 3 (品詞細分類3), conjugation type (活用型), inflectional form (活用形), original form (原形), reading (読み), pronunciation (発音) Take, for example, the entry for tabero 食 ( た ) べろ in Verb.csv or the entry for the grammatical particle ha は: 食べろ,623,623,7175,動詞,自立,,,一段,命令ｒｏ,食べる,タベロ,タベロ は,14,,助詞,係助詞,,,,,は,ハ,ワ The cost column indicates how likely the word is to occur (with a smaller number indicating a higher likelihood to occur). The context IDs are internal references to categories defined in the *.def files (specifically left-id.def and right-id.def ) and classify the meaning of the morpheme within the text content seen either from the left or right (in this case, those IDs both refer to (動詞,自立,*,*,一段,命令ｒｏ,*). The other categories are self-explanatory: 食べろ, read and pronounced as タベロ, 13 is the commanding ro conjugation of the dictionary form 食べる, an independent ichidan verb. Note After installing MeCab with the UTF-8 option, the MeCab default dictionary will have been recompiled with UTF-8 character encoding. Usually, running MeCab with the mecab.exe executable in (in /bin/) or by using the command ( mecab ) in the command prompt would allow usage as seen in figure 1 . Depending on the character encoding of the console prompt (can be tested by using chcp ), running MeCab as-is could result in encoding issues (see Figure 2 ), however. Having the Region Settings of the system locale set to Japanese, for example, would change the encoding (the code page) for console applications to code page 932 (e.g. Shift JIS). Fortunately for us, there is hardly any use to running MeCab as-is; we'll be running MeCab either with KH Coder or through Python scripts (which handles encoding for us). The above problem is therefore not of high importance and can nevertheless be by-passed with some simple workarounds. 13 Finally, it should be noted that MeCab accepts several arguments in dealing with unknown words or for formatting output. This will be more relevant for the next part of this tutorial series, but for now, see figure 4 for a demonstration: -d : path to one or more tokenizer dictionaries. -E : Which escape sequence to use at the end of the parsed string (tabs: \\t, backspaces: \\b, newlines: \\n, etc. -O : Quick-formatting options. Wakati (short for wakachi-gaki , 分かち書き), for example, is a tokenizer option for returning a string with only the surface form of each token, separated by spaces. 14 -F : Grants us more control over the output formatting. Adding -F%m:\\\\t\\\\t%f[0]\\\\n , for example, will format the outcome as seen in figure 4: the token surface form and the first element of the ‘ features ‘ array (part-of-speech), separated by tab spaces. --unk-feature : set the return value of the part-of-speech column for unknown words (e.g. to ‘ unknown ‘ or to michigo ‘未知語'). Can be alternated with -x (e.g. -x 'unknown' ). mecab-ipadic-NEologd installation (optional) POS-tagging Japanese text on social media is particularly difficult due to the volatile nature of language usage on social media, including usage of neologisms and colloquialisms, as well as Japanese-specific usage of half-width characters, different Unicode characters and text emoji ( kaomoji ). The standard dictionary provided with MeCab, IPADic, hasn't been maintained in over a decade and does not satisfy our needs. While optional, it is therefore highly recommended to install the neologism dictionary mecab-ipadic-NEologd , a MeCab dictionary expansion of the IPADic dictionary built using manual and machine learning methods on a wide variety of online texts. It contains a lot of entries of terms common on Japanese social media, as well as common names not yet belonging to the default MeCab dictionary (see figure 3 for a comparison when inputting prime minister Shinzo Abe's name using respectively the default IPADic dictionary and the NEologd expansion). Figure 3: Parsed results of 安倍晋三 (Abe Shinzō) with respectively default and NEologd dictionary. Figure 4: Parsed text with various tokenizer settings. Installation of the Neologism dictionary is rather obtuse; as it requires us to manually build the dictionary based on our existing copy of the IPADIC dictionary and the source files for the NEologd expansion. 15 This process is simple enough on Unix or Mac (as described on the project page), but less so on Windows. The following steps are, again, Windows-specific. The project-page for NEologd is hosted on GitHub and periodically updated. For those who have git installed, clone the project to a directory of choice with $ git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git . If not, select Clone or download → Download ZIP and extract to a directory of choice. As mentioned earlier, MeCab dictionaries are compiled based on a list of CSV files containing grammatical and semantic information for each morpheme. We will need to recompile that dictionary based on the NEologd CSV files. Those CSV files are highly compressed as .xz files listed in the seed directory of the NEologd project folder. We will have to extract * those (select all the files with xz extension and extract them using 7zip or another file archiver of choice). Next, back in the ‘MeCab/dic' folder make a copy of the ‘ipadic' folder. Name it ‘ipadic-neologd'. As-is, the IPADic source files (the CSV and def files) remain encoded in EUC-JP (or SHIFT-JIS for older copies). The NEologd CSV files, however, are encoded in UTF-8. Compiling those together will lead to inconsistencies; we should therefore change the character encoding of the IPADic source files to UTF-8 as well. Note This can be done manually with most advanced text editors, or in bulk with iconv (part of GNUWin32 ). 16 For the sake of convenience, this tutorial provides the re-encoded files on a dedicated GitHub repository . Simply download the repository as zip file and extract those files in the ‘ipadic-neologd' folder to replace the previous copies. 17 Finally, move the previously extracted NEologd CSV files to the ‘ipadic-neologd' folder. We can now compile our IPADic-NEologd dictionary from the command prompt by changing to that directory and running mecab-dict-index with encoding set both from and to UTF-8: 1 2 cd C: \\P rogram Files \\M eCab \\d ic \\i padic-neologd mecab-dict-index -f utf-8 -t utf-8 Note If a reading error occurs, open the folder ‘/bin/' (e.g.'C:\\Program Files\\MeCab\\bin'), right-click mecab-dict-index.exe → settings → compatibility → check \"run as Administrator\", and try again. From here, edit MeCab's settings to point to the IPADic-NEologd dictionary instead of the default IPADic dictionary. To do so, open the mecabrc file in ‘MeCab/etc/' with a text editor and change dicdir to the relevant path (e.g. C:\\Program Files\\MeCab\\dic\\ipadic-neologd\\ ). mecab user dictionary (optional) It is likely that during close readings of the texts we're analyzing, new, specific terms will come up that are not yet part of the IPADic and NEologd dictionaries. Or perhaps we'll be dealing with application-oriented text classification. As an example, and although it falls more under NER processing, 18 one project 19 involved a dictionary consisting of Japanese book titles generated from a bibliographic database. Their pipeline involved processing incoming tweets from the Twitter Streaming API with MeCab, 20 building a database of tweets containing tokens of bibliographic nature. On top of the system dictionary we have compiled with the above methods, MeCab permits additional user-generated dictionaries: Create a CSV file with terms formatted according to the data structure and column scheme seen above and save it a convenient folder. In our case, a user.csv file, saved in ‘MeCab\\dic\\ipadic-neologd\\user', containing one row 旦那ストレス,,,1234,名詞,固有名詞,一般,*,*,*,旦那ストレス,ダンナストレス,ダンナストレス . Open the command prompt. Navigate ( cd ) to the folder containing the system dictionary (e.g. MeCab\\dic\\ipadic-neologd) and run mecab-dict-index with the parameter -u set to the output dictionary: e.g. mecab-dict-index -u user\\user.dic -f utf-8 -t utf-8 user\\user.csv . Finally, open mecabrc with a text-editor and add a new variable userdic , pointing to one or multiple dictionaries (separated by comma): e.g. userdic = C:\\Program Files\\MeCab\\dic\\ipadic-UTF8\\user\\book_titles.dic,C:\\Program Files\\MeCab\\dic\\ipadic-UTF8\\user\\emoji.dic . Figure 5: 安倍晋三 (Abe Shinzō) with NEologd dictionary. Note The cost field above was filled in randomly. While not harmful when dealing with very specific terms, doing so with a very large dictionary can obviously mess with our results. For a guide on estimating the cost, read コストの自動推定 (JP). Normalizing Japanese text with Neologdn (optional) Within the context of NLP, normalization refers to the process of converting words to their ‘canonical' form or implied meaning, regardless of its spelling (usage of upper- and lowercases, acronyms, alternative spellings, misspellings, etc). Like stemming or lemmatization (reducing words to their root form by removing inflection), this should be done on a case-by-case basis. In the Japanese language, for example, りんご、リンゴ and 林檎 ( ringo ) are all common notations for the word ‘apple'. 21 Likewise, 可愛い, かわいい, カワイイ and even ｶﾜ(・∀・)ｲｲ!! ( kawaii ) all mean ‘cute', but – unlike the rather neutral variants for apple – differ slightly in demographic usage. Normalizing Japanese text with mixed spellings of kana and kanji is a complicated process. 22 for now, however, this small library consists of several regular expressions already helpful in normalizing common tendencies of Japanese text on social media, such as converting half-width characters to full-width. As always, install with pip : pip install neologdn . KH Coder KH Coder is an open-source tool for performing quantitative content analysis, both by conducting statistical analysis on the whole set of text, or by applying coding rules and categorizing pieces of text. Although KH coder supports a variety of other languages (using the Stanford POS tagger), KH Coder was originally written with the Japanese language in mind and supports both ChaSen and MeCab. Having taken the above steps, we can now conduct analysis with KH Coder using a tokenizer and POS-tagger that should be more suitable for dealing with social media content. KH Coder can be downloaded from the link above. I wholeheartedly recommend to view the author's powerpoint slides as well as read both of his publications on how to apply KH Coder for the purpose of quantitative content analysis. 23 Preparing tweet content: python Before we start with KHCoder, however, let's build a CSV file based on the CSV file of (re)tweets generated in the previous article. This new CSV, consisting of two columns (the tweet text and its author), will have such artifacts as hashtags and user mentions removed. Our script (as usual, available for download on GitHub ), thus look as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import sys import csv from itertools import chain from functools import reduce import re import os import pandas as pd import neologdn def parse_tweets ( sys_args ): file_name = sys_args [ 1 ] text_column = sys_args [ 2 ] if len ( sys_args ) > 2 else \"text\" chunksize = 100000 line_count = 0 tweet_list = list () exists = os . path . isfile ( './results/ %s _tweet_ %s .csv' % ( file_name , text_column )) for chunk in pd . read_csv ( './results/ %s .csv' % file_name , encoding = \"utf-8\" , chunksize = chunksize , iterator = True , usecols = [ text_column , \"hashtags\" , \"user_mentions\" , \"is_retweet\" ]): for index , tweet in chunk . iterrows (): tweet_row = {} line_count += 1 hashtags = tweet [ \"hashtags\" ] . split ( \",\" ) if pd . notna ( tweet [ \"hashtags\" ]) else [] user_mentions = tweet [ \"user_mentions\" ] . split ( \",\" ) if pd . notna ( tweet [ \"user_mentions\" ]) else [] if tweet [ \"is_retweet\" ] == False : tweet_text = neologdn . normalize ( tweet [ text_column ], repeat = 2 ) tweet_text = '' . join ([ i for i in tweet_text if i . isalpha () or i . isspace ()]) tweet_text = clean_tweets ( tweet_text , hashtags , user_mentions ) tweet_row [ \"text\" ] = tweet_text . encode ( 'cp932' , \"ignore\" ) . decode ( 'cp932' ) print ( tweet_row [ \"text\" ]) tweet_list . append ( tweet_row ) print ( 'Processed %s lines.' % line_count ) print ( 'Processed total of %s lines.' % line_count ) tweet_json = pd . DataFrame ( tweet_list ) with open ( './results/ %s _tweet_ %s .csv' % ( file_name , text_column ), mode = 'a' , encoding = \"cp932\" , newline = '' ) as file : tweet_json . to_csv ( file , header = ( not exists ), index = False ) def clean_tweets ( text , hashtags , user_mentions ): text = reduce ( lambda t , s : t . replace ( s , \"\" ), chain ( hashtags , user_mentions ), text ) text = re . sub ( \"#|＃\" , \"\" , text . replace ( \" \\n \" , \"\" )) return text if __name__ == '__main__' : parse_tweets ( sys . argv ) Note Similar to the scripts provided in the previous tutorials, the script above should be saved in the folder we have used thus far to store the rest of our script files (e.g. ‘ C:\\python_examples ‘ → ‘ python_tweet_content.py '). The script takes two argument: the name of the input CSV file and the name of the save-file. The commands used to execute our script could thus respectively look like: python python_tweet_content.py #旦那ストレス danna (output CSV files are stored in the same folder as the input CSV, e.g. ‘results/旦那ストレス_tweet_content.csv'). That latter step is necessary because KH Coder only accepts files with ASCII file-names. On that matter, KHCoder only supports EUC and SHIFT-JIS encoded text-files. Thus far we have been encoding our files as UTF-8. While obtuse, lines 28 and 35 are necessary steps to re-encode that content. Using KH Coder Having started KH Coder, let us first change some settings. Click Project → Settings , and make sure ‘MeCab' is selected as method for Word Extraction and the path refers to the correct executable. 24 Furthermore, make sure Unicode Dictionary is checked: we are using a Unicode (UTF-8)-encoded dictionary. Next, click Project → New , and select the CSV file we extracted using the script above (make sure Japanese and MeCab are selected as language and POS-tagger). From there, click Pre-Processing → Run Pre-Processing (this might take quite a while depending on the size of the file). If everything went well, we can now conduct statistical analysis and produce various visualizations that offer quantitative insights in the corpus of tweets we collected. Stop-words (optional) Stop-words refers to the most commonly used words in natural language; words we might want to filter out of our corpora depending on the context. Although we have done some filtering of noise already, figure 7 reveals suru する (an irregular verb meaning to do, often used in combination with compound verbs) and nai ない as some of the most frequent lemmas. Another (or complimentary) method to remove such elements from textual content is thus to make use of stop-word lists. I will write an extended blog post about how one might generate a Japanese language stop-word list, but for now I recommend this one . 25 Simply save in a convenient location and open with a text-editor of choice. Add an extra row \"—cell—\" (necessary for KH Coder's inner workings) and an extra row \"ー\". Next, in KH Coder, click Pre-Processing → Select Words to Analyze → force ignore . Mark the Read from a file check-button, and click browse to select the above stop-word list. Finally, run pre-processing again. Note The usage of the Japanese ー in informal online language is to extend vowels (e.g. しかーし → しかーし) or to flatten consecutive vowels in a way that sounds more rough (e.g. うるさい → うるせー). Neologd does includes many terms that are abbreviated in such matter (知らねー) which are then normalized to the root form (知る), but does not, for example, process variations using both ー and small kana to extend the same vowel (知らねぇー、じゃねぇー,スゲェー), as well as various exclamations like はぁー. In those cases, ー is wrongfully tagged as a proper noun. This behavior actually stems from MeCab's default behavior in trying to identify and tag unknown words. Running MeCab with the unk-feature parameter set helps in that we can choose to tag unknown words as such. Unfortunately, KH Coder is built around MeCab's ChaSen output (using the ‘ -OChasen ' parameter), and changing this requires rewriting a lot of the inner workings of KH Coder. An easier option is thus to just add words that clearly do not add value to the force ignore list. Analysis As an illustration, this article uses a corpus of Japanese tweets containing the hashtag # 旦那 ( だんな ) ストレス (husband stress), collected over the span of several weeks during the COVID-19 crisis (see part two of this series for more information). The focus on that particular hashtag took place within a larger discussion – as part of a BA project – of the COVID-19 crisis and gender hegemony in Japan. One question that came up was whether Twitter users were using those hashtags as means of forming communities of self-empowerment, or instead resorted to using the medium as a form of semi-public diary. Moreover, what were the main topics discussed in this corpus of collected tweets? Frequency List In order to navigate that question, one of the first steps we might do is generate a frequency list (see figure 6 ); by clicking Tools → Words → Frequency List , and generating a CSV or Excel file based either on the general Top 150 tokens or sorted by POS tags. This gives us not only a clear overview of the most frequent terms and thus of key topics, but also of possible errors occurring during tokenization (e.g. wrongfully parsed terms). Having imported that file in KH Coder, we could immediately see that our corpus of 2,555 tweets consists out of 73,590 tokens (40,473 after stop-word filtering), with 7,914 of those unique terms (7,257 after stop-word filtering). A term frequency distribution (Tools → Words → Descriptive Stats), illustrated in figures 7 and 8 , further reveals that about half of those 7,914 terms occur only once. Figure 6: 30 most frequent terms. Figure 7: Term frequency distribution list. Figure 8: Term frequency distribution plot. There is not much else we take away from this information alone. It is not a particular surprise that the most common term is 旦那 ( だんな ) ( dan'na , husband). Frustrations seem likely related to household chores like cleaning, laundry and cooking. Therefore, we might want to rely on co-occurrence matrices such as co-occurrence networks and multidimensional scaling (MDS) of words as visual representations of similarity in meaning. Co-occurrence network A co-occurrence network is a form of network analysis exploring co-occurring terms in our text. In other words, a visual network is formed based on the proximity of high-frequency terms (in non-inflicted form) as a means of identifying clusters of implied meaning. 26 This could help to empirically reveal some of the major trends in these tweets. figure 10 (without stop-word list) and figure 11 (filtered of stop-words) are limited to words with a term frequency of 40 (accounting for about 2% or approximately top 150 most frequent terms, and approximately 40% of all terms) and of all POS-categories excluding grammatical ones such as joshi 助詞 and hitei jodōshi 否定助動詞 (negating auxiliary verbs such as zu ず and nu ぬ). Both figures are based on the top 50 combinations of proximity, while figure 12 and the interactive HTML variant thereof, figure 9 , take only combinations with a similarity coefficient of 0.1 or higher. Figure 9: Interactive co-occurrence network of TF => 50 & similarity coefficient => 0.1 (filtered by stop-word list). Figure 10: co-occurrence network of TF => 50 & top 50 combinations of proximity (unfiltered). Figure 11: co-occurrence network of TF => 50 & top 50 combinations of proximity (filtered by stop-word list). Figure 12: Co-occurrence network of TF => 50 & similarity coefficient => 0.1 (filtered by stop-word list). KWIC concordance Clicking on any of the terms in the co-occurrence network in KH Coder opens the KWIC concordance screen, 27 revealing all the tweets containing that particular term as well as the closeness of other frequent terms. A big trend among these posts seems to be the frustration of the tweet users towards the lack op cooperation of their husband in daily housework – cleaning, laundry, cooking, taking care of children, groceries – as well as lack in assisting with parental duties, lack of hygiene and lack of communication. This happens on top of the COVID-19 crisis leading to measures such as remote work or temporary unemployment. Take, for example, the following three randomly selected and translated tweets based respectively on the terms ‘corona', cleaning' and ‘bath': \"Being in the same room, on top of the stress of self-quarantining during the Corona-crisis, is just too difficult.\" \"A week of vacation during Golden Week. During that time I worked 4 days. Regardless of the holidays, a housewife operates as usual: cooking, cleaning, laundry, playing with children… During that time, my husband did nothing but playing on his smart phone or reading manga. Even if I ask him to play with the children, it's impossible…\" \"In spite of working in the service industry, you come back home and fall asleep in the sofa without washing your hands or taking a bath? I told you to take a bath when coming back. Even if I ask several times, after one or two days it's back to usual. We have three children and we all use that sofa. Don't undo our effort to self-quarantine!\" Coding topics Judging from the above information, we could identify several topics: children, household tasks, the novel corona-virus, family-in-law, … Based on the term frequency information we obtained through the methods above, a very rudimentary coding dictionary of terms that align with those topics was thus created: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 * Children 子供達 OR 子供 OR こども OR 子ども OR 子ども OR 娘 OR 赤ちゃん OR あかちゃん OR 息子 OR 妊婦 OR 育児 OR 保育園 * Household_Tasks 掃除 OR 干す OR 洗濯 OR 片付け OR 洗い物 OR 洗物 OR 洗濯機 OR 風呂 OR 入れる OR 掃除機 OR トイレ OR エアコン OR 床 OR 布団 OR 家事 OR ゴミ箱 OR ゴミ袋 OR near ( ゴミ - 出す ) OR near ( ゴミ - 捨てる ) OR 買い物 OR 買物 OR 乾く OR ごはん OR ご飯 OR 炊事 OR 料理 OR 作る OR 食べる OR 食う OR 弁当 OR 晩ご飯 OR 朝ごはん OR 食 OR 育児 OR 保育園 OR 食器 * Divorce 離婚 * Alcohol 酷い OR グレープフルーツサワー OR 酒サワー OR ビール OR near ( 飲む - 行く ) * Family_in_Law 義母 OR 義父 OR 義妹 * Idle_Complaints 機嫌 OR 悪い OR 気持ち OR near ( やめる - ほしい ) OR キモイ OR 愚痴 OR 吐く OR イライラ OR ストレス OR バカ OR 馬鹿 OR 馬鹿野郎 OR モラ OR アホ OR デブ OR しねる OR モラハラ OR 腹立つ OR 死ねる OR クズ # Note : しね and 死ね are mistakingly stemmed to しねる instead of しぬ * Corona 十万円 OR １０万円 OR コロナ OR 自粛 OR 感染症 OR 感染 OR 危機感 OR リスク OR 新型コロナウイルス * Financial_Employment 給付金 OR テレワーク OR 在宅ワーク OR 失業 OR 仕事 OR 業 OR 収入 OR テレワーク OR 在宅ワーク OR 在宅 OR 休み OR 休む OR 会社 OR 出社 OR 勤務 Saving the above file and applying it as coding rule against the corpus ( Tools → Coding → Frequency → H5 ) reveals how many tweets contain keywords that we have coded among the above categories (as seen in Figure 13 ). Considering the origin of the corpus, it is safe to say that the majority of tweets will contain grievances concerning the Twitter user's husband. Indeed, over a quarter of those posts contain harsh, aggressive language. Posts that directly reference COVID-19 (about 5%) or even the employment status of the husband (approx. 15%), however, are relatively low in numbers. The most prevalent category is that of household tasks; confirming the likelihood that these users are using the medium to vent daily frustrations in regards to the labor performed as a housewife and the lack of compassion or assistance from the husband; something that is further aggravated due to the virus. Figure 13: Topic frequency. Figure 14: Term frequency of user description corpus. Figure 15: Co-occurrence network of of user description corpus (TF > 30 & Coef. >= 0.10). Figure 16: Topic frequency of of user description corpus. Finally, we might want to apply the same methods on a corpus of the personal description of Twitter users to gain more insight into the behavioral patterns of those engaging with the # husbandstress hashtag. The following data is based on a list of user profiles that have posted at least one tweet with that hashtag (a total of 1,429 profiles), filtered by the same standards as above. Figure 15 and Figure 16 , in particular, shows several trends among those users: Nodes that refer to personal hobbies or things those users might be interested in lately, such as animation and video games (something for which NEologd is particularly useful), The personal identity marker of being a mother, Complaints concerning the husband, with the related possibility of a divorce, as well as reference to living together with the mother-in-law. The usual Twitter structures such as mugon forō shitsureishimasu 無言 ( むごん ) フォロー 失礼 ( しつれい ) します (\"forgive me for following without saying anything\"); implying that many users will use the account to follow others, possible in similar situations, without the secondary purpose to communicate with each other, References to the purpose of the Twitter account: a second account intended to express daily grievances. \"My husband is just impossible. I am comforted by seeing the tweets of people expressing the same stress.\" \"I feel murderous rage towards my husband and parents-in-law. This is a complaints-account. Forgive me for silent-following.\" \"Account dedicated to complaining about my husband. I want someone with whom I can freely share such complaints.\" \"I only write complaints about my husband. I am 9 months pregnant and have one 3 year old child. I like Arashi and Korean dramas. Sorry for silent-following. Feel free to do so yourself.\" To do The above analysis is of course a mere illustration of how these tools could contribute towards a quantitative content analysis approach on Japanese tweets. There are various issues remaining in the above example, including a lack of taking into account the general weighing impact of disproportionately frequent tweeters, a too rudimentary topic coding, a lack of theoretical base and, as is common in criticism on content analysis, perhaps too liberal or reductive an interpretation of co-occurring high-frequency terms. It is recommended that BA and MA students interested in applying these techniques delve further into the methodology of content analysis as well as look up other, published examples utilizing MeCab or KH Coder. Wait! There is more! This brief tutorial offered a brief introduction to natural language processing in Japanese context as well as some of the tools available, such as KH Coder. Yet, while the graphical user interface of KH Coder and its plethora of statistical analysis options provided are definitely valuable for in-depth quantitative content analysis, in our next post we will be looking into the benefits of dedicated NLP tools for Python and apply a more extensive method of Japanese normalization. A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 1: Twitter Data Collection A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 2: Basic Metrics & Graphs A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 4: Natural Language Processing With MeCab, Neologd and NLTK A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 5: Advanced Metrics & Graphs On a final note, it is my aim to write tutorials like these in such a way that they provide enough detail and (technical) information on the applied methodology to be useful in extended contexts, while still being accessible to less IT-savvy students. If anything is unclear, however, please do not hesitate to leave questions in the comment section below. Still image from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under a Fair Use doctrine. ↩ Again, this tutorial is Windows-centric. It provides some guidelines for Mac users, but those steps have not personally been tested. ↩ This is of course based on my own limited understanding; entire degrees are set up around those fields. For serious research projects (such as during a graduate thesis), it might be worthwhile to contact someone from the local Digital Humanities department. For a general introduction to NLP, Your Guide to Natural Language Processing (NLP) is a good article to start with. Finally, for a more general introduction to Japanese NLP and computational linguistics, I recommend the free introductory chapter ( available here ) of Bond, F. et al's 2016. \"Readings in Japanese Natural Language Processing\". ↩ For a brief overview of the various dictionaries available for Japanese NLP, read An Overview of Japanese Tokenizer Dictionaries . ↩ For a brief overview of POS-tagging methods, read NLP Guide: Identifying Part of Speech Tags using Conditional Random Fields . ↩ As pointed out in this Quora thread , ChaSen used Hidden Markov Model (HMM) chains in its statistical calculation of probability and inferring relationships between each other. As outlined in this paper by the author of MeCab, MeCab uses Conditional Random Fields (CRFs), a probability model similar to Maximum Entropy Markov Models (MEMM). ↩ There are various others, such as ChaSen, JUMAN, Kuromoji and KyTea , but those haven't been in development for a while and don't bring anything extra to the table. CaboCha is sometimes incorrectly listed among them, but is actually a syntactic dependency analyzer using Support Vector Machines that works side by side with MeCab and is written by the same developer. Nagisa , another python-based ‘Japanese tokenizer based on recurrent neural networks', looks promising but at this point doesn't seem to have any benefits over its RNN alternatives such as JUMAN++. Stanford NLP is an encompassing NLP solution with recently added support for Japanese texts, based on the Japanese Google Universal Dependency Treebank, but I haven't checked this out in further detail. ↩ Its dedicated promotional character, a little girl holding an eponymous janome (bullseye) umbrella, is pretty cute too. ↩ Moreover, Sudachi appear to be the only one among these to perform in-depth normalization of words with inconsistent spelling. ↩ Running MeCab on Mac, Unix or x86 Windows architecture is fairly straightforward. Running Mecab on x64 architecture, however, was rather complex and required users to edit and recompile the source code manually. For the sake of archiving or future reference, these different blog entries were helpful in lining out the required steps. Fortunately, as of 2019, another developer provided a forked 64 bit installer. ↩ An expansion of the IPADic dictionary. Although there is an NEologd expansion of the NINJAL-developed UniDic (an actively maintained dictionary upholding Universal Dependencies conventions), the coarse nature of the much-older IPADic in tokening might arguably be a better fit for this kind of project. ↩ While the particle は would, for example, return a reading of ha ハ and pronunciation of wa ワ. ↩ Unless we edit the source code manually and recompile the application, there is not much we can do to fix this at this point anyway. One workaround, as this Japanese blogger writes, is to set the output encoding of the command prompt to utf-8 using chcp 65001 and piping the Japanese query with echo to mecab (e.g. !#batch echo 猿も木から落ちる | mecab ), or alternatively, to use another command line utility program such as git Git Bash. ↩ ↩ The other options are \"-O chasen\" (to display the POS tagged tokens in ChaSen format), \"-O yomi\" (for displaying the reading of the token) and \"-O dump\" (the default options, dumps all information). ↩ As pointed out here, an alternative method is to compile NEologd as a user dictionary and to use it in conjunction with the standard UNIDic dictionary, rather than to compile the two in one system dictionary. ↩ Having installed iconv , we could do the following in our command prompt to encode all CSV files to UTF-8. Simply replace the originals with the ones in the child directory afterwards: for %%a in (*.csv) do \"C:\\Program Files (x86)\\GnuWin32\\bin\\iconv.exe\" -f EUC-JP -t UTF-8 %%a > ./utf8/%%a . ↩ Because of the frequent updates and massive size of the raw NEologd CSV files, this tutorial won't provide a compiled dic file, however. ↩ Named Entity Recognition; identifying and tagging tokens that are real-world objects (persons, locations, products, etc). The NEologd dictionary has some basic NER as part of the pos sub-classification. The entry for Belgium, for example, returns the following: \"ベルギー,名詞,固有名詞,地域,国,,,ベルギー,ベルギー,ベルギー\". In other words, Belgium is classified as a noun → proper noun → region → country. ↩ S. Yada and K. Kageura . 2015. Identification of Tweets that Mention Books: An Experimental Comparison of Machine Learning Methods. Digital Libraries: Providing Quality Information: 17 th International Conference on Asia-Pacific Digital Libraries, ICADL 2015, Seoul, Korea, December 9-12, 2015. Proceedings ↩ For a process like this that requires handling large amounts of incoming tweets per minute, speed is particularly of essence. MeCab is therefore the most appropriate option, as of writing. ↩ The Japanese term for such words with different written forms and spelling inconsistencies is hyōkiyure # 表記 ( ひょうき ) ゆれ (or… 表記揺れ 🙃). ↩ For a brief overview of this problem and one potential solution, read , Ikeda, Taishi, Hiroyuki Shindo, and Yuji Matsumoto . 2016. ‘Japanese Text Normalization with Encoder-Decoder Model'. In Proceedings of the 2 nd Workshop on Noisy User-Generated Text (WNUT), 129–137. Osaka, Japan: The COLING 2016 Organizing Committee. https://www.aclweb.org/anthology/W16-3918 . ↩ KH Coder is a complex application with a broad variety of tools. The application of KH Coder highlighted in this article serve as a mere introduction, and I wholeheartedly recommend anyone interested in what it has to offer to read the full manual as well. ↩ The one we have installed earlier. KH Coder 3 comes with a version of MeCab pre-installed in khcoder3/dep which can be safely deleted. ↩ Another commonly used stop-word list is the one released by the developers of the SlothLib web library, available here. Stop words are calculated not just on term frequency in large example corpora but also by its relation with other (based on n-grams). For an extensive overview of manually calculating such a list in Japanese, see https://mieruca-ai.com/ai/nlp-stopwords/ . ↩ A method of content-analysis first applied by psychologist Charles Osgood in his 1959 paper \"The Representational Model and Relevant Research Methods\". KH Coder's visual application is based on force-directed graph drawing algorithms known as Fruchterman–Reingold forces, a method developed by Fruchterman & Reingold in 1991. ↩ KWIC stands for Key Word In Context. A KWIC concordance shows a list of all the pieces of text containing a particular lemma (KH Coder performs a form of lemmatization by taking the base form of the lemma based on the MeCab dictionary employed: i.e. stripping conjugation of verbs). ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2020/06/a-quick-guide-to-data-mining-textual-analysis-of-japanese-twitter-part-3/","loc":"https://steviepoppe.net/blog/2020/06/a-quick-guide-to-data-mining-textual-analysis-of-japanese-twitter-part-3/"},{"title":"A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 2: Basic Metrics & Graphs","text":"This short series of blogs chronicles the bare-bones required to conduct a basic form of social media analysis on corpora of (Japanese) Tweets. It is primarily intended for undergraduate and graduate students whose topics of research include contemporary Japan or its online vox populi, and want to strengthen their existing research (such as an undergraduate thesis or term paper) with a social media-based quantitative angle. The purpose of this second blog is two-fold: 1) to introduce the reader to some possibilities in regards to basic social media analysis (applicable almost immediately upon having finished the previous guide ), and 2) to touch upon a crucial, yet sometimes ignored aspect of social media analysis: the legal and ethical caveats regarding privacy and informed content when researching user-generated content on social media. By using a concrete, real-world example, we will thus: Think about how we might integrate Twitter analysis into our project, Use the Python scripting language to further process our dataset to our needs and obtain relevant metrics, Use spreadsheet software such as Openlibre or Excel to produce pivot tables and graphs, Reflect on the ethical and legal ramifications of working with social media data in academic research. Context I assisted several third-year BA students working on a group essay covering aspects of the COVID-19 virus and marital issues within a larger framework of hegemonic masculinity, femininity and gender hegemony in Japan. Their choice of incorporating an element of social network analysis was not ungrounded. Correlating to increased remote work measures taken in Japan as a COVID-19 precaution, several hashtags concerning marital issues began to trend on Japanese Twitter starting Mid-April 2020 and saw further acceleration after being promptly picked up on by several news outlets and tabloids both within- and outside of Japan (such as the Japanese hashtag # coronadivorce , # koronarikon #コロナ 離婚 ( りこん ) ). One of my research interests concerns the many ways the internet, as a cultural artifact, augments or subverts our personal reality. Of interest to this project, then, was how the explosion in usage of the above hashtags reifies in Japan a situation that is undoubtedly being felt worldwide among many (married) couples during the pandemic. Concretely we had several questions: how does Twitter (and general social media usage) fit within the lives of struggling couples trying to cope with the uncertainty of the Corona crisis and its amplification of internal relational struggles? Does the usage of those hashtags reveal any attempts on Twitter to interconnect with one another, form communities and to share advice or frustrations? If so; can we pinpoint particular group dynamics? Might we find a steady increase of Twitter users empowered enough to engage in discourse with each other, or is its usage mostly limited to a form of semi-public, anonymous diaries kept by a small, stable group of users venting frustration? Moreover, what are the common topics of these posts? Can they be further divided in larger categories of frustration (e.g. relating to parents-in-law, housework, finances, children, etc)? Finally, what can this, in a greater socio-cultural context, tell us about gender roles and marriage expectations in Japan? Those are the kind of questions for which quantitative analysis of social media datasets might inductively help to lead the researcher to new theories, or instead strengthen one's existing hypotheses. Having completed the previous tutorial and given a bit more work outlined below, this blog post introduces the reader to some very basic metrics and graphs that could already assist us in shedding some light on the above questions. Preliminary analysis Datasource For the purpose of both this article and the project highlighted above, one specific hashtag tying into the discourse of marital stress on Twitter was taken as the target of our analysis: #husbandstress ( dan'na sutoresu , # 旦那 ( だんな ) ストレス), using the archaic master / husband (dan'na, 旦那 ( だんな ) ) to refer to one's husband. The dataset was obtained using our historical search script with the lang = 'ja' option enabled (limiting tweets to the Japanese language) on the above hashtag; ran for the first time on 8 May, 2020 (right after an extended golden week), with subsequent runs on 15 May, 2020, 22 May, 2020 and 29 May, 2020 (with the since_id argument set to the ID of the last tweet in each preceding set). The first set includes 886 tweets and retweets from the period between Thu Apr 30 05:55:28 UTC and Fri May 08 14:25:30 UTC. 2 The second set includes 1178 (re)tweets tweeted between Fri May 08 15:21:46 UTC and Fri May 15 01:04:47 UTC. The third set includes 857 (re)tweets between Fri May 22 15:34:06 UTC and Fri May 15 01:55:21 UTC. The fourth and final set includes 686 (re)tweets posted between Fri May 22 16:51:16 and Fri May 29 13:54:15 UTC. Note It should again be noted that even in the case of a query that has relatively low usage, such as the one used here, the Twitter search API does not guarantee completeness and the results are thus indicators of trends among the engaging audience, rather than exhaustive resources. As stated on developer.twitter.com : \"The Twitter Search API searches against a sampling of recent Tweets published in the past 7 days. […] the standard search API is focused on relevance and not completeness. This means that some Tweets and users may be missing from search results.\" Processing & Cleaning In order to built a clean dataset in CSV format, used to produce a variety of graphs, this blog provides a slightly edited variant of the parsing script from the previous tutorial. Some of the significant changes to the previous script are as follows: An option to exclude retweets, or to include more meta-information when opting to keep them The addition of user_mention information (i.e. other Twitter accounts tagged with @) An option to choose the CSV output filename An option to localize time to the suspected timezone An option to calculate the total amount of retweets in our dataset and save tweets predating our calculations that were nevertheless retweeted since An option to check for double entries Performance & scalability: able to handle millions of rows and files taking up several gigabytes of disk space. Pre-processing Our edited script (also available for download on GitHub ) thus look as follows: Python script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 import json import tweepy import sys import csv import os import pandas as pd import ijson from pytz import timezone from pathlib import Path from datetime import date , datetime def parse_tweets ( sys_args ): file_name = sys_args [ 1 ] time_zone = sys_args [ 2 ] if len ( sys_args ) > 2 else \"utc\" keep_rt = sys_args [ 3 ] if len ( sys_args ) > 3 else \"True\" save_file_name = sys_args [ 4 ] if len ( sys_args ) > 4 else ( ' %s _parsed' % file_name ) add_totals = sys_args [ 5 ] if len ( sys_args ) > 5 else \"False\" check_duplicates = sys_args [ 6 ] if len ( sys_args ) > 6 else \"False\" count_tweets = 0 exists = os . path . isfile ( './results/ %s .csv' % save_file_name ) tweet_list = list () with open ( \"./results/ %s .json\" % file_name , mode = 'r' , encoding = \"utf-8\" ) as tweet_data : tweets = ijson . items ( tweet_data , 'objects.item' ) for tweet in tweets : entities = tweet [ \"entities\" ] user = tweet [ \"user\" ] hashtags = () user_mentions = () if 'user_mentions' in entities : user_mentions = ( user_mention [ \"screen_name\" ] for user_mention in entities [ \"user_mentions\" ]) tweet_row = {} tweet_row [ \"tweet_id\" ] = tweet [ \"id\" ] tweet_row [ \"text\" ] = \"\" tweet_row [ \"hashtags\" ] = \"\" tweet_row [ \"user_mentions\" ] = ',' . join ( user_mentions ) tweet_row [ \"created_at\" ] = localize_utc_object ( tweet [ \"created_at\" ], time_zone ) is_retweet = ( \"retweeted_status\" in tweet ) if keep_rt == \"True\" : tweet_row [ \"is_retweet\" ] = is_retweet tweet_row [ \"retweet_id\" ] = 0 tweet_row [ \"retweet_created_at\" ] = None if keep_rt == \"True\" and is_retweet == True : retweet = tweet [ \"retweeted_status\" ] re_entities = retweet [ \"entities\" ] tweet_row [ \"retweet_id\" ] = retweet [ \"id\" ] tweet_row [ \"retweet_created_at\" ] = localize_utc_object ( retweet [ \"created_at\" ], time_zone ) tweet_row [ \"text\" ] = \"RT @\" + retweet [ \"user\" ][ \"screen_name\" ] + \": \" + ( retweet [ \"extended_tweet\" ][ \"full_text\" ] if \"extended_tweet\" in retweet else retweet [ \"full_text\" ] if \"full_text\" in retweet else retweet [ \"text\" ]) if 'hashtags' in re_entities : hashtags = ( hashtag [ \"text\" ] for hashtag in re_entities [ \"hashtags\" ]) if len ( re_entities [ 'user_mentions' ]) > 0 : re_user_mentions = ( re_user_mention [ \"screen_name\" ] for re_user_mention in re_entities [ \"user_mentions\" ] if re_user_mention [ \"screen_name\" ] not in user_mentions ) re_user_mentions = ',' . join ( re_user_mentions ) tweet_row [ \"user_mentions\" ] += ( \",\" + re_user_mentions ) else : tweet_row [ \"text\" ] = ( tweet [ \"extended_tweet\" ][ \"full_text\" ] if \"extended_tweet\" in tweet else tweet [ \"full_text\" ] if \"full_text\" in tweet else tweet [ \"text\" ]) if 'hashtags' in entities : hashtags = ( hashtag [ \"text\" ] for hashtag in entities [ \"hashtags\" ]) tweet_row [ \"hashtags\" ] = ',' . join ( hashtags ) tweet_row [ \"text\" ] = tweet_row [ \"text\" ] . strip () tweet_row [ \"user_screen_name\" ] = user [ \"screen_name\" ] tweet_row [ \"user_description\" ] = user [ \"description\" ] tweet_row [ \"user_following_count\" ] = user [ \"friends_count\" ] tweet_row [ \"user_followers_count\" ] = user [ \"followers_count\" ] tweet_row [ \"user_total_tweets\" ] = user [ \"statuses_count\" ] tweet_row [ \"user_created_at\" ] = localize_utc_object ( user [ \"created_at\" ], time_zone ) tweet_row [ \"retweet_count_listed\" ] = tweet [ \"retweet_count\" ] tweet_row [ \"retweet_count_dataset\" ] = 0 count_tweets += 1 tweet_list . append ( tweet_row ) #these are memory intensive, do some garbage collecting del tweets del tweet_data tweet_json = pd . DataFrame ( tweet_list ) with open ( './results/ %s .csv' % save_file_name , mode = 'a' , encoding = \"utf-8\" , newline = '' ) as file : tweet_json . to_csv ( file , header = ( not exists ), index = False ) del tweet_json del file if check_duplicates == \"True\" : remove_duplicate_rows ( save_file_name ) print ( \"Finished processing %s tweets. Saved to ./results/ %s .csv\" % ( count_tweets , save_file_name )) if add_totals == \"True\" : get_retweet_count ( save_file_name ) #note, changing the timestring to match the location based on an estimation (by language filters for ex.) is sloppy: #-> what about Japanese tweets posted abroad? Difficult to detect since utc_offset is no longer supported by Twitter #https://en.wikipedia.org/wiki/List_of_tz_database_time_zones def localize_utc_object ( time_string , time_zone ): date_object = datetime . strptime ( time_string , ' %a %b %d %H:%M:%S %z %Y' ) if time_zone != \"utc\" : date_object = date_object . astimezone ( tz = timezone ( time_zone )) #for example Asia/Tokyo return date_object . isoformat () # Only way to find duplicates in CSV files that are too large too load in one time is to iterate by chunks #collecting doubles in the first iteration and removing them in the second one def remove_duplicate_rows ( save_fn ): double_ids = [] chunksize = 100000 count = 0 header = True for chunk in pd . read_csv ( './results/ %s .csv' % save_fn , chunksize = chunksize , iterator = True ): double_ids . extend ( chunk [ 'tweet_id' ] . tolist ()) #Get list of double tweet_id seen = set () seen2 = [] seen_add = seen . add seen2_add = seen2 . append for tweet in double_ids : if tweet in seen : seen2_add ( tweet ) else : seen_add ( tweet ) del seen os . rename ( \"./results/ %s .csv\" % save_fn , \"./results/ %s _temp.csv\" % save_fn ) for chunk in pd . read_csv ( './results/ %s _temp.csv' % save_fn , chunksize = chunksize , iterator = True ): for index , tweet in chunk . iterrows (): if tweet [ \"tweet_id\" ] in seen2 : chunk . drop ( index , inplace = True ) count += 1 seen2 . remove ( tweet [ \"tweet_id\" ]) print ( 'Tweet ID: %s dropped from CSV.' % tweet [ \"tweet_id\" ]) chunk . to_csv ( './results/ %s .csv' % save_fn , encoding = \"utf-8\" , header = header , index = False , mode = 'a' ) header = False print ( ' %d duplicate row(s) removed.' % count ) os . remove ( \"./results/ %s _temp.csv\" % save_fn ) def get_retweet_count ( file_name ): chunksize = 100000 header = True line_count = 0 Path ( \"./results/metrics_ %s /\" % file_name ) . mkdir ( parents = True , exist_ok = True ) os . rename ( \"./results/ %s .csv\" % file_name , \"./results/ %s _temp.csv\" % file_name ) df = pd . read_csv ( './results/ %s _temp.csv' % file_name , usecols = [ \"retweet_id\" ]) retweets = df [ 'retweet_id' ] . value_counts () . to_dict () if 0 in retweets : del retweets [ 0 ] print ( \" %s unique retweets found. Processing...\" % len ( retweets )) for chunk in pd . read_csv ( './results/ %s _temp.csv' % file_name , chunksize = chunksize , iterator = True ): for index , tweet in chunk . iterrows (): if tweet [ \"tweet_id\" ] in retweets : chunk . at [ index , 'retweet_count_dataset' ] = retweets [ tweet [ \"tweet_id\" ]] del retweets [ tweet [ \"tweet_id\" ]] chunk . to_csv ( './results/ %s .csv' % file_name , encoding = \"utf-8\" , header = header , index = False , mode = 'a' ) header = False os . remove ( \"./results/ %s _temp.csv\" % file_name ) print ( \" %s retweets not in data set. Processing...\" % len ( retweets )) with open ( './results/metrics_ %s / %s _old_retweets.csv' % ( file_name , file_name ), mode = 'w' , encoding = \"utf-8\" , newline = '' ) as file : writer = csv . writer ( file ) writer . writerow ([ \"tweet_id\" , \"text\" , \"hashtags\" , \"user_screen_name\" , \"user_mentions\" , \"created_at\" , \"retweet_count\" , \"url\" ]) for chunk in pd . read_csv ( './results/ %s .csv' % file_name , usecols = [ \"tweet_id\" , \"text\" , \"hashtags\" , \"user_mentions\" , \"retweet_created_at\" , \"retweet_id\" ] , chunksize = chunksize , iterator = True ): for index , tweet in chunk . iterrows (): if tweet [ \"retweet_id\" ] in retweets : if pd . isna ( tweet [ \"user_mentions\" ]): user_mentions = re . findall ( r '((?<=&#94;|(?<=[&#94;a-zA-Z0-9-\\.]))(?<=@)[a-zA-Z0-9\\/_]+(?=[&#94;a-zA-Z0-9-_\\.]))' , tweet [ \"text\" ]) else : user_mentions = tweet [ \"user_mentions\" ] . split ( \",\" ) screenname = user_mentions [ 0 ] f_text = \"RT @ %s : \" % screenname ## There's a rare flaw in the Twitter API that doesn't return the user mention of the og tweeter ## In such cases, use a regular expression f_mentions = \",\" . join ( user_mentions [ 1 :]) if len ( user_mentions [ 1 :]) > 0 else None hashtags = None if pd . isna ( tweet [ \"hashtags\" ]) else tweet [ \"hashtags\" ] url = \"https://twitter.com/ %s /status/ %s \" % ( screenname , str ( tweet [ \"retweet_id\" ])) writer . writerow ([ tweet [ \"retweet_id\" ] , tweet [ \"text\" ] . replace ( f_text , '' ), hashtags , screenname , f_mentions , tweet [ \"retweet_created_at\" ], retweets [ tweet [ \"retweet_id\" ]], url ]) del retweets [ tweet [ \"retweet_id\" ]] line_count += 1 print ( \"Finished calculating total tweets. Processed %s old retweets.\" % line_count ) if __name__ == '__main__' : parse_tweets ( sys . argv ) Note The script above requires three new external libraries that need to be imported in our script (see lines 6 - 8 ): Pytz (used for date manipulation), ijson (used for reading large JSON files in chunks) and Pandas (a very powerful data manipulation and analysis library). Before doing so, first install them with the command prompt using pip ( pip install pytz , pip install ijson and pip install pandas ). Technical notes Some technical notes about the above script: Twitter adheres to the Coordinated Universal Time (UTC) time standard for date & time information of objects such as tweets and user accounts. If we have strong reason to believe that the tweets in our dataset were tweeted within the same timezone, and if we intend to use that data for our work, we may localize the timezone of all tweets (and/or the Twitter accounts tweeting those) in our dataset (see lines 8, 15, 41, 53, 78 and 107-111 ). In regards to our current dataset, which is fairly limited in size, it is fair to assume that with the possibility of some negligible exceptions, the vast majority (if not all) of those tweets were created within the Japanese timezone. Note Taking into account any daylight saving time adjustments, Pytz uses the UTC offsets of the desired timezone for its localization. The time_zone information we're passing to the above localize_utc_object function should adhere to the TZ database names of the desired country (e.g. Asia/Tokyo ). By adjusting the mode argument of the open function to a (append), rather than w (write, which would overwrite any existing files), our script can append the results from a second JSON dataset to an existing CSV file. Furthermore, to ensure that we append results to the same output file every time we want to update our dataset over time, the script now takes an argument to define the filename of the CSV file our results will be written to (see lines 17 and 90 ). If we decide to include retweets in our set (useful for meta information on those who retweet content), we could calculate the amount of times tweets have been retweeted based on the amount of retweets of that tweet in our dataset. Using a temporary helper CSV file, the original CSV is then rewritten to include this information. 3 Moreover, if our dataset contains retweets of original tweets that predate the earliest tweet data we possess, this script further generates a list of those original tweets (e.g. /metrics/#旦那ストレス_old_retweets.csv ), including information such as tweet ID, text, hashtags, user name of the original tweeter, time of creation, amount of retweets and a direct URL to that tweet (see lines 16, 49-68, 101-102 and 150-199 ). When working with JSON data files compiled over a larger period of time, it is not unlikely duplications of tweets might have entered our dataset. In order to remove possible duplicate tweets, a helper function iterates through all rows of a CSV file, keeping track of the rows with a unique ID, and rewrites the output file based on those unique values (see lines 19, 96-97 and 115-148 ). 4 Both scripts on this page use the Pandas data manipulation library to read CSV files in chunks, which slightly impacts speed but drastically reduces memory consumption by iterating over an arbitrary amount of rows at a time (see lines 6, 89, 91, 159-160, 165-171 and 182-199 ). The script above thus takes several new optional arguments when run in command line. In order, they are: The timezone to localize Tweets by (defaults to 'utc' ) A boolean to decide whether or not to include retweets (defaults to True ) The name of the output file (defaults to the name of the JSON file with '_parsed' attached to it) A boolean to decide whether or not to calculate retweets for unique tweets (defaults to True ) A boolean to decide whether to check for doubles in CSV based on the tweet ID (defaults to True ) Note The command in our command prompt (after first navigating to the correct folder using the change directory command cd ) will therefore resemble something akin to: python python_parse_tweet_ver2.py search_tweets_#旦那ストレス_20200515_030826 Asia/Tokyo True #旦那ストレス True True . That command runs the above python script ( python_parse_tweet_ver2.py ) on a raw JSON file search_tweets_#旦那ストレス_20200515_030826.json , localizes the Tweets to Japanese Standard Time (JST), includes retweets, outputs to a CSV file called #旦那ストレス.csv , calculates the amount of times tweets have been retweeted since they were tweeted, and finally, checks for any duplicates in the CSV file. Metrics & Graphs Calculating Metrics with Python Based on our existing CSV we can now start calculating various metrics and produce graphs with our spreadsheet software of choice. As-is, there is plenty we could do already solely by importing the ‘master CSV' file generated earlier and using pivot tables and graphs. This blog post, however, provides another script with a variety of helper methods to calculate and export some of the most prevalently used metrics in separate CSV files. This part will therefore, again, briefly elaborate on that python script, before proceeding to demonstrate the kind of visuals that can be extracted from the generated data and how it might fit into one's research project. The script below thus contains four general helper functions: 5 Note Similar to before, the script below (available on GitHub ) should be saved in the folder we have used thus far to store the rest of our script files in (e.g. ‘ C:/python_examples ‘ → ‘ python_metrics.py' ). The next script take one argument: the name of the input CSV file generated with the script above. The commands used to execute our script could thus respectively look like: python python_metrics.py #旦那ストレス (output CSV files are stored in a new folder named after the input, e.g. ‘ /metrics_#旦那ストレス /'). User data : generates a list of all unique users in our dataset, including 1) calculated fields such as the amount of tweets and retweets each user has contributed to our dataset or how many times their tweets were retweeted, and 2) general information such user name, description, account creation date, following/follower count and the total amount of tweets (taken at the time of the most recent contribution to our dataset). Hashtag frequency : generates a list of each unique hashtag, the amount of times it appeared in (re)tweets and the amount of (re)tweeters having used that hashtag. Date frequency : generates, for each day in our dataset, the amount of tweets, retweets, unique tweeters, retweeters and overlap between tweeters | retweeters in our dataset. Time frequency : generates the same as above, filtered by hour. Essentially, this script iterates through each tweet in our input CSV file, storing the required information in python ‘dictionaries'. Upon finishing, it again iterates through each of those python dictionaries, writing the content to helper CSV files containing metrics for each of the four functions defined above. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 import json import sys import csv from datetime import date , datetime import time from pathlib import Path import pandas as pd def parse_tweets ( sys_args ): file_name = sys_args [ 1 ] keep_rt = sys_args [ 2 ] if len ( sys_args ) > 2 else \"True\" chunksize = 100000 hashtags = {} date_set = {} time_set = {} user_set = {} line_count = 0 Path ( \"./results/metrics_ %s /\" % file_name ) . mkdir ( parents = True , exist_ok = True ) for chunk in pd . read_csv ( './results/ %s .csv' % file_name , encoding = \"utf-8\" , chunksize = chunksize , iterator = True ): for index , tweet in chunk . iterrows (): line_count += 1 is_retweet = 1 if tweet [ \"is_retweet\" ] == True else 0 hashtag_metrics ( tweet , hashtags , is_retweet ) date_metrics ( tweet , date_set , is_retweet ) time_metrics ( tweet , time_set , is_retweet ) user_metrics ( tweet , user_set , is_retweet , keep_rt ) print ( 'Processed %s lines.' % line_count ) print ( 'Processed total of %s lines.' % line_count ) save_hashtag_metrics ( hashtags , file_name ) save_date_metrics ( date_set , file_name ) save_time_metrics ( time_set , file_name ) save_user_metrics ( user_set , file_name ) def hashtag_metrics ( tweet , hashtags , is_retweet ): if not pd . isna ( tweet [ \"hashtags\" ]): c_hashtags = tweet [ \"hashtags\" ] . replace ( \",,\" , \",\" ) . split ( \",\" ) for hashtag in c_hashtags : if hashtag != '' : if hashtag in hashtags : hashtags [ hashtag ][ is_retweet ] = hashtags [ hashtag ][ is_retweet ] + 1 else : retweet_status = [ 0 , 0 , 0 ] retweet_status [ is_retweet ] = 1 retweet_status [ not is_retweet ] = 0 retweet_status [ 2 ] = [ 0 , 0 ] retweet_status [ 2 ][ 0 ] = [] retweet_status [ 2 ][ 1 ] = [] hashtags [ hashtag ] = retweet_status hashtags [ hashtag ][ 2 ][ is_retweet ] . append ( tweet [ \"user_screen_name\" ]) def date_metrics ( tweet , date_set , is_retweet ): tweet_created_date = datetime . fromisoformat ( tweet [ \"created_at\" ]) . strftime ( \"%m/ %d /%Y\" ) if not tweet_created_date in date_set : retweet_status = [ 0 , 0 , 0 ] retweet_status [ is_retweet ] = 1 retweet_status [ not is_retweet ] = 0 date_set [ tweet_created_date ] = retweet_status retweet_status [ 2 ] = [ 0 , 0 ] retweet_status [ 2 ][ 0 ] = [] retweet_status [ 2 ][ 1 ] = [] else : date_set [ tweet_created_date ][ is_retweet ] += 1 date_set [ tweet_created_date ][ 2 ][ is_retweet ] . append ( tweet [ \"user_screen_name\" ]) def time_metrics ( tweet , time_set , is_retweet ): tweet_created_time = datetime . fromisoformat ( tweet [ \"created_at\" ]) . strftime ( \"%H\" ) #change to %I %p for AM/PM if not tweet_created_time in time_set : retweet_status = [ 0 , 0 , 0 ] retweet_status [ is_retweet ] = 1 retweet_status [ not is_retweet ] = 0 time_set [ tweet_created_time ] = retweet_status retweet_status [ 2 ] = [ 0 , 0 ] retweet_status [ 2 ][ 0 ] = [] retweet_status [ 2 ][ 1 ] = [] else : time_set [ tweet_created_time ][ is_retweet ] += 1 time_set [ tweet_created_time ][ 2 ][ is_retweet ] . append ( tweet [ \"user_screen_name\" ]) def user_metrics ( tweet , user_set , is_retweet , keep_rt ): if ( \"retweeted_status\" in tweet ) == False or keep_rt == \"True\" : user = {} if tweet [ \"user_screen_name\" ] not in user_set : user [ \"screen_name\" ] = tweet [ \"user_screen_name\" ] user [ \"description\" ] = tweet [ \"user_description\" ] user [ \"following_count\" ] = tweet [ \"user_following_count\" ] user [ \"followers_count\" ] = tweet [ \"user_followers_count\" ] user [ \"total_tweets\" ] = tweet [ \"user_total_tweets\" ] user [ \"created_at\" ] = tweet [ \"user_created_at\" ] user [ \"total_in_data_set\" ] = [ 0 , 0 ] user [ \"total_in_data_set\" ][ is_retweet ] = 1 ## Note, if full retweet count is preferred, replace \"retweet_count_dataset\" with \"retweet_count_listed\" if \"retweet_count_dataset\" in tweet : user [ \"total_times_retweeted_set\" ] = int ( tweet [ \"retweet_count_dataset\" ]) user_set [ tweet [ \"user_screen_name\" ]] = user else : user_set [ tweet [ \"user_screen_name\" ]][ \"total_in_data_set\" ][ is_retweet ] += 1 if \"retweet_count_dataset\" in tweet : user_set [ tweet [ \"user_screen_name\" ]][ \"total_times_retweeted_set\" ] += int ( tweet [ \"retweet_count_dataset\" ]) if tweet [ \"user_following_count\" ] > user_set [ tweet [ \"user_screen_name\" ]][ \"following_count\" ]: user_set [ tweet [ \"user_screen_name\" ]][ \"following_count\" ] if tweet [ \"user_followers_count\" ] > user_set [ tweet [ \"user_screen_name\" ]][ \"followers_count\" ]: user_set [ tweet [ \"user_screen_name\" ]][ \"followers_count\" ] if tweet [ \"user_total_tweets\" ] > user_set [ tweet [ \"user_screen_name\" ]][ \"total_tweets\" ]: user_set [ tweet [ \"user_screen_name\" ]][ \"total_tweets\" ] #if tweet[\"user_screen_name\"] not in unique_users[is_retweet]: # unique_users[is_retweet].append(tweet[\"user_screen_name\"]) def save_hashtag_metrics ( hashtags , file_name ): with open ( './results/metrics_ %s / %s _hashtags.csv' % ( file_name , file_name ), mode = 'w' , encoding = \"utf-8\" , newline = '' ) as file_hashtags : writer_hashtags = csv . writer ( file_hashtags ) writer_hashtags . writerow ([ \"hashtag\" , \"total_normal\" , \"total_retweet\" , \"total\" , \"unique_tweeters\" , \"re_unique_tweeters\" , \"re_unique_tweeters_filtered\" , \"total\" ]) for hashtag , value in hashtags . items (): normal = set ( value [ 2 ][ 0 ]) retweet = set ( value [ 2 ][ 1 ]) unique = [ x for x in retweet if x not in normal ] writer_hashtags . writerow ([ hashtag , value [ 0 ], value [ 1 ], value [ 0 ] + value [ 1 ], str ( len ( normal )), str ( len ( retweet )), str ( len ( unique )), str ( len ( normal ) + len ( unique ))]) print ( \"Finished. Saved to ./results/metrics_ %s / %s _hashtags.csv\" % ( file_name , file_name )) def save_date_metrics ( date_set , file_name ): with open ( './results/metrics_ %s / %s _date.csv' % ( file_name , file_name ), mode = 'w' , encoding = \"utf-8\" , newline = '' ) as file_date : writer_date = csv . writer ( file_date ) writer_date . writerow ([ \"date\" , \"total_normal\" , \"total_retweet\" , \"total_tweets\" , \"unique_normal_tweeters\" , \"unique_retweeters_exist\" , \"unique_retweeters_filtered\" , \"unique_retweeters_total\" , \"total_tweeters\" ]) for date , value in date_set . items (): #unique_users[is_retweet] normal = set ( value [ 2 ][ 0 ]) retweet = set ( value [ 2 ][ 1 ]) unique = [ x for x in retweet if x not in normal ] writer_date . writerow ([ date , value [ 0 ], value [ 1 ], value [ 0 ] + value [ 1 ], str ( len ( normal )), str ( len ( retweet ) - len ( unique )), str ( len ( unique )), str ( len ( retweet )), str ( len ( normal ) + len ( unique ))]) print ( \"Finished. Saved to ./results/metrics_ %s / %s _date.csv\" % ( file_name , file_name )) def save_time_metrics ( time_set , file_name ): with open ( './results/metrics_ %s / %s _time.csv' % ( file_name , file_name ), mode = 'w' , encoding = \"utf-8\" , newline = '' ) as file_time : writer_time = csv . writer ( file_time ) writer_time . writerow ([ \"time\" , \"total_normal\" , \"total_retweet\" , \"total_tweets\" , \"unique_tweeters\" , \"re_unique_tweeters\" , \"re_unique_tweeters_filtered\" , \"total_tweeters\" ]) for hour , value in time_set . items (): normal = set ( value [ 2 ][ 0 ]) retweet = set ( value [ 2 ][ 1 ]) unique = [ x for x in retweet if x not in normal ] writer_time . writerow ([ hour , value [ 0 ], value [ 1 ], value [ 0 ] + value [ 1 ], str ( len ( normal )), str ( len ( retweet )), str ( len ( unique )), str ( len ( normal ) + len ( retweet ))]) print ( \"Finished. Saved to ./results/metrics_ %s / %s _time.csv\" % ( file_name , file_name )) def save_user_metrics ( user_set , file_name ): with open ( './results/metrics_ %s / %s _users.csv' % ( file_name , file_name ), mode = 'w' , encoding = \"utf-8\" , newline = '' ) as file_users : writer_users = csv . writer ( file_users ) if \"total_times_retweeted_set\" in list ( user_set . values ())[ 0 ]: writer_users . writerow ([ \"screen_name\" , \"total_posted_normal\" , \"total_posted_retweets\" , \"total_posted\" , \"user_description\" , \"user_following_count\" , \"user_followers_count\" , \"user_total_tweets\" , \"total_times_retweeted_set\" , \"user_created_at\" ]) with open ( './results/metrics_ %s / %s _old_retweets.csv' % ( file_name , file_name ), mode = 'r' , encoding = \"utf-8\" ) as old_retweets : csv_reader = csv . DictReader ( old_retweets ) for tweet in csv_reader : if tweet [ \"user_screen_name\" ] in user_set : user_set [ tweet [ \"user_screen_name\" ]][ \"total_times_retweeted_set\" ] += int ( tweet [ \"retweet_count\" ]) else : writer_users . writerow ([ \"screen_name\" , \"total_posted_normal\" , \"total_posted_retweets\" , \"total_posted\" , \"user_description\" , \"user_following_count\" , \"user_followers_count\" , \"user_total_tweets\" , \"user_created_at\" ]) for a in user_set : user = user_set [ a ] if \"total_times_retweeted_set\" not in user : writer_users . writerow ([ user [ \"screen_name\" ], user [ \"total_in_data_set\" ][ 0 ], user [ \"total_in_data_set\" ][ 1 ], ( user [ \"total_in_data_set\" ][ 0 ] + user [ \"total_in_data_set\" ][ 1 ]), user [ \"description\" ], user [ \"following_count\" ], user [ \"followers_count\" ], user [ \"total_tweets\" ], user [ \"created_at\" ]]) else : writer_users . writerow ([ user [ \"screen_name\" ], user [ \"total_in_data_set\" ][ 0 ], user [ \"total_in_data_set\" ][ 1 ], ( user [ \"total_in_data_set\" ][ 0 ] + user [ \"total_in_data_set\" ][ 1 ]), user [ \"description\" ], user [ \"following_count\" ], user [ \"followers_count\" ], user [ \"total_tweets\" ], user [ \"total_times_retweeted_set\" ], user [ \"created_at\" ]]) print ( \"Finished. Saved to ./results/metrics_ %s / %s _users.csv\" % ( file_name , file_name )) if __name__ == '__main__' : #pass in the target filename without \"json\" as argument in command prompt. parse_tweets ( sys . argv ) Spreadsheet Graphs This section contains some graphs we might create upon importing the metric CSV files, generated with the script above, into a spreadsheet application. 6 Note The content of the Tweets and the description of the users remain as-is, including newlines, URLs, hashtags and emojis. Older versions of MS Excel, in particular, might have trouble correctly importing CSVs because of that. If using an older version of Excel is a must and importing the CSV results in faulty data, the best option would be to use OpenRefine to export the CSV as a valid Excel file (make sure that UTF-8 is selected for character encoding and that the radio button indicating columns are separated by commas is checked). Moreover, ensure that the Tweet ID columns are set to text rather than int64, in order to prevent Excel from cutting of digits in its conversion. Date & time frequency The time-series graphs below represent changes over time and were made after importing the date frequency and time frequency CSV files in Excel 2019. Both Figure 2 (a line chart measuring all occurrences over time) and figure 3 (a stacked area chart showcasing the total amount of tweets of a daily timespan split in original tweets and retweets) reveal a strong peak in retweets on the 13 th of May. This spike obscures the visual effectiveness of the rest our graphs. A solution is thus to split the count of Tweets and retweets over two different horizontal axes ( figure 4 ), or use a 100% stacked bar graph to view the relation between the total tweets and retweets ( figure 5 ). Note Data for the first and last day in our set (respectively 04/30/2020 and 05/29/2020) contains data only up to a certain point of that day and has thus been left out of the following graphs. Figure 1: Amount of daily active Twitter users (re)tweeting #旦那ストレス Figure 2: Total combined tweets containing #旦那ストレス Figure 3: Total tweets & retweets Containing #旦那ストレス Figure 4: Total tweets & retweets Containing #旦那ストレス Figure 5: Tweet/retweet division of combined total containing #旦那ストレス per day In regards to the question of a public sphere and interconnected discourse forming around this hashtag, we might be interested in knowing how many of those retweeters are actually actively writing similar Tweets themselves (and thus engaging with others connected by the hashtag) as well. In that case, we could use a mixed graph with stacked bars for visualizing retweeters in two groups (the right axis) and a line measure for the count original tweeters on the left ( figure 1 ). One immediate takeaway is that only a very small number of retweeters have actually tweeted similar content themselves, suggesting that there might not be much engagement among active tweeters (and thus unlikely to develop into a public or community developed through this hashtag). 7 From a glance it is furthermore clear that the amount of active tweeters practically doubled during the first couple of days of the extended holiday (golden week) in Japan, with gradual ups and down in activity since. The same goes for retweet activity, with a clear exception present around the period of May 13 (more on that below). Using the spreadsheet table overview of our full CSV data as well as spreadsheet pivot functionality helps bring clarity to this trend deviation. The deviation concerns several tweets of one particularly active user with thousands of followers, but with a nevertheless low amount of posts actually being retweeted or replied to. Although the most prominent Tweet (which accuses the husband of intending to hoard the cash handouts each household member would receive in light of the Corona virus) was made prior to the first Tweet in our set, it didn't get picked up on until late May 12 (with an increase in pace after being retweeted by a prominent Twitter influencer with over 80.000 followers). Subsequent retweets more or less lasted until the early night of May 14. Moreover, the follower count of that Twitter user increased fifteen fold between the first and last raw JSON data files we obtained including that user's Tweets. It should also be noted that although having frequently used both the husband stress and related hashtag #husbanddeathnote ( dan'na desunōto , # 旦那 ( だんな ) デスノート) before, the user has since such an increase in attention refrained from using them at all. The renewed peak in (original) Tweets and (unique) tweeters shortly after the initial retweet peak of May 13 suggests a brief reinvigorating effect after a period of stagnation. Nevertheless, it is recommended to conduct a closer content analysis (using quantitative and qualitative methods) of the Tweet content to deduce whether those Tweets are indeed signaling an increase in users utilizing the medium to discuss dissatisfaction in regards to their spouses, or are instead meta-conversations on the existence of those hashtags (as is so often the case with viral hashtags). Figure 6: Total hourly (re)tweets containing #旦那ストレス Figure 7: Total hourly (re)tweeters engaging with #旦那ストレス Finally, while spreading the tweeting ( figure 6 ) or tweeter ( figure 7 ) rate on an hourly basis might have its benefits for specific use cases, it does not yield particularly surprising results when applied to our dataset. While it is not much of a stretch to assume that the vast majority of tweets in our dataset are posted by mothers 8 with a particular rigid daily schedule, it is reasonable to believe that the peaks in activity in the late evening, as well as right before and after lunch are common among unrelated Twitter users as well. Hashtag frequency These next graphs are based on the top ten hashtags sorted by frequency of appearance (excluding, of course, the hashtag we have used as the query to collect our data). With the exception of figure 10 (which counts based on the total amount of unique Twitter users rather than total tweets) as demonstration of marginally different results, all are sorted by total combined (re)tweets. Figure 8 is an English translation of Figure 9 , and Figure 11 juxtaposes the amount of tweeters and retweeters with the total amount of tweets and retweets per hashtag. It should be noted that these statistics are based on the individual occurrence of each hashtag and thus does not keep in mind the overlap of multiple such hashtags in one particular tweet (some tweets consisted of nothing more than several hashtags and an annexed image). The majority of the hashtags are idle complaints targeting the spouse with various levels of playfulness. The highest ranking hashtag, #husbanddeathnote ( dan'na desunōto , # 旦那 ( だんな ) デスノート), for example, refers to a popular 2000's manga, anime and live action series (Death Note) and in particular to its eponymous notebook, which results in the death of every person whose name is penned down. The concept has since been juxtaposed with various demographics as an expression of anger or frustration. A google search suggests that this hashtag predates our targeted hashtag # 旦那 ( だんな ) ストレス, with media attention dating all the way back to around 2017. Furthermore, the top result links to an anonymous bulletin board danna-shine.com (a transliteration of # 旦那死 ( だんなし ) ね, husband, die! ). A brief visit to that website reveals that it is, perhaps unsurprisingly, intended for posting anonymous complaints, 9 claims to have over 180.000 monthly users and makes a visual reference to Twitter and that specific death note hashtag in its banner. 10 Figure 8: Top 10 used hashtags sorted by combined total (re)tweets containing #旦那ストレス (English translation) Figure 9: Top 10 used hashtags sorted by combined total (re)tweets containing #旦那ストレス Figure 10: Top 10 used hashtags sorted by original tweets containing #旦那ストレス Figure 11: Top 10 used hashtags original tweets containing #旦那ストレス Other noteworthy hashtags are \" #coronadivorce \" ( Korona rikon , #コロナ 離婚 ( りこん ) ), \" #emotionallyabusivehusband \" ( morahara otto , モラハラ 夫 ( おっと ) ) and \"#mother-in-lawstress\" ( gibo sutoresu , # 義母 ( ぎぼ ) ストレス). The first, of course, refers to the greater discourse in which we situate this study, although actual references to the threat of divorce appear to be relatively low in our sample. The second refers to morahara (モラハラ, an abbreviation of the pseudo-Anglican Japanese term moral harassment), indicating a sociological concept of emotional and psychological abusive behavior through means of gas-lighting and humiliation. The last hashtag, \" #mother-in-law stress \", then, is exceptional in that it is not just the only hashtag in our top ten not explicitly connected to the spouse, it is also the only one that has a disproportionate amount of retweets and retweeters. 11 User Data Based on the generated user data CSV, we might use pivot functionality to build graphs visualizing the relation between things such as date of user registration and tweets. Figure 13 , for example, visualizes users, with at least one actively tweeted message containing # 旦那 ( だんな ) ストレス, in a pie of pie chart on a yearly basis, demonstrating that 41% of such users were created in 2020. Figure 15 further divides the period of 2018 to May 2020 by month, illustrating again that there is indeed a fairly consistent monthly increase since late 2019 in registered accounts utilizing Twitter to display marital problems. Based on those graphs, we can thus conclude that the majority of the Twitter users actively posting tweets containing this hashtag registered their account on Twitter extremely recently, most likely as a second, private account (often referred to as ura'aka, # 裏垢 ( うらあか ) ) created specifically for this purpose. In comparison then, figure 14 suggest that there is less of a psychological barrier for Twitter users to retweet such content on existing accounts; a retweet might insinuate sympathy towards the original tweeter and/or personal engagement with the topic at hand, but enough distance remains to not be used negatively towards those retweeters. The fact that the green line (representing the total amount of Twitter users in our dataset) is almost consistent with the combine value of the stacked bars (total tweet & retweet posters) further demonstrates that there is a very low tendency for users to both actively tweet content with this hashtag and simultaneously retweet the content of others. Taking a different angle to demonstrate the same thing, Figure 12 displays the amount of (re)tweets posted on a yearly basis both separate and as running total. Again, this graph suggest that there is not anything particularly outstanding concerning the registration date for users retweeting such content, while simultaneously showing that the most actively tweeting accounts have all been created very recently. Figure 12: (Cumulative) totals of (re)tweets containing #旦那ストレス spread over date of account creation by year Figure 13: Year of creation of Twitter users in our dataset having posted more than one post containing #旦那ストレス Figure 14: Twitter users engaging with #旦那ストレス, sorted by account year of creation Figure 15: Registered accounts with active #旦那ストレス usage per month (2018 - 2020) Figure 16: Twitter users sorted by amount of original tweets containing #旦那ストレス Figure 16: Twitter users posting original tweets containing #旦那ストレス, sorted by followers and following count Finally, the two ugliest graph on this blog ( figure 16 and figure 17 ) reveal more about the level of engagement of Twitter users actively posting themselves, as well as the possible reach and/or attempts to interconnect with others based on the amount of followers and ‘friends' (i.e. their following count). About half of those users have tweeted content with the # 旦那 ( だんな ) ストレス hashtag on a repeated basis within the timespan of our dataset and are thus likely to have a particular strong investment in the topic. It is not unlikely that there are those, among the other half of those users, who have no personal investment but rather feel some curiosity towards the actual hashtag and the discourse it itself represents. One such Twitter account, for example, has over 300.000 followers. By sorting the users table by followers, this was revealed to be the public account of Shinya Arino, a famous Japanese comedian who might have possibly come into contact after a particular tweet mentioned earlier went relatively viral on the 13 th of May. By cross-referencing our full CSV table with this name, the content and date of creation of Shinya's tweet was easily revealed: 「このハッシュタグは # 勉強 ( べんきょう ) になる # 旦那 ( だんな ) ストレス」 (Kono hasshu tagu wa benkyō ni naru, Eng. \"This hashtag is illuminating\"), posted on 18 May, 2020. It is difficult to gouge what impact this ‘endorsement' by a public media figure has or if it was perhaps intended mockingly, but at the very least figure 4 reveals a stark drop rather than rise in accounts tweeting such messages right after his tweet. Automation: Bringing Everything Together In a research project that requires the collection of Twitter data over a long period of time, it is understandably a drag to manually update the preprocessed data (and its derisive metrics in CSV format) through various separate scripts run from the command line each time new data is to be collected. One method of simplifying those steps is to use a batch script 12 that automatizes several or even all of the acts we have been doing manually. Upon saving and running the following script, for example, it will iterate through all the unprocessed JSON files with a filename matching the name of the batch file (e.g. #旦那ストレス.bat → all JSON files containing #旦那ストレス ) and run them through python_parse_tweet_ver2.py with predefined options such as setting the timezone to \"Asia/Tokyo\" (keeping the total calculation and check for doubles until the final JSON has been processed). Finally, this batch script will run the preprocessed CSV file through the parsing metric script we have outlined in this article. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @echo off :: Necessary for Unicode support chcp 65001 >NUL cd /d C: \\p ython \\ set safe_csv = %~n0 set time_zone = Asia/Tokyo set include_retweets = \"True\" set add_totals = \"True\" set double_check = \"True\" set query = %~n0 set since_id = \"None\" :: Count total amount of files for /f %%A in ( 'dir /A:-D /B .\\results\\*%query%*.json &#94;| find /v /c \"\"' ) do set total = %%A :: Necessary to count in loop setlocal enabledelayedexpansion set count = 0 for %%a in ( results/*%query%*.json ) do ( :: Create CSV of all relevant query JSON echo \"Pre-processing: %%a\" set /a count += 1 IF !count! == %total% ( py python_parse_tweet_ver2.py %%~na %time_zone% %include_retweets% %safe_csv% %add_totals% %double_check% ) else ( py python_parse_tweet_ver2.py %%~na %time_zone% %include_retweets% %safe_csv% ) ) endlocal :: Create metrics CSV files echo \"Calculating metrics...\" py python_metrics.py %safe_csv% PAUSE This script could be further edited to begin by first datamining new JSON data with any of the API scripts provided earlier, potentially using the Windows Registry to keep track op the since_id variable. If desired, we might also schedule this script to run periodically using the Windows schedule manager (e.g. to run the Search API script and its preprocessing and parsing scripts on a weekly basis, or even on a daily basis if daily updated statistics on account information, such as follower and following count, is preferred). If we are using spreadsheet software that maintains a connection with the CSV data sources for its tables and graphs, updating our graphs to reflect those changes is a matter of pressing a single update button (such as the Data → Refresh All button in MS Excel 2019). Legal and Ethical Caveats Having glanced over both the above tutorial and the provided graphs or data, the reader might have (with one exception) noticed a lack of direct references to concrete Tweets or users. Japanese Tweet content has either been translated to English without supplying any identification, or visualized through calculated metrics. Moreover, this series does not provide access to the original JSON files or to the preprocessed CSV files. This is done deliberately both according to Twitter API's terms of service (which prohibits sharing raw, unprocessed Tweet data) and, from an ethical standpoint, to protect the identity of its users (regardless of the public accessibility of those users' Tweets and the levels of anonymity their accounts provide). One jarring exception this article made is the explicit reference to Shinya Arino, a public figure who is using Twitter with the explicit purpose of reaching a large public. Verbatim quoting his tweet, then, was done purposefully to illustrate the ethical line one must thread when revealing personal information of social media users. Traditionally, informed consent, anonymity and confidentiality are crucial elements of research involving public opinion of private individuals. For over a decade, social media platforms has been providing (budding) researchers the opportunity to engage with an unprecedented amount of data representing public opinion, creating somewhat of a gray zone within academic research concerning the above elements. From a legal perspective, Twitter's Privacy Policy does indeed inform its users that all public data could be used for the purpose of academic research. Yet, as Fiesler and Proferes (2018) 13 rightly point out, social media users are often not aware of the extent their content on those platforms can be used. Instead, Fiesler and Proferes highlight several prominent examples of scandals or public outrage in regards to publications that have used such data, even if they did not break any legal boundaries. One could argue that this ethical boundary is highly individual. Data on platforms such as Twitter is explicitly open, more so when containing hashtags whose prime function is to reach a wider audience. 14 Moreover, websites such as Buzzfeed have financially thrived on using tweets from private individuals for comical effect. In an era of meme-dominated pop culture, it is furthermore extremely common to find memefied tweets spreading across other media platforms. Especially in light of the recent Cambridge Analytica scandal, however, utter care must be taken that no risks befalls any individual whose social media content is part of one's research data; especially when dealing with sensitive topics such as the one highlighted in this article. Generating quantitative data that does not pinpoint individual content (such as the statistics provided in this article, and text analytics such as frequency tables, sentiment analysis, etc) is but one specific part of social media-based research, and often goes hand in hand with qualitative readings of certain content. What if I published a paper on this topic that would later be cited by a major Japanese news outlet? If I had referenced a particular tweet by a user who had afterwards revealed personal information, could it not potentially endanger that Twitter user? This is of course highly unlikely, but nevertheless not impossible and something that should be kept in mind when taking a research angle dealing with such data. Although Woodfield (2017, ch.4) 15 refers to a research project that took a similar stance as I did concerning the masking of individual user names and paraphrasing rather than quoting tweet content, Woodfield has also referred to another research project in which researchers retrospectively searched informed content of each Twitter account whose tweet content they would individually use in their publications. This might seem excessive, and on a purely legal basis not even required, but even in the case of a graduate thesis rather than a PhD thesis or academic publication, it is at the very least not unwise to contact the board of ethics of one's university for clarity on how to best deal with data obtained from social media from an ethical point of view. Wait! There is more! Some of the graph examples and analyses in this article might appear overly excessive, especially when keeping in mind the limited scope of the highly specific hashtag analyzed. 16 Nevertheless, it is my hope that those graphs served some purpose in illustrating how social network analysis and the methods outlined in this article might fit in research topics covering contemporary issues and public opinion expressed online. Throughout the next entries in our blog series, we will dive further into actual textual analysis of Japanese corpora. In the third blog, we will look into the Japanese computational linguistics tool KH Coder, the morphological parser MeCab and its extensions, as well as the various options they offer us in our quantitative text analysis. The fourth blog will expand on the script provided in this article, including options for animating changes over an arbitrary timespan. A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 1: Twitter Data Collection A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 3: Natural Language Processing With MeCab, Neologd and KH Coder A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 4: Natural Language Processing With MeCab, Neologd and NLTK A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 5: Advanced Metrics & Graphs On a final note, it is my aim to write tutorials like these in such a way that they provide enough detail and (technical) information on the applied methodology to be useful in extended contexts, while still being accessible to less IT-savvy students. If anything is unclear, however, please do not hesitate to leave questions in the comment section below. Still image from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under a Fair Use doctrine. ↩ The Coordinated Universal Time (UTC) time standard; a universal standard similar to the Greenwich Mean Time (GMT) timezone. ↩ Although the original JSON tweet objects contain a retweet_count variable, this contains all the retweets of that post from its conception up to the point we first acquired it in our dataset. This is not sufficient for long term data mining projects. Unfortunately it's quite taxing to calculate replies the same way, and the Twitter reply_count variable is only available to Premium and Enterprise Twitter API services. ↩ It's important to check on tweet ID doubles rather than on unique rows because secondary data such as follower and following count of the user might have changed by the time we have obtained new data with some duplications. In other words, even if the Tweet itself is a duplication, the row in our CSV file wouldn't be counted as such. ↩ While functional and rigidly tested through various use-cases, this script is far from optimized and not very pythonic . I will rewrite this in due time, using in particular the python package pandas . ↩ It should be noted that some of the data we calculate by means of python could fairly easily have been derived from the base, preprocessed CSV file using pivot tables. About half of the data couldn't, however, and centralizing everything in one script was a deliberate choice. ↩ Another option to gouge this is by viewing overlap of followers and followings of the most active users (network analysis) over a certain timespan, which is what we will be doing in a next blog entry. ↩ A glance over the description of all tweeters seems to suggest so (with language and emojis indicating one's social identity as a mother or raising children), and could be further confirmed by textual analysis. ↩ Or to be exact, death wishes. 💀 The slogan furthermore refers to the corona virus, demanding the spouse to get infected as fast as possible. ↩ Indicating the limited nature of analyzing only one particular social media platform, since various platforms might perform together to form unique eco-systems on which discourse can thrive. ↩ The social problem of toxic relations with parents-in-law was furthermore the topic of the bestseller 2016 novel The House on the Slope ( Saka no Tochū no Ie , 『坂の途中の家』) by Mitsuyo Kakuta, and its 2019 live action adaption. Both reinvigorated public interest in contemporary marital issues in Japan and concluded with the possible importance of social media towards empowering women stuck in toxic marriages. ↩ Short for computerized batch processing. ↩ Fiesler, Casey, and Nicholas Proferes . 2018. ‘\"Participant\" Perceptions of Twitter Research Ethics'. Social Media + Society 4 (1): 2056305118763366. https://doi.org/10.1177/2056305118763366 . ↩ Even though its usage might often be for aesthetic or comic purposes. ↩ Woodfield, Kandy . 2017. The Ethics of Online Research. Emerald Group Publishing. ↩ Not withstanding the greater social relevance of the topic that encompasses this hashtag, either. ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2020/05/a-quick-guide-to-data-mining-textual-analysis-of-japanese-twitter-part-2/","loc":"https://steviepoppe.net/blog/2020/05/a-quick-guide-to-data-mining-textual-analysis-of-japanese-twitter-part-2/"},{"title":"A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 1: Twitter Data Collection","text":"This short series of blogs chronicles the bare-bones required to conduct a basic form of textual analysis on corpora of Japanese tweets. Examples of similar tutorials on the Internet are numerous, 2 but less so are accessible beginner tutorials guiding the reader throughout the processes of: setting up the initial technical environment, compiling corpora of clean, processed data, and, adding a visual, quantitative element to any qualitative reading of that text, by utilization of textual analysis tools tailored for Japanese content . This series is therefore primarily intended for undergraduate and graduate students whose topics of research include contemporary Japan or its online vox populi , and want to strengthen their existing research (such as an undergraduate thesis or term paper) with a social media-based quantitative angle. Keeping in mind that many of those situated in the humanities might experience an initial technical hurdle, this first blog will focus primarily on the how , rather than on the why of doing Twitter-based research, by detailing the minimal necessities for getting up and running — supplemented by a brief optional, technical explanation for those who are interested . With this first blog, the reader will thus concretely: Set up a Twitter Developer account and obtain Twitter credentials, Set up a Python development environment, Run tailored Python scripts to build datasets of tweets, based either on keywords or on the tweet history of particular users Use Python for preprocessing the dataset into a usable corpus. This first blog assumes that the reader has already chosen a topic or target of analysis for which a form of Social Network Analysis (SNA) or content analysis of Twitter data is well-suited. A more thorough epistemological introduction to the why , what , when and who of SNA, as well as further recommended reading, will follow in the future. Suffice to say, the technical ease of working with the Twitter APIs, as well as the global-spread use of Twitter (roughly half a billion tweets are sent every single day, with Japanese per capita usage ranking particularly high), offer an excellent introduction to getting acquainted with SNA through practical, real-life examples. Set-up It must be emphasized that the field this tutorial roughly falls under, Digital Humanities (DH), is extremely broad; and understanding the various possibilities DH offers, as well as when and how to apply those, have their own intricate challenges. Within the scope of our brief tutorial series, however, the initial technical hurdles of setting up a proper technical environment and just getting scripts running will probably be the most challenging for most readers. The set-up and approach we will be applying throughout this series might seem daunting at first, but as of writing, there is no free alternative with a graphical user interface that offers as much control as doing things manually would. Twitter API credentials APIs (Application Programming Interfaces) are pieces of code that permit cross-platform and cross-programming language communication between different software. A web-application, a desktop application or a simple script of code (such as the ones in our article) might access an API in order to exchange (retrieve, create, update or delete) information. Mobile versions of Twitter (Android, iOS), for example, are relatively simple applications that might access the Twitter API to get tweet data from its online servers to display it on-screen, or instead sent and save a newly written tweet. This kind of interaction between different applications, written in different programming languages, is everywhere: even a simple retweet button on a blog article, or a Buzzfeed news article peppered with a bunch of relevant tweets, rely on those Twitter APIs. Like many other large social media platforms such as Facebook and YouTube, Twitter has an extensive list of APIs made available to developers, researchers and market strategists alike. The most extensive ones are limited to expensive Premium and Enterprise editions targeting commercial enterprises, but, while undeniably limited, the free standard APIs and its Terms of Service (ToS) do permit us a certain degree of data accumulation sufficient for our goals. Note There are some pitfalls that must be noted in regards to the limitations of the free-to-use Twitter APIs . None of the methods provided below permit the collecting of an exhaustive collection of tweets. Instead, queries will be executed against a sample of the global total amount of (historical) tweets. 3 Therefore, any conclusions drawn based on the amount of tweets per timespan will be estimates rather than absolutes. Moreover, due to the time restraints of the Search API (7 days), ad-hoc research of older phenomena is nearly impossible. Depending on the scope of the search query, the Search API and Timeline API in particular could yield more accurate results than using the Streaming API does, however. 4 Before we are able to begin, however, we should first apply for a Twitter developer account and obtain several credentials required to access those Twitter APIs. Open the Twitter developer page (if you don't yet have a Twitter account, you will have to create one now) and click on Apply for an account. Throughout the next few screens, select Doing academic research → verify your personal personal information (if you haven't do so yet, you will likely have to verify your cellphone number) → describe your intended use → review your application → accept the Developer Agreement → click Submit Application . Your application will be judged in-person based on your Intended Use and should be well thought-out. I have written a brief example— for your reference only —, as to how you might approach this, in the screenshots below. Upon receiving a confirmation of approval (an application is usually approved or denied within a matter of hours), head to the Apps management screen → click Create an app and fill in the required information: an ‘app' name, brief application description, Website URL, and information regarding how your application will be used. Again, something similar to what is written in the screenshot below should be sufficient for your description. Moreover, the field how it will be used can repeat what was written in the previous application (it is not required to wait for further external approval after creating an ‘app', so this step is less important). Neither is it important to have a personal website; it is fine to substitute this with an URL to your Facebook, LinkedIn or Twitter profile. Next, click on the details button for the new ‘app' and open the Key and tokens tab. Generate Consumer API keys and Access token and access token secret keys , and note these down in a separate file. They are required to connect to the Twitter API through our Python scripts after we have finished our set-up. Python Although there are plenty of other programming languages with which we might access the Twitter API for similar results (such as Java or Ruby), in this series of blogs we will use the easy-to-read, well-documented scripting language Python . Python (along with the statistical research language R) has, due to its extensive library of third-party modules, become somewhat of a de facto lingua franca within the (digital) humanities. Within DH, its usage covers anything from data processing, visualization and chore automation to machine learning, Natural Language Processing (NLP) and general linguistic analysis. Note It is recommended to follow a brief, optional tutorial. 5 Python is relatively easy to learn and doesn't require any prior knowledge of programming. Now head to the Python homepage and download the latest installer version matching your operating system. Recent installers will already be packaged together with necessary add-ons such as pip , a Python package manager for installing custom packages. Make sure to check the Add Python 3.x to PATH check button before proceeding. 6 Next, open the Windows command prompt (or the Terminal on Mac OS X). 7 To do so on Windows, press Win + R , enter cmd and press Enter . 8 Now input python (or its abbreviation py ) and press Enter again. Given that the installation went off without a hitch (and that the python executable was successfully added to your PATH variable), this should open the python interpreter as shown in the screenshot below. Play around a bit and input quit() to exit the python interpreter environment. Finally, we will need to install tweepy , a Python package required by our example scripts in order to simplify our access to the Twitter APIs. Input pip install tweepy to install Tweepy and any dependent packages. 9 Accumulating Data Having set up our development environment, let us now dive into some working examples: Copy and paste the desired script(s) below in a text editor of choice, 10 select save as and save them with a suitable name with the .py file extension (python files)) in an easily accessible folder (e.g. save the first script as account_scraper.py in c:\\python_examples\\ ). Alternatively, you could also download them from this article's corresponding GitHub page . Don't forget to replace the placeholder Twitter API credentials ( #### ) with the credentials obtained earlier. Open the command prompt again. Navigate to the folder containing the python script(s) you have just saved (e.g. use the command cd to change directories: cd C:\\python_examples\\ ). Run the python script by invoking the name you saved it by, using the python command and the name of the query (either the search query or the name of the target Twitter profile, e.g. python python_twitter.py poppestevie or py python_search.py poppestevie ). Pressing Ctrl + C in your command prompt at any time will cease the process. Due to the API limitations, the account API script will finish in a matter of seconds, while the real-time streaming API example will run until the process is terminated and, depending on the popularity of the search query, the historical search API script could run for over a day despite a hard limitation of 7 days. The APIs return tweets matching our search queries as unstructured data formatted in JavaScript Object Notation (JSON, a lightweight data-interchange format). 11 Our script saves those to a valid JSON formatted file in a ‘results' subfolder (e.g. C:\\python_examples\\results\\ . 12 Note When encoding to UTF8 is enabled, Unicode characters (such as Japanese characters or emoji) and other characters that fall outside the ASCII range are, by default, escaped (e.g. 🤷 → ‘\\u1F937'). This is a common practice taken to avoid data mangling among legacy systems, and is far more memory effective due to the large size of Unicode characters (which are up to 4 times larger than their ASCII representations). The scripts on these pages, however, bypass this behavior with the ensure_ascii = False argument. Datasets that are expected to contain several hundreds of thousands of tweets are recommended to have that argument set to True, as such datasets will easily take up to several gigabytes of disk space. Decoding texts to their actual Unicode value is, in those cases, best kept on a need-only basis during the preprocessing phase. 13 By Account: Twitter REST API Our first example script collects tweets posted by specific Twitter users, up to the most recent ~3200 tweets posted by those accounts (a limitation inherent to the Twitter API itself, which cannot be easily bypassed). 14 In essence this script uses Tweepy's pagination method Cursor to iterate through the target's timeline, 200 tweets at a time (the maximum amount permitted per access call it makes to the Twitter GET API). Objects are returned as dictionaries of JSON objects, which are iterated through and written to a new, JSON-compliant file (e.g. poppestevie_search_tweets.json ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import tweepy import json import sys from datetime import datetime from pathlib import Path #Twitter API credentials consumer_key = '####' consumer_secret = '####' access_key = '####' access_secret = '####' search_query = '' def get_timeline_tweets ( screen_name ): auth = tweepy . OAuthHandler ( consumer_key , consumer_secret ) auth . set_access_token ( access_key , access_secret ) api = tweepy . API ( auth , wait_on_rate_limit = True , wait_on_rate_limit_notify = True ) tweet_count = 0 ; timestamp = datetime . today () . strftime ( '%Y%m %d _%H%M%S' ) #create dir results if != exists Path ( \"./results/\" ) . mkdir ( parents = True , exist_ok = True ) with open ( './results/timeline_tweets_ %s _ %s .json' % ( screen_name , timestamp ), mode = 'w' , encoding = \"utf-8\" ) as file : #bit of a hacky way to create valid JSON but easier on memory file . write ( '{\"objects\":[' ) try : # cursor pagination, 200 is limit of returned Tweets per access call for status in tweepy . Cursor ( api . user_timeline , screen_name = screen_name , count = 200 , tweet_mode = 'extended' ) . items (): #set ensure_ascii to true to encode Unicode in ascii. #',' conditional operator is part of the manual JSON parsing hack file . write (( ',' if tweet_count > 0 else '' ) + json . dumps ( status . _json , ensure_ascii = False , sort_keys = True , indent = 4 )) tweet_count += 1 if tweet_count % 200 == 0 : print ( \"Downloaded %d tweets\" % tweet_count ) except KeyboardInterrupt : print ( \"Process terminated.\" ) file . write ( ']}' ) print ( \"Downloaded %d tweets, Saved to ./results/timeline_tweets_ %s _ %s .json\" % ( tweet_count , screen_name , timestamp )) if __name__ == '__main__' : #pass in the username of the target account as argument in command prompt. if len ( sys . argv ) > 1 : search_query = sys . argv [ 1 ] get_timeline_tweets ( search_query ) By keyword The following two scripts will accumulate tweets based on one or several search queries. The first second example collects tweets from the existing pool of tweets up to about a week prior to running the script, while the second script opens a direct stream to filter incoming content based on the required keyword in real-time. Note If you have not formerly worked with the command prompt in Windows, inputting Japanese or other non-western characters (as a search query, for example) will likely result in gibberish. The easiest solution is to change the display font of the command prompt to one that contains all Unicode characters (right click on the title bar → settings → font → select a font such as MSゴシック). Historical Search: Twitter REST API Similar to our previous example, this script relies on Tweepy's Cursor pagination; collecting approximately 100 tweets per access call and writing these to local files as valid JSON. In order to both prevent crashes caused by a memory leak in Tweepy's pagination method and in order to keep the file size of our JSON files manageable (particularly trending topics might return up to millions of of results over the timespan of many hours running this script, taking up several gigabytes worth of disk space per single file), results are split over different files by an arbitrary number of tweets per file (defaulting to 10 000 tweets, set in line 11 ). Since this script runs until either the imposed API limit of 7 days is hit or the extent of all relevant tweets within the Twitter sample are collected (which could take up tens of hours depending on the popularity of the queries), this script can therefore be ceased mid-process by pressing Ctrl + C within the command prompt the script is currently running in. As is well-documented on Twitter API's documentation , using the Application only authentication instead of user authentication permits us a much higher amount of requests within a single window of 15 minutes; translating to a faster and more maintainable approach to data-mining historical tweets (roughly 100 tweets x 450 access calls per 15 minutes, for a total of roughly 180 000 tweets per hour ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 import tweepy import sys import json import os from datetime import datetime from pathlib import Path #Twitter API credentials consumer_key = '####' consumer_secret = '####' max_counter = 10001 #set to 0 to save all tweets to one file instead of chunking in pieces max_id = None #Optional: Until which ID? since_id = None #Optional: Since which ID? language = 'ja' #Optional: filtering by which language? Japanese? -> 'ja' search_query = 'China' quit = False def search_tweets ( sys_args ): global max_id global since_id global search_query global language if len ( sys_args ) > 1 : search_query = sys_args [ 1 ] if len ( sys_args ) > 2 : max_id = sys_args [ 2 ] if len ( sys_args ) > 3 : since_id = sys_args [ 3 ] if len ( sys_args ) > 4 : language = sys_args [ 4 ] auth = tweepy . AppAuthHandler ( consumer_key , consumer_secret ) api = tweepy . API ( auth , wait_on_rate_limit = True , wait_on_rate_limit_notify = True ) tweet_total_count = 0 timestamp = datetime . today () . strftime ( '%Y%m %d _%H%M%S' ) part = 0 tweet_count = None ; #create dir results if != exists Path ( \"./results/\" ) . mkdir ( parents = True , exist_ok = True ) while tweet_count != 0 and quit == False : part += 1 tweet_count = process_tweets ( api , search_query , timestamp , part ) tweet_total_count += tweet_count #To do: save last tweet ID in the registry in order to automatize with batch scripts print ( \"Finished process. Downloaded %d total tweets. Last tweet ID was %s \" % ( tweet_total_count , max_id )) def process_tweets ( api , search_query , timestamp , part ): global max_id global quit with open ( './results/search_tweets_ %s _ %s _part- %s .json' % ( search_query , timestamp , part ), mode = 'w' , encoding = \"utf-8\" ) as file : #bit of a hacky way to create valid JSON but easier on memory file . write ( '{\"objects\":[' ) tweet_count = 0 try : # cursor pagination, 100 is limit of returned tweets per access call for status in tweepy . Cursor ( api . search , q = search_query , count = 100 , tweet_mode = 'extended' , lang = language , since_id = since_id , max_id = max_id ) . items ( max_counter ): # tweepy takes max_id as first id to return: already have this so skip # (that's also why the max_counter is 10001 instead of 10k) if max_id != status . id_str : max_id = status . id_str #conditional operator is part of the manual JSON parsing hack file . write (( ',' if tweet_count > 0 else '' ) + json . dumps ( status . _json , ensure_ascii = False , sort_keys = True , indent = 4 )) tweet_count += 1 if tweet_count % 100 == 0 : print ( \"Downloaded %d tweets\" % tweet_count ) except KeyboardInterrupt : print ( \"Process terminated. Last tweet ID was %s \" % max_id ) quit = True except tweepy . TweepError : print ( \"Memory error. Last tweet ID was %s \" % max_id ) file . write ( ']}' ) #To do: we don't know if we've reached the last tweet until the tweepy API call, # which happens after creating a new JSON file. For now, this just removes the empty, final json file #optionally we could keep all objects, per chunk of 10k, #in memory and save at the end but this is way more memory-taxing if tweet_count > 0 : print ( \"Downloaded %d tweets, Saved to ./results/search_tweets_ %s _ %s _part- %s .json\" % ( tweet_count , search_query , timestamp , part )) else : os . remove ( \"./results/search_tweets_ %s _ %s _part- %s .json\" % ( search_query , timestamp , part )) #no need to loop if all tweets are saved in one file if max_counter == 0 : quit = True return tweet_count if __name__ == '__main__' : search_tweets ( sys . argv ) Note If we wish to resume this process starting from where we left off, we might do so using the max_id argument during our Search API access call (which can be set in line 12 ). Simply replacing None with the tweet ID of the last JSON object in our previously compiled JSON list of results will do the trick. Likewise, we could do the same, using since_id ( line 13 ), to collect tweets over a longer period of time (by taking the tweet ID of the first object in the last compiled JSON file as the entry point). If we intent to further limit the requested tweets to a particular language , we could also optionally filter our results by setting an language argument ( line 14 ) for our access call: e.g. \" language = 'ja' \". It is perfectly possible to filter by several different languages, as well (e.g. \" language = [ \"ja\" , \"en\" ] \"). Using a filter to limit tweets by location, however, is not recommended due to the limited amount of Twitter accounts that accurately add such information. 15 Finally, it might be worthwhile to look into the Twitter API documentation in regards to filtering incoming tweets. If we intend to filter out retweets, for example, we could further adjust our script ( line 63 ) by changing \" q = search_query \" to \" q = search_query + \" -filter:retweets\" \". Moreover, in order to search by multiple queries, we could just input several queries or use the logical operators OR in-between (e.g. \" py python_twitter_stream . py \"#corona #covid19\" \" for tweets containing both, or \" py python_twitter_search . py \"#corona OR #covid19\" \" for those containing either. Don't forget to enclose the query in [] brackets, however! Real-Time: Twitter Streaming API Unlike the previous two examples, the following script does not pull data from a RESTful API but creates a listener that is perpetually connected to the Twitter Streaming API (referred to as the firehose , limited to about ∼1% of incoming Twitter traffic). It signals to the API which queries to filter by, upon which the Twitter API pushes back all matching incoming tweets. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 import tweepy import json import sys from datetime import datetime from pathlib import Path #Twitter API credentials consumer_key = '####' consumer_secret = '####' access_key = '####' access_secret = '####' tweet_count = 0 search_query = '' timestamp = datetime . today () . strftime ( '%Y%m %d _%H%M%S' ) language = '' #Optional: filtering by which language? Japanese? -> 'ja' class StreamListener ( tweepy . StreamListener ): def on_data ( self , data ): try : with open ( './results/stream_tweets_ %s _ %s .json' % ( search_query , timestamp ), 'a' , encoding = \"utf-8\" ) as file : global tweet_count status = json . loads ( data ) #make sure the incoming data is tweet JSON, not rate related JSON if \"created_at\" in status : #prettifying json by parsing status string as json and then redumping ?? oof file . write (( ',' if tweet_count > 0 else '' ) + json . dumps ( status , ensure_ascii = False , sort_keys = True , indent = 4 )) tweet_count += 1 if tweet_count % 10 == 0 : print ( \"Downloaded %d tweets\" % tweet_count ) return True except BaseException as e : print ( \"Error on_data: %s \" % str ( e )) return True def on_error ( self , status ): print ( \"Error status on_error: %s \" % str ( status )) return True if __name__ == '__main__' : auth = tweepy . OAuthHandler ( consumer_key , consumer_secret ) auth . set_access_token ( access_key , access_secret ) api = tweepy . API ( auth , wait_on_rate_limit = True , wait_on_rate_limit_notify = True ) if len ( sys . argv ) > 1 : search_query = sys . argv [ 1 ] if len ( sys . argv ) > 2 : language = sys . argv [ 2 ] print ( \"Python stream started. Press ctrl-c to disconnect.\" ) #create dir results if != exists Path ( \"./results/\" ) . mkdir ( parents = True , exist_ok = True ) #very hacky way of creating valid JSON but easier on memory with open ( './results/stream_tweets_ %s _ %s .json' % ( search_query , timestamp ), 'w' , encoding = \"utf-8\" ) as file : file . write ( '{\"objects\":[' ) try : while True : StreamListener = StreamListener () stream = tweepy . Stream ( auth = api . auth , listener = StreamListener , tweet_mode = 'extended' ) stream . filter ( track = [ search_query ], languages = [ language ]) except KeyboardInterrupt as e : stream . disconnect () #disconnect the stream and stop streaming print ( \"Stream disconnected. Downloaded %d tweets, Saved to ./results/stream_tweets_ %s _ %s .json\" % ( tweet_count , search_query , timestamp )) with open ( './results/stream_tweets_ %s _ %s .json' % ( search_query , timestamp ), 'a' ) as file : file . write ( ']}' ) Note If the reader intents to limit the requested tweets to a particular language, they can impose a filter by editing the language variable ( line 16 ) to the intended language (e.g. \" language = 'ja' \"). 16 Data Processing By now, we should have one or several files containing raw tweet data formatted in JSON. opening one of those files with our text editor of choice permits us a closer look at the skeleton of such tweet objects. As seen in the example below, each single tweet contains a large amount of meta information (the Twitter Developer page offers a brief structural overview of each field in a JSON tweet object), not all of which might be relevant to us. The following fields are some that might be immediately relevant to us at this stage: \"created_at\" : \"UTC time when this tweet was created.\" \"id\" : \"The integer representation of the unique identifier for this tweet\" \"text\" : \"The actual UTF-8 text of the status update. \" \"lang\" : \" Nullable . When present, indicates a BCP 47 language identifier corresponding to the machine-detected language of the tweet text\" \"user\" → \"name\" : \"The name of the user, as they've defined it.\" \"user\" → \"screen_name\" : \"The screen name, handle, or alias that this user identifies themselves with. \" \"user\" → \"location\" : \" Nullable . The user-defined location for this account's profile. Not necessarily a location, nor machine-parseable. \" For this tutorial, the reader will mostly likely require only one or several elements of each tweet, such as the text, time-stamp, and user-name. It is generally best practice to save only the data required, and in that case the above scripts could have easily been edited to do so instead of returning unnecessary large JSON dumps. It could be argued, however, that due to the volatile state of data mining on Twitter, it is still beneficial to have an untainted and complete copy of the data we will be working with. Data seemingly unnecessary at first glance might turn out to be useful halfway through your writing process. For that reason, we will use the complete JSON dumps acquired through the methods above to build the processed sets necessary for our analysis. With preprocessing, this blog post thus actually refers to the process of removing irrelevant data and any other form of noise until we have obtained exactly what we need. Note During this phase, it is further worthwhile to think about how to deal with the other contextual data surrounding each tweet. What about the attached media (URLs, images, videos or sound bites)? How does the tweet fit within a larger thread of conversation? what should we know about the original of retweets or quotes? What about shortened URLs in retweets or quotes? Is the tweet still relevant to our research if it was mined because the full URL of a retweeted tweet contained a matching keyword? Especially for larger datasets, it is important to remove ‘noise' (e.g. irrelevant tweets and other data) to ensure more precise results. OpenRefine (optional) The final section of this article provides another Python script for preprocessing any obtained tweet data to something we can actually use for further analysis. Optionally, we might also install the data cleanup and transformation application OpenRefine . Although the most clear cut way to obtain data to one's own needs would be to alter the python scripts provided in this article (Python really is a fairly straightforward programming language), for those who are turned off by the prospect of editing code, the graphical interface of OpenRefine might offer some respite. Moreover, for those collecting data written in different writing systems (such as Japanese), OpenRefine's data cleanup functionality might turn out particularly useful when dealing with file conversation (e.g. to older versions of MS Excel). Again, it is recommend getting a bit acquainted with the application. Programming Historian (an open-source and open-access journal of peer-reviewed technical tutorials for those in the humanities) offers a useful introductory guide . Preprocessing with Python In essence, the script below is a simple parser that loads the content of the JSON files generated through the above methods and saves several relevant fields (such as the tweet text content, its hashtags and date of creation, as well as basic information pertaining the author) of each tweet in a new CSV file. 17 This script serves as a basic skeleton that can be edited to include or exclude desired fields, 18 or could be used for further preprocessing (such as cleaning the textual content of URLs or stop words). As of late 2017, Twitter doubled the allowed character size, which particularly benefits tweets written in Japanese. This script always takes the most complete data (such as the full_text or extended_tweet field), in case the tweet content is longer than 140 characters. Nevertheless, a retweet of a message that exceeds the 140 character length will still get cut off in the JSON Twitter's APIs return, potentially losing user mentions or hashtags in the process and significantly messing with our metrics. This is not optimal, as the Twitter API will still return search results based on keywords that might have been cut off. To remedy this, the script will reconstruct the retweet based on the content, hashtags and other entity information from the original tweet. Of final note is the addition of simple helper method for converting the time of creation (in standard UCT) to a ISO 8601 compliant format, another international standard for exchanging date/time-related data. In order to run the script below, we will again invoke the script using the command prompt. This script expects one argument: the name of our target document (excluding its .JSON file extension, e.g. \" py python_parse_tweet.py timeline_tweets_abeshinzo_20200510_211848 \"). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 import tweepy import sys import csv from datetime import date , datetime , timezone from pathlib import Path def parse_tweets ( file_name ): Path ( \"./results/\" ) . mkdir ( parents = True , exist_ok = True ) with open ( \"./results/ %s .json\" % file_name , mode = 'r' , encoding = \"utf-8\" ) as tweet_data : #newline parameter is necessary for correctly formatting newlines inside quoted fields with open ( './results/ %s .csv' % file_name , mode = 'w' , encoding = \"utf-8\" , newline = '' ) as file : writer = csv . writer ( file , dialect = 'excel' ) #table headers writer . writerow ([ \"tweet_id\" , \"text\" , \"hashtags\" , \"created_at\" , \"is_retweet\" , \"user_screen_name\" , \"user_description\" , \"user_friends_count\" , \"user_followers_count\" , \"user_total_tweets\" , \"user_created_at\" ]) tweets = json . load ( tweet_data ) for tweet in tweets [ \"objects\" ]: tweet_id = tweet [ \"id\" ] entities = tweet [ \"entities\" ] user = tweet [ \"user\" ] user_screen_name = user [ \"screen_name\" ] user_description = user [ \"description\" ] . strip () user_following_count = user [ \"friends_count\" ] user_followers_count = user [ \"followers_count\" ] user_total_tweets = user [ \"statuses_count\" ] user_created_at = string_to_dt ( user [ \"created_at\" ]) created_at = string_to_dt ( tweet [ \"created_at\" ]) retweet_count = tweet [ \"retweet_count\" ] is_retweet = ( \"retweeted_status\" in tweet ) hashtags = () if is_retweet == True : retweet = tweet [ \"retweeted_status\" ] retweet_original_id = retweet [ \"id\" ] re_entities = retweet [ \"entities\" ] text = \"RT @\" + entities [ \"user_mentions\" ][ 0 ][ \"screen_name\" ] + \": \" + ( retweet [ \"extended_tweet\" ][ \"full_text\" ] if \"extended_tweet\" in retweet else retweet [ \"full_text\" ] if \"full_text\" in retweet else retweet [ \"text\" ]) if 'hashtags' in re_entities : hashtags = ( hashtag [ \"text\" ] for hashtag in re_entities [ \"hashtags\" ]) else : text = ( tweet [ \"extended_tweet\" ][ \"full_text\" ] if \"extended_tweet\" in tweet else tweet [ \"full_text\" ] if \"full_text\" in tweet else tweet [ \"text\" ]) if 'hashtags' in entities : hashtags = ( hashtag [ \"text\" ] for hashtag in entities [ \"hashtags\" ]) #converts hashtag dict to comma-seperated string, can be commented out if original list is preferred hashtags = ', ' . join ( hashtags ) text = text . strip () writer . writerow ([ tweet_id , text , hashtags , created_at , is_retweet , user_screen_name , user_description , user_following_count , user_followers_count , user_total_tweets , user_created_at ]) print ( \"Finished. Saved to ./results/ %s _tweets.csv\" % ( file_name )) #converts Tweet date to ISO 8601 compliant string. Tweet timezones are standard UTC def string_to_dt ( time_string ): date_object = datetime . strptime ( time_string , ' %a %b %d %H:%M:%S %z %Y' ) return date_object . isoformat () if __name__ == '__main__' : #pass in the target filename without \"json\" as argument in command prompt. parse_tweets ( sys . argv [ 1 ]) Note Spreadsheets such as OpenLibre have strong CSV support. MS Excel versions prior to 2019, however, has some issues with handling newlines, which will most likely mess with our data structure.19 If working with such versions of Excel is a must, the easiest option for dealing with this problem is to import the CSV in OpenRefine (as seen in the screenshot above) and export as Excel file. Wait! There is more! This brief tutorial outlined the bare necessities to accumulate tweets, either in real time or historical, based either on user profiles or on particular keywords, using the Python scripting language and several working example scripts. Furthermore, this tutorial outlined a basic method for preprocessing those results into a viable dataset suitable to apply methods of quantitative analysis on. Using a preprocessed CSV generated through the steps taken above, the next guides in this series will cover existing tools and methods that may assist the reader in strengthening their topic of research with a Social Media Analysis angle. 20 A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 2: Basic Metrics & Graphs A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 3: Natural Language Processing With MeCab, Neologd and KH Coder A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 4: Natural Language Processing With MeCab, Neologd and NLTK A Quick Guide to Data-mining & (Textual) Analysis of (Japanese) Twitter Part 5: Advanced Metrics & Graphs On a final note, it is my aim to write tutorials like these in such a way that they provide enough detail and (technical) information on the applied methodology to be useful in extended contexts, while still being accessible to less IT-savvy students. If anything is unclear, however, please do not hesitate to leave questions in the comment section below. Still image from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under a Fair Use doctrine. ↩ Moreover, the majority of general tutorials found online relied on dated methods and did not take into account recent Twitter changes such as extended length of tweets or quotes. ↩ Unfortunately little information is available concerning to how Twitter samples this data. While Twitter, by design, has a particular sociocultural demographic that might not not be fully representative of a greater offline public sphere, even conclusions regarding Twitter usage itself cannot in good faith be called scientifically proof as long as there is not sufficient knowledge on the way Twitter handles its sampling methods. ↩ Several attempts have been made to increase sample size and accuracy. One of such, focusing on building a dataset representative of the Japanese Twitter public sphere, is: Hino, Airo, and Robert A. Fahey . 2019. ‘Representing the Twittersphere: Archiving a Representative Sample of Twitter Data under Resource Constraints'. International Journal of Information Management 48 (October): 175–84. https://doi.org/10.1016/j.ijinfomgt.2019.01.019. ↩ This blog recommends Automate the Boring Stuff and the interactive Computer Science Circles or its video series Python from Scratch . Earth Data Science has great tutorials as well. ↩ PATH is an environmental variable; doing this will allow us to run the Python compiler from command line without having to manually locate its executable. ↩ This tutorial was written with PC users in mind but won't differ that much for other platforms. ↩ Or, I mean, whatever method you personally prefer. ¯\\_(ツ)_/¯. ↩ If you get a message that pip is not a recognized command, you will either have to manually install pip or add the path of your existing pip installation to your PATH variable. ↩ I personally use Sublime Text 3 and Atom looks pretty great as well, but for the sake our tutorial, even notepad is sufficient ↩ A standard for cross-platform changing of data. Data and its meta-data are represented by key-value pairs: e.g. \" { \"first_name\" : \"Stevie\" , \"last_name\" : \"Poppe\" } \" . ↩ Technically, this will return a file of comma-separated JSON objects, which is not 100% compliant but works either way and is less memory intensive than creating a massive JSON array, especially with the Streaming API. ↩ Moreover, optionally adding b as part of the access mode argument in the open class indicates that the script should write in binary mode as opposite to text mode, which is uncommon in such scripts, but decodes already escaped Unicode characters. In this case, it is necessary to encode our JSON dump to UTF8 → by calling the method . encode ( \"utf8\" ) . ↩ The only other option to access historical tweets of someone's timeline beyond the initial 3200 tweets, is to resort to text scraping (e.g. using Javascript to simulate scrolling down and python to scrape the AJAX-loaded tweets). If there's a demand for an in-depth tutorial I will add an appendix blog for that eventually . ↩ Language uses BCP 47 language identifiers. Language of each tweet is machine-detected and not 100% accurate. Read more . ↩ Again, the Twitter Stream API has several limitations in regards to the amount of tweets returned per second. Neither does it allow more than one established stream connection at one time. The above script will be sufficient to retrieve a sizable dataset but unless we have access to the paid full firehose , there are no methods available to guarantee an exhaustive collection. ↩ CSV is another open data exchange format for storing records of data, with fields separated by a comma. It might be easier to visualize the format as a kind of Excel spreadsheet, and indeed, spreadsheet applications such as OpenLibre or Excel 2019 offer quite strong integration of the CSV format. ↩ Running this script on a dataset of tweets by a single account will produce a lot of unnecessarily repeated user data, for example. ↩ Even then, Excel has some issues with importing (CSV) files that contain long numerals such as tweet IDs: only the first 15 significant digits are interpreted, displaying the remaining digits as 0. The best solution would be to thus select Text for the relevant column formatting upon importing the CSV data. ↩ For two excellent and recent English language papers which utilize a form of quantitative analysis of Japanese tweets in order to strengthen their main arguments, see: Tamara Fuchs & Fabian Schäfer (2020) : Normalizing misogyny: hate speech and verbal abuse of female politicians on Japanese Twitter, Japan Forum, DOI: 10.1080/09555803.2019.1687564 , and Fabian Schäfer, Stefan Evert, and Philipp Heinrich (2017) : Japan's 2014 General Election: Political Bots, Right-Wing Internet Activism, and Prime Minister Shinzō Abe's Hidden Nationalist Agenda, Big Data. 294-309. DOI: 10.1089/big.2017.0049 . ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2020/04/a-quick-guide-to-data-mining-textual-analysis-of-japanese-twitter/","loc":"https://steviepoppe.net/blog/2020/04/a-quick-guide-to-data-mining-textual-analysis-of-japanese-twitter/"},{"title":"Travels: Two Weeks in Mainland China (Changsha, Yunnan, Xi'an)","text":"Friday, 16 August 2019, approximately 2PM and approximately 2 hours of sleep. Calling that day a somewhat surreal and emotionally taxing day, might, in retrospect, be putting it mildly. I had just spent the night proofreading my graduate thesis for the thousandth time, buried in anxiety that a minor typo in a footnote still buried somewhere on the tenth page of my appendix would destroy my chances of graduation. Within the timespan of barely an hour, I retrieved the print copies of said thesis at a local print shop, handed them in at my university, shared a cup of coffee with some of my classmates—by now, my good friends—at our local coffeehouse hangout, checked my watch, and rushed towards the train-station. I had a flight to China to catch. Despite the unwarranted stress, my five years as both undergraduate and graduate student at KU Leuven—enrolling at this university must have been the single most transformative decision I have ever taken—were now definitely at an end. To ‘celebrate', I would now partake in a graduation trip to mainland China, accompanied by some of my closest friends while living in Japan. Both literally and figuratively a graduation trip, this trip was simultaneously a finale to my student life at KU Leuven, to my period of living in Japan, and to my youth in general. A finale written by a director of slice-of-life situational comedy, that is; while contemplating all the above at the airport, a stoic voice announced that my flight was canceled. Itinerary My initial plan was to take the plane to Hong Kong, spend the afternoon hanging out with a friend in Tsim Sha Tsui, and take a late evening train straight to Changsha, my initial destination. Fortunately, after my initial flight was canceled and making my connection in time seemed increasingly unlikely, I received a new itinerary to Changsha through connecting flights in the Netherlands and Shanghai. I would have to wait approximately seven more hours in the airport of Amsterdam, but I've had worse experiences and I could just take a nap after checking in. Or so I thought. A couple of hours afterwards I was informed that my flight to Shanghai was canceled. Instead, I was offered a direct flight to Changsha from Frankfurt, Germany the next day as well as a free stay in a nearby hotel. Oh well. Heading towards the exit, I even met some familiar faces. For several weeks during the end of June and early August, I assisted a team of Kyoto University scholars in digitizing Edo-era Ukiyo-e woodblock prints at the Brussels Royal Museum of Arts. Turns out they took a detour in the Netherlands and were heading to the United States from Amsterdam Airport. Although at the time of boarding it turned out there was some data mismatch with my rescheduled flight, it was, at least, sorted out in time to like, actually let me board. The flight to Changsha was quite pleasant, though, and I even ended up playing a couple of rounds of Xiangqi (\"Chinese chess\") with the guy sitting next to me, a Chinese student living in Germany. 2 Immigration and passport control was no doubt by the far the most strict I have ever experienced, but generally speaking, there were no further hiccups and, 3 after meeting up with friends at the airport our trip was good to go. Changsha (长沙市) Calling Changsha, the capital of Hunan (湖南) Province, a tourist-friendly destination is a bit of stretch, and for most foreign tourists, the city and its airport will serve primarily as layover. Changsha downtown in particular has a bit of an industrious, developing atmosphere. This was particularly obvious when our friend guided us to a local private cinema place; for a handful of yuen per person (the equivalent of about $2), we had access to a room with a large reclining sofa, HD beamer and a large selection of movies, which was quite comfortable where it not for a handful of cockroaches in our vicinity. The building itself had seen better days, and aside from one employee it felt as if we were the only people there. Or when we, deciding on Karaoke, found a nearby place in another worn down building—the guard at the front fast asleep—, took the elevator to one of the top floors and ended up at what appeared to be an intense fight. We heard glass break as soon as the elevator doors opened, and promptly entered a strange scene of drunk, shirtless men shouting and waving billiard cues around, someone hiding behind a counter, and dark smoke rising from the corner. For a second everyone stared at us, most likely wondering what a white, western tourist was doing at 2AM in a downtown karaoke, and we solemnly took that silence as a cue to make our departure. Such initial impressions aside (that wouldn't apply to the usual foreign tourist either way), Changsha had its share of vibrant urban life with some lovely, scenic hot spots such as the surroundings of Hunan University (near a massive statue of Mao Zedong—Mao was born in a nearby village), Having lived in Japan, even in an international metropolis such as Tokyo, I was more accustomed to standing out and being either approached, avoided or stared at due to being foreign-looking. Even then, I was not prepared for how much elevated this would be during my trip and stay in Changsha in particular; I can't recall how many times I have heard the terms 外国人 (wàiguórén) or 老外 (lǎowài), both more or less meaning foreigner, or was cautiously approached by children wanting to practice what they had learned at their English classes. On a side note, me and friends communicate in Japanese, which, while culturally possibly insensitive, was unavoidable. On one of the scenic spots, a young and male group of local Chinese tourists were making comments about us akin to \"Why is that Japanese girl with a white guy and why does he speak Japanese\" and \"Tss, Japanese women are so cheap\". Oh - and this counts for other regions in China as well -, I highly recommend looking into local food delivery and uber-style transport services; they are both reliant and cost-effective, and for non-locals a more ideal way of getting around rather than figuring out each local public transport system. Yunnan (云南省) Our next destination was Kunming, the capital of Yunnan. One of the first things that hit on arrival me was the drastic difference in climate compared to Changsha. Although separated only by one province in-between, the temperature in Changsha was at that time of the year unbearably hot to the point we could barely engage in outdoor activities during day-time, whereas the high altitude of Kunming and Yunnan in general provided a very pleasant, temperate climate. Moreover, despite stereotypes of air pollution in China, the air around Kunming and the other places of Yunnan we visited was surprisingly clean, often with crystal clear blue skies befitting the time of the year. Kunming (昆明市) Some of the central areas of Kunming were particularly vibrant, with a lovely youthful vibrant vibe. My friends introduced me to their university, famed for its focus on Chinese ethnic minorities, anthropology and cultural studies, as well as the surrounding parks, the nearby shopping districts Wenhua Alley and Qianju street, Daguan Park and Guandu Old Town. Because of the lovely weather, the park were filled with people enjoying a walk or performing music. Mahjong tables drew formidable crowds of elderly people commenting on in progress games, and other parts of the park drew rows of people stretching, dancing an exercising. The spicy dishes recommended to me in Changsha were right up my alley but the food I've eaten here was personally at an entirely different level. The mushroom hotpot was by far one of the best things I have ever eaten, and I'm not overreacting when I say I would definitely consider a detour back to Kunming next time I'm in China, purely to enjoy its taste again. An interesting sight I would've liked to capture on photo was the juxtaposition of a young, fashionable girl, taking a selfie in front of a historical Socialist-era statue celebrating the working class, situated right in front of feudal-era Qing dynasty architecture: a fascinating micro-representation of a layered society with clear elements of three distinctly different modes of production. 『向前一小步、文明一大步』 - Neil Armstrong Famous Toilet Proverb On a side note, this plate found around public restrooms was quite comical; translating to something akin to \"One small step for man, one giant leap forward towards civilization\", a polite way of asking men to stand closer to the urinals. Lijiang (丽江市) Lijiang, the nearby Dali and Shangri-la are historical towns near the border of Sichuan and Tibet, popular among in-land tourists for the natural beauty of its surrounding mountains and the architectural traditions of the various ethnic minorities inhabiting these regions. Without exaggeration the old town of Lijiang and the hotel we were staying at are among the most beautiful places I've visited in my life. The historical center is particularly lively even in the late evening, with daily bonfires and dancing; but the nearby Black Dragon Pool made for a perfect breathing space. One particular height of our stay was an afternoon of horse riding along the Jade Dragon Snow Mountain. One thing I must admit is that the high altitude of the region takes a while to get used to and is fairly draining; leaving us absolutely exhausted after a full day out and a long climb up back to our hotel. Upon arrival at the airport we were informed that our flight would be delayed by about five hours. While several people were noticeably aggravated, others took the news more calmly: one guy took out his jump rope and started exercising in the middle of the hallway. Another guy, a local musician practicing acoustic guitar, upon noticing me walked over and asked me to join him. To be fair, it was clear he was more interested in taking pictures together to post on social media, but it was a nice way of passing and he helped me out afterwards by sharing his 4G hot spot when I needed to make a phone call (passengers would receive a small financial compensation for the delay, which was to be deposited on our personal WePay account, which I as a foreigner did not have). Because of our late arrival in Xi'an I was unable to leave the airport and find a place to stay so I used part of the financial compensation to rent a small cabin with a couch at the airport instead. Xi'an, Shaanxi (西安市，陕西省) Xi'an, the capital of Shaanxi Province, is famed worldwide for its rich history as the historic capital of China for about a millennium of its existence, and for its international importance in the era of the Silk Road. Of particular fame are the Terracotta Warriors, an in-progress excavation site containing an impressive army of around 2,000 incredibly detailed and life-like statues of armed soldiers and horses. The site has become one of China's major tourist attractions and a main staple of guided tours throughout the country. Aside from a couple of exceptions in Lijiang, it was my first time being confronted with other western tourists. Biangbiang noodles, a famous regional dish of Shaanxi Province, has some notoriety among learners of Chinese characters due to its highly complex character with 58 strokes, that as of 2019 has not yet been encoded in Unicode. Although none of my Chinese friends actually like Biangbiang noodles, it's something I've been wanting to try for a while, and thus entered one of plenty of local eateries that decorated their storefront with those characters. The noodles itself were interesting; an appearance I hadn't encountered before. A highly affordable and decent meal, although I'm not particularly fond of bok choy. More amusing was my communication with the waitress. Because I was by myself at the time and could not understand what she was saying, the waitress started typing something on what I assumed was an automatic translation app on her smartphone. What I was shown, however, was a text editor with the sentences she wrote down in Chinese characters as-is. Based on my limited study of the language and recognition of characters I recognized as a Japanese speaker, we were able to have a short conversation. Still not sure if she thought it normal for western foreigners to read Chinese, or if she just assumed I might have been a foreign student starting out. My flight back included a final day in Hong Kong. Since I was unable to meet my friend there after a failed first attempt, now would have been a good opportunity. Upon arrival in Hong Kong the afternoon a day ahead of departure, the ongoing Hong Kong protests reached new heights that left either of us unable to meet each other, with clusters of protesting right around the airport. Moreover, my credit card was for an unforeseen reason blocked and I could not withdraw cash from an ATM, leaving me once again stranded in in airport, forced to spend the night there. At least my flight wasn't canceled this time, though. ¯\\(ツ)_/¯ (Oh, and while it's bit of a cheap shot to laugh at such honest mistakes, the boring / boarding typo on the automatic check-in system was quite perfect. While my boarding pass might have been boring, the trip and everything it represented wasn't, for sure.) Music While traveling, I make a habit out of listening to local radio stations or asking recommendations to people I meet in hostels or bars, as my own form of personal souvenir. This time I have grown particularly fond of folk musician Hongyu Chen (陈鸿宇) and the singer-songwriter Hua Chenyu, whose experimental composition Cancer gives me goosebumps regardless how many times I listen to it. Conclusion Not so much a real tourist trip through China as a short and wholesome visit to some people dear to me, I nevertheless thoroughly enjoyed my stay and was glad to experience firsthand some elements of daily life in a nation that is due to various socio-political an economical reasons still highly prone to a wide variety of negative stereotypes and misconceptions abroad. Thanks to my limited knowledge of the Chinese language, as well as the assistance of my Chinese friends, I was generally able to get by quite well as well. Due to a rise in local tourism, accessibility has reportedly increased immensely, but I must admit that for foreign travelers, especially those traveling alone and lacking Chinese language skills, going \"off the beaten path\" will be quite a challenge and require sufficient planning ahead. Obviously there are highly contentious political elements that could and should be discussed concerning China. This blog post is however not the appropriate space for that; my trip there was intended primarily as a graduation trip in the company of some of my closest friends during my period as exchange student in Japan, and that I, dispute the hiccups, enjoyed immensely. I definitely look forward to return for a longer period of time, China is immense with a highly diverse ecosystem, natural beauty and rich history that I would love to further explore, perhaps next time while taking on the role of a more traditional tourist. Gallery Gallery Changsha | Kunming | Lijiang | Xi'an Lijiang by Stevie Poppe ( https://flic.kr/p/2fMA1vY - CC BY-SA 2.0) ↩ Or, well, get utterly destroyed, that is. I've won the occasional round playing with my friends in Japan, but in hindsight I wonder if they let me win out of pity. ↩ By the way, the process of applying for a single entry 30-day tourist visa was surprisingly easy as well, given that you have a local friend able to vouch for you through an invitation letter (there are plenty of templates for that online). I did make the mistake of providing only the front-side of my friend's passport, which could not be accepted. While trying to submit my application, I couldn't reach my friend for a new scan, but fortunately another friend saved my skin by skipping class early and providing a complete invitation letter—we communicate in Japanese so that was a bit of a weird phone call in the Chinese embassy, though. ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2019/09/travels-two-weeks-in-mainland-china-changsha-yunnan-xian/","loc":"https://steviepoppe.net/blog/2019/09/travels-two-weeks-in-mainland-china-changsha-yunnan-xian/"},{"title":"Several tips for exchange students heading to Japan / Tokyo","text":"My home university's student circle asked me for some tips for new exchange students coming to Japan and after living in Tokyo, Japan for over 18 months I learned some things that might be useful for future exchange students, often glanced over in similar lists. Although this list is a perpetual work-in-progress, I hope it can at least offer some concrete, directly applicable tips. Shopping Get an Amazon.co.jp Prime account I'm reluctant for promoting a company with an employment situation as terrible as Amazon. Nevertheless, students are legible for a year-long trial of Amazon Prime Student which might be quite useful for exchange students on a budget. The prime reason to do so would be free expedited shipping, but some other benefits include streaming through Prime Video and free selection of Kindle novels. Personally, I found amazon.co.jp useful for difficult to find research material and access to some other second-hand sellers. Speaking of which… Use Mercari Mercari (メルカリ) is a widely used e-commerce application popular for second-hand sales and small time mom-and-pop independent stores. I used it for buying my TV and some rare-to-find records. It's common to haggle on the price and you pay through convenience stores. Get used to customer cards A lot of stores have customer cards, usually freely obtainable and immediately applicable. A card at my OK Supermarket cost 100 yen and saved me 3% on all my groceries. Places like Big Camera, the must-visit dirt-cheap second hand store Book-Off, Tully's Coffee, etc have free customer cards. Some stores moved on to smart-phone applications, like the shoes-market chain ABC Market. Services (Manga) cafes Although I loved my apartment and spent a lot of time at my university lab, I generally felt the most productive studying or reading in bakeries or the omnipresent coffee-shops like Tully's and Renoir. Perhaps not as romantic, but another cheap option might be studying at McDonalds chains equipped with electric outlets and ordering a cheap coffee. Personally, I grew fond of manga cafes and particularly of the Kaikatsu Club (快活クラブ) chain. Students get steep discounts, private booths are comfortable and equipped with both blankets and pillows for a quick nap, as well as a computer and additional screen I could attach to my laptop, and sufficient electrical outlets. All you can drink soft-ice, hot drinks and soft drinks are included, and food-service is relatively decent for the price. I confess to having spent the night there more than once after missing my last train, and with their student discounts its generally a cheaper and more comfortable option over spending several hours at coffee shops. Convenience stores 24/7 convenience stores offer decent food, fresh coffee, ATMs, printers, public restrooms and usually even a place to sit. One can purchase tickets for concerts, events, attraction parks like Disney Land, etc at the ticket machines there. Bills like your health insurance can be paid there, as well as most purchases on-line. Karaoke Karaoke is a common staple of the foreign exchange student experience in Japan. For those staying in Tokyo, I personally recommend the manekineko (まねきねこ) chain. Great price/quality balance (コスパ最高) including all-you-eat soft-ice, and with student discount its actually the cheapest one I've been to within central Tokyo. Food It would be a waste to not enjoy the wide range of different food available in Japan. Even if with a vegetarian lifestyle or on a budget it is definitely possible to explore various options; not just within the domain of more traditional Japanese cuisine ( washoku 和食) but all over the world. I definitely recommend popping into random places, but I still have my go-to places. By the way, just a tip but aside from being dragged to places by friends, I've often relied on the tabelog (食べログ) application or even just googling things like \"best bakery ikebukuro\". Izakayas Seriously, just try out a bunch of different izayakayas , even if it means taking the elevator to a random 7 th floor. Get out of your comfort zone. When on a steep budget, however, Torikizoku is by far the most famous izakaya chain in Japan, and a good, cheap choice for having a beer and some snacks with friends. Chinese While it's definitely worth visiting the historic Chinatown in Yokahama, the most authentic and affordable experience for Chinese cuisine in Tokyo is centered around Ikebukuro, a region by some considered as a modern Chinatown. Has many highly affordable dining options around, with popular selections like Szechuan dishes (Mapo Tofu is a personal favorite), Hainan Chicken, xiaolongbao (小笼包 or ショーロンポー, shanghai style buns), etc. A household name amongst mainland Chinese people is haidilao (海底捞), a famous Chinese chain of hotpot restaurants. There's one in Shinjuku (kabukicho) and in Ikebukuro. Go there during lunch or after 10PM for a 30% discount. Korean While Korean restaurants are widely available, those in the Korean neighborhood (新大久保 Shin-Okubo) are most frequented by Korean natives. I personally really like Shinchan (辛ちゃん) for its Korean-style fried chicken, Shijan Dakgalbi (市場ダッカルビ) for its Cheese Dak galbi (an instagram hype amongst high-school students back in 2017), and Saemaeul Sikdang (セマウル食堂) for their Kimchi Jigae. Other dishes to look out for are Samgyeopsal (삼겹살), Bulgogi (불고기) and Bibimbap (비빔밥). If you like alcohol, don't forget to try the Korean sparkling rice wine makkoli (막걸리)! Desserts I like pancakes. A Happy Pancake (幸せのパンケーキ) has incredibly fuwafuwa pancakes and are definitely worth the trip to Ikebukuro or Kichijoji. Sushi Eating at the counter of an old-school Sushi-bar and talking to the chef ( itamae , 板前) is a fun tradition losing popularity amongst young students. I've had many interesting conversations with both itamae and other patrons, and more than once received something on the house (サービス). Worth trying at least once. School Zemi -style classes If you have some freedom in picking your own courses, it's definitely worth picking up several zemi ゼミ(ナール). Valuable from a didactic standpoint, but due the interactive elements also a great opportunity to get to know your professors and classmates. If you're in luck you'll be able to participate in gashuku 合宿 and nomikai 飲み会 as well. Circle activities If you're able to converse in Japanese to a certain extent, I honestly think you'd be missing out skipping on the opportunity of joining at least one circle as exchange student. Traditionally, circles actively recruit at the start of the first semester, so definitely walk by the different stands around your campus and don't be afraid of approaching the stand holders. You'll probably be invited to join their Line groups and participate in newcomer events to get a feel of the group and its activities before making a decision. Exchange students are often stuck in an bubble of other exchange-students: being surrounded by mostly other exchange students, as well as the occasional Japanese student interested in internationalization and/or practicing foreign languages. Circles and zemi are an effective way of expanding your friend circle at your university. Daily life Keep a diary and/or blog Doesn't have to be particularly long, just the essence of what you did each day. Maybe go more in-depth when you've experienced something memorable. You'll be grateful to have something substantive to look back upon after finishing your exchange. Don't be afraid to try out new things but don't forget to stick to your favorites Sounds contradictory but I both recommend trying out as many activities and places around, such as restaurants, coffee-shops, preferred clothing stores, etc; and building a network of places you frequent. It's a fun way to build a human connection with the local staff. Baito After arrival in the airport it's worth immediately applying for a work-permit as well. It's quite easy to find student jobs in Japan and aside from some extra dispensable income, its a great way to increase your network and learn about cultural habits. Teaching English is a classic one, but definitely look around your university as well. Consume media I assume most studying Japanese or going on exchange to Japan appreciate Japanese media such as video games, movies, series, manga, anime or the music scene. Its worth getting more in-depth in a certain field and having a general feel on most mainstream fields as well. I've often been inquired on my favorite Japanese geinin 芸人 (Naomi Watanabe, Buruzon Chiemi, Matsuko), joyu 女優 (Sakura Ando, Fumi Nikaido, Ai Hashimoto) and haiyu 俳優 (Abe Hiroshi), and even been asked opinions on male-idol label Johnny's (ジャニーズ) and how I feel about Arashi 嵐. Having some knowledge on these topics helps daily conversation, or you might end up stuck discussing cultural differences between Japan and your country for the majority of your stay, which would get boring pretty fast. You'll have more luck discussing morning drama's ( asadora , 朝ドラ) and variety shows (good ways to improve listening skills by the way, I recommend 月曜の夜更かし, but itte-Q and 水曜日のダウンタウン are fun as well) than you'll have discussing Game of Thrones. Social media The most popular form of social media remains Twitter, although Instagram is rapidly catching up and TikTok is the hot newcomer for the youth. These are crucial for keeping up with artists or people of note. As for communication tools, Line is the most popular messaging tool amongst Japanese, but if you have some Chinese friends it's definitely useful to install WeChat as well. Another option, more popular amongst South Koreans, is Kakaotalk. [More to come, maybe ] Photograph of Chuo University, Tama Campus at night. ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2019/05/several-tips-for-exchange-students-living-in-japan-tokyo/","loc":"https://steviepoppe.net/blog/2019/05/several-tips-for-exchange-students-living-in-japan-tokyo/"},{"title":"Japanese E-books, vocab-mining, DRM and copyright law","text":"The E-book industry in Japan is gaining momentum. Both contemporary releases and more popular classics are seeing an increase in digital publication alongside physical distribution. Through their kindle service Amazon.co.jp has a particularly strong hold on this market, and their extensive catalog could offer good news for any intermediate or advanced Japanese language learner. It is easier to look up new vocabulary from digital media than it is to look something up from print media, and both physical Kindles as Kindle for Windows are surprisingly feature-rich, including in-built dictionaries and flashcard options. In prior blogs I've described my method of reading digital texts as HTML files through a web-browser; using pop-up dictionaries such as Rikaisama or Yomichan in combination with flashcard application Anki for ‘vocab mining' 2 native materials. 3 This blog describes options of combining that method with kindle E-books, as well summing up several other alternative sources of E-books. Conclusively it is possible to efficiently utilize the Japanese kindle market for study purposes, but doing requires a bit of technical prowess and borders on technical illegality. Amazon Kindle For the past two years or so I've often relied on Amazon.co.jp's Kindle service for both academic purposes and as leisure, especially while living in Japan. 4 Their selection of Japanese language material is immense, with Kodansha-published E-books alone accounting for over 40.000 titles in their library. Having relied on above-mentioned study method for a while now I looked into my options of doing so with Kindle E-books as well. Through Browsers The first option would be to open Kindle E-books with my browser and use Rikaisama or Yomichan to import vocabulary into Anki. When purchasing an E-book on Amazon, one specifically purchases the right to read the E-book and download an amazon proprietary file through the Kindle software (the .AZW format). This proprietary format is practically identical to the .MOBI format (which again is basically a more highly compressed HTML file, specifically using the Ebook HTML syntax) with additional DRM-protection on commercial releases as attempt to counter copyright infringement. Both Chrome and Firefox have plug-ins for parsing those two formats, but these plug-ins are frankly not sufficient for our purposes, and for obvious reasons don't support files with DRM. One option would be removing the DRM through software and converting the Kindle .AZW format to a ZHTML format, which is quite is literally a compressed (zipped-up) HTML format; unzipping the .ZHTML file and opening the resulting HTML page with your browser of choice is all it takes to achieve our goal of simultaneously reading and vocab Kindle E-books with Yomichan or Rikaisama. Converting an E-book is a simple process done with the popular open-source tool Calibre . Calibre supports third-party plug-ins, including one used to bypass E-book DRM. While the process would thus be as simple as locating your legally purchased Kindle E-book in .AZW format with Calibre, converting the book to ZHTML, extracting the ZHTML and opening the content with a browser, 5 I had some interest in the legal aspects of bypassing DRM and converting E-books for such fair-use purposes. International copyright law in the digital age is an ever-changing field, and I am not a legal expert by any stretch, but I will briefly summarize my understanding of the current-day situation. Legality Kindle E-books are protected by DRM (Digital Rights Management), a form of Technological Protection Mechanisms (TPM) to prevent copyright infringement on a digital level (i.e. piracy). DRM is a controversial topic and while some industries, such as the music industry, are backing away from DRM (no doubt due a rise in streaming services), DRM mechanisms are still inherent to the E-book industry. Regardless of one's ethical principles, the legality of fair use DRM stripping (i.e. place shifting as means of back-up, or format shifting as described in this article) should be touched upon. If one has for example a large, expensive library of DRM-protected E-books and its distributor goes out-of-service, or the tools to read those E-books with are no longer supported, what options remain for the customer to enjoy their legally-obtained works? 6 Technically, removal of digital copy protection mechanisms such as DRM fall under anti-circumvention laws. Now, laws might differ depending on the region, and although laws could directly forbid the removal of digital copy protection mechanisms, others might contradictory permit shifting of formats or creating back-ups for personal use. 7 Nevertheless, most countries today are members of the World Intellectual Property Organization ( WIPO ) and adopt the World Intellectual Property Organization Copyright Treaty (WIPO Copyright Treaty or WCT ). While specific implementations of the WCT differ on its member states, WIPO (a United Nations agency) explicitly states the general act of DRM-removal as illegal. USA The anti-circumvention of the WCT-inspired DCMA for example, as upheld in the United States, explicitly states that while circumvention of copy-control measures 8 is not illegal, circumvention of access-control measures very much is. The latter (quite literally measures to control which platform can access the media) applies to E-books as well, and thus format-shifting, regardless of fair-use intent , 9 does fall under this category. Unfortunately, the reality is that most DRM implementations contain both measures. Furthermore a controversial 1999 ruling held the very act of linking to pages hosting circumvention software as illegal trafficking. There is however a more recent legal case concerning this topic, in which an E-book store, contractually obliged to sell E-books with DRM, was sued after closing down and disclosing information on how to remove this DRM. Although they were sued for inducement of contributory infringement, a federal judge in New York ruled that A) Abbey House did not induce infringement as there was no factual direct knowledge of infringement, and B) the infringement referred to was of illegal redistribution, not of DRM removal. 10 EU The European Union implements WCT through the Copyright Directive (also known as the InfoSoc Directive ), which had a controversial update just weeks before this post (26 March 2019). 11 DRM falls under the 2001-era Article 6(3) and Article 7(2), and while member states of the European Union have again different interpretations and implementations of these articles, there are little to no practical exceptions protecting the individual in the actual Copyright Directive. 12 ‘ 13 Japan Japan too implements WCT as WIPO member. Although copying media for back-up purposes itself is legal, it is not just the illegal distribution of copyrighted material on-line, but downloading of music and movies that are considered offenses under criminal law as well. 14 ‘ 15 As for Technological Protection Measures, the topic of DRM in particular falls under Copyright Act Article 2.1 Clause 20 (著作権法2条1項20号) and Article 2 Section 7 of the Unfair Competition Prevention Act (不正競争防止法2条7項). 16 Circumvention (「技術的保護手段の回避」) of both copy-control measures and access-control measures are from a legal point of view criminal offenses. On a final note, when someone purchases media such as Kindle E-books through a distributor such as Amazon, they gain limited rights of access, just as a distributor has limited rights of distribution. These rights are, unlike digital media in physical formats (such as CDs), determined through a contract with the distributor. Regardless of the applicable law, removal of DRM is foremost a breach of the Terms of Service one signs when signing up with Amazon and the Terms of Service does generally uphold in court. TL;DR To throw in my own two cents, I believe the technical implementations of anti-circumvention law are outdated and do not accurately represent the role of digital data in our lives. Furthermore these laws imply circumvention of copyrighted protection to be done solely for the purpose of illegal redistribution. Having said that, I also sincerely doubt anyone to actually face legal consequences for such benign personal fair-use purposes such as converting a Japanese language E-book to an HTML format for vocab-mining purposes (as implied in this article), and as of this article's date there are no such legal precedences (not to mention the lack of traceability when done in private spheres). From a legal perspective however, bypassing DRM is illegal in WIPO countries and punishable by the legal framework of the country one resides in, with at least the possibility of termination of services if detected. Through the Kindle Application With above-mentioned issues of legality, detractors of that that method could opt for another method: creating flashcards based on using your Kindle hardware and/or the Kindle application itself. While reading a novel, one can look up words using in-built dictionaries (e.g. J - E, or J - J). These \"look ups\" will be saved in a vocabulary file and in case you're using Kindle hardware, synced to your desktop Kindle application: a vocab.db file. Using the service on https://fluentcards.com/kindle , one can then convert the vocab.db file client-side and import the result into Anki. To add example sentences, you'd be required to convert the Anki note-type to the one we've been using prior and use the bulk-edit feature of the Example Sentence Anki plug-in. Unfortunately adding audio is a bit more complicated. One could rely on text-to-speech services like the Anki plug-in AwesomeTTS. Another option would be to bulk-edit cards to import audio from Jpod101's database, just like Rikaisama and Yomichan do. Unfortunately there is no such plug-in yet. I will look into it myself at a later time. Geo-blocking For those not currently residing in Japan, I should also mention that there are some (legal) limitations to purchasing Kindle E-books outside of Japan. As of yet, Amazon has due licensing and logistical reasons not yet opened its E-book market to the international E-market, and similar to how video-streaming services as Netflix and Prime Video block content based on region ( geo-blocking ), kindle has certain technical limitations as well. Those without an official address in Japan would require at minimum an Amazon.co.jp account with existing Japanese address, and kindle E-book downloads abroad, while not blocked, are at the very least limited in frequency one can purchase Interestingly, Japanese E-book competitors https://honto.jp/ and https://www.ebookjapan.jp/ebj/ , while obviously enforcing DRM, do not employ such geo-restrictions. While outside the scope of this article, googling these topics reveal many threads on bypassing geo-blocks to purchase digital contents abroad and offers an interesting case-study within the concept of economics of digitization. The European Commission has voted in favor of a Digital Single Market for countries already belong to the European Common Market , but as far as I'm aware the concept of an international digital single market crossing existing market boundaries seems unlikely for now. Aozora Several years ago it was still a challenge finding native materials on-line. One option remains Aozora — the Japanese answer to the Project Gutenberg , which freely hosts tons of public-domain books online. 18 ‘ 19 Due the nature of public domain literature, these might not be that accessible or entertaining even for the casual intermediate-level learner; but regardless there are some absolute classics such as Natsume Soseki's Botchan (坊っちゃん) and Tanizaki Jun'ichiro's In Praise of Shadows (陰翳礼讃) which both content-wise and length-wise are actually quite doable after just several years of studying the language. While you can read Aozora novels as-is online using Yomichan or Rikaisama, I personally recommend downloading the file and formatting the text-file using Jnovelformatter to be more comfortable on the eyes. 20 Other I should write a more extensive blog on this topic at some point, but while not enjoying the same popularity as approximately 15 years ago, the cellphone novel genre is still very much alive and many successful releases still find their way to the physical world. Many of these are romantic of nature or deal with daily life issues as seen from the perspective of teenage girls or young adult women. The genre's target audience is evident when accessing the most popular platform, Maho i-Land , greeting the visitor with a slogan claiming to be 「日本最大級のガールズポータルサイト」 (Japan's largest girls portal site). Due their very nature these works are quite accessible even for early intermediate students, and can be read on-line using my suggested method. Another interesting source of literature is the Japan P.E.N. Club Digital Library, an international association of progressive intellectual writers. The Japanese movement has strong ties to important Japanese writers as Endō Shūsaku and Kawabata Yasunari, and is part of a larger association with ties to Belgian Nobel prize winner Maurice Maeterlinck, Heinrich Böll, Jorge Luis Borges and even J.K. Rowling to name a few. I recommend Kawabata's One Arm (片腕) . Piracy of Japanese media is widespread, and while some argue piracy has led to the success of Japanese animation in the west in the first place, the argument that piracy is killing the industry has led to harsh crackdowns on piracy in Japan. Nevertheless, there does seem to be an active piracy scene in Japan, spreading digital versions, formatted in as .TXT file, of more popular modern literature; such as light novels. For obvious reasons I can't provide any sources, but this could make a good topic for a future article, perhaps. Image taken from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under Fair Use doctrine. ↩ The practice of accumulating not yet learned vocabulary for creating flashcards. ↩ Several years later this is still my go to method. While the amount of new definitions I actually add to my Anki-sets has of course drastically decreased over time, the learning process never really stops and I still encounter new expressions or technical jargon on a daily basis; especially while reading non-fiction texts related to my study-field. ↩ Amazon's business controversial practices aside, the free Amazon Prime student account is indeed extremely convenient and offers free access to Kindle Prime Reading and Prime Video. ↩ Or alternatively, to a .TXT format for further processing with Jnovelformatter . The end-result will be kinder to the eyes. JNovelFormatter is a neat little tool by the developer of Rikaisama that converts Japanese literature formatted as .TXT into cleanly parsed HTML-files. Layout is fairly customizable, although I think the original settings are easy on the eye enough as-is (I like dark backgrounds when reading for hours at a time, makes me feel less like I'm gazing straight into a light-bulb). End of Sentence dots are turned into book-markable anchors so you won't lose track of your progress. ↩ Several start-ups are playing with the idea of using blockchain technology to counter some of the problems inherently tied to E-book DRM. An interesting read: https://www.forbes.com/sites/billrosenblatt/2018/08/18/can-blockchains-disrupt-the-E-book-market-two-startups-will-find-out/#5d7a84435a0b .Another solution is watermarking, allowing the user more ownership over the E-book. https://copyrightandtechnology.com/2016/10/05/E-book-retail-platform-offers-choice-of-watermarking-or-drm/ . Speaking of blockchain and DRM, Sony actually filed a patent for in 2018 this as well. https://www.ccn.com/sony-files-for-blockchain-fueled-drm-patent ↩ While specific implementation of these laws differ on region, the World Intellectual Property Organization (WIPO) explicitly states the general act of DRM-removal as illegal https://www.wipo.int/ip-outreach/en/ipday/2016/ip_digital.html . ↩ Thus under this provision of the DCMA, it would be legal to bypass copy-control measures for private back-up purposes. http://www.dmlp.org/legal-guide/circumventing-copyright-controls ↩ <https://info.legalzoom.com/dmca-backup-copyrighted-content-22827.html https://www.wired.com/2010/03/dmca-muscle-strong-arms-dvd-copying/ ↩ Abbey House Media v. Apple Inc. Gizmodo has a brief piece on this at https://gizmodo.com/its-perfectly-legal-to-tell-people-how-to-remove-drm-1670223538 . ↩ Directive Article 17 (known as Draft Article 13) makes on-line platforms directly liable for copyright infringement by its users and could lead to implementation of filters to remove copyrighted material on most big on-line platforms. Directive Article 15 (known as Draft Article 11) will effectively limit social media and search engines in their capability of aggregating and hot-linking. Both articles are widely criticized controversy lies in the (solid) assumption this article will lead to a decrease in creative content and limit access to information, but member states have another two years to implements these measures and it is yet to see how social media platforms will reply. On a positive note, the EU did at least implement an exception in copyright law for scientific text and data mining (TDM) purposes. ¯\\ (ツ) /¯ https://www.wired.co.uk/article/what-is-article-13-article-11-european-directive-on-copyright-explained-meme-ban https://copyrightandtechnology.com/2018/09/13/eu-parliament-approves-watered-down-copyright-directive/ ↩ This absurd 2013 Wired article even goes so far to suggest that the technological prowess of circumventing DRM should be interpreted as a form of cyber crime and thus fall under criminal law, rather than seen as a potential civil offense. ↩ Although some member states distanced themselves from implementing any specific measures, such as Poland and Portugal https://www.communia-association.org/2017/10/11/european-parliament-talking-drm-right-now/ . ↩ Recently proposed and highly controversial changes to Japanese copyright law would extent that scope to any copyrighted material without permission, as an attempt specifically meant to counter piracy of Japanese comics. As is, the implications of this for regular Internet users could however be quite severe, with little legal ground to stand on, and are often compared to the EU's implementation of Draft Article 11 and 13. Prime Minister Abe has decided to postpone the bill for now. https://japantoday.com/category/crime/digital-dilemma-japan-flirts-with-overly-aggressive-online-copyright-law ↩ Although to be fair, there is quite some leeway in Japanese Copyright Law when it comes to the industry of derivative works (二次創作 nijisosaku or 同人誌 dojinshi ). ↩ English translations of both are available respectively at http://www.cric.or.jp/english/clj/cl1.html and http://www.japaneselawtranslation.go.jp/law/detail_main?id=83&vm=2&re= . ↩ From a technical point of view, this is based on one's IP address. Although methods such as using VPNs or dynamic IP addresses have been popular means of bypassing ( spoofing ) geo-locks, many streaming services are aggressively blocking access through such means. ↩ According to Wikipedia, they host over 10.000 works including both out-of-copyright works or those made freely available by the authors. Read more: https://en.wikipedia.org/wiki/Aozora_Bunko ↩ Although the Trans-Pacific Partnership (TPP) trade-agreement did not take effect after a United States withdrawal, discussions on copyright law concerning the TPP did however lead to a new definition of what concerns public domain in Japan. As a result, rather than 50 years, literature now falls into PD 70 years after the death of the author. This has led to a massive removal of literature on Aozora. ↩ While the Aozora-recommended web-application airzoshi is an attractive alternative, its formatting effectively blocks the usage of Yomichan or Rikaisama, rendering the whole exercise pointless. ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2019/05/japanese-e-books-vocab-mining-drm-and-copyright-law/","loc":"https://steviepoppe.net/blog/2019/05/japanese-e-books-vocab-mining-drm-and-copyright-law/"},{"title":"Travels: Hong Kong & Macau","text":"At the end of my third semester in Japan, I took use of an extended school holiday and my last remaining months in Japan to plan a short trip to Taiwan and Hong Kong. During my stay I've had the pleasure to acquaint people from both regions and took this as the perfect excuse for some last minute sightseeing. Brief history As one of the most important trade and financial hubs in Asia and one of the most densely populated regions in the world; Hong Kong (香港) has a distinct international feel. While historically undeniably Chinese, Hong Kong was conceded by the Qing Dinasty to British rule after the First Opium War and remained a British colony until 1997, with brief control under the Japanese Empire during the Second World War. The majority of Hong Kong's population have Cantonese ancestry and Cantonese remains the region's main language. Although Hong Kong, like Macau, has some political independence under Deng Xiaoping's \"one country, two systems\" principle (一国两制), both Macau and Hong Kong are officially part of The People's Republic of China under the statute of Special Administrative Region. This in turn does lead to occasional political tensions concerning the extent to which mainland China should be allowed governance over Hong Kong. Living in Japan with mutual friends from mainland China, Hong Kong and Taiwan, I've had the opportunity to get a more nuanced take on both sides, but I feel this particular blog is not the appropriate platform for such a topic. Itinerary While I spent the first half of my last-minute trip (around Taipei) mostly by myself, the second half of my trip (Hong Kong and Macao) was mostly spend with company and as a leisure trip to charge our batteries. I took a Hong Kong Airline flight from Taipei to Hong Kong International Airport (a short flight but possibly the most comfortable I've flown so far) and arrived at my hotel early in the evening. Similar to the EasyCard in Taiwan and Pasmo in Japan, I bought a RFID metro card, the Octopus Card, in Hong Kong. Rather than a card, this one was designed a cute bear key-chain, again applicable as general payment method in most convenient stores and some restaurants. Hong Kong Ginza Square I stayed at Harbour Plaza Resort City in Kingswood , close to Ginza Square. While this place looks remarkably classy and far out of my league, it's in what's called the New Territories (新界) in Tsin Shui Wai . fairly far from central regions like Kowloon and thus quite affordable (also, a friend arranged a strong discount). As I arrived fairly late in the evening, most of the restaurants in the vicinity of the hotel were already closed. On advice of the hotel staff, we took a cab to a nearby region popular amongst locals for late night food and drinks. Rather than the likes of bars however, these were small pop-up joints set up in people's garages. Luckily, my friend speaks Cantonese and ordered us some My Dim Sum and other dishes. The cook struck a conversation with my friend and it turns out she was quite curious what I felt of the food since I was the first visitor from the west. I quite enjoyed it and the general atmosphere, despite not speaking a word of Cantonese. One habit I was formerly unaware of though is washing one's utensils with the hot tea provided with your meal. Central Central (中環) is most known as the financial district of Hong Kong, with massive skyscrapers of distinct architectural design. In contrast, heading a bit further in Central reveals small residential neighborhoods with artsy boutiques popping up around; as well as Hong Kong's arguably most infamous outgoing scene: Lan Kwai Fong , popularized in the west thanks to success of Hong Kong cinema, although I can't say I particularly enjoyed the atmosphere there. Hong Kong has a reputation as shopping paradise, due tax-free shopping and a haggling culture on the many street markets located both in the Kowloon strip and Hong Kong Island. We spent some time walking around Li Yuen Street Market in Central, and the popular Ladies' Market in Kowloon. Even during week days, these places are fairly crowded and in combination with less-than-stellar air quality make these trips quite exhausting. Kowloon We spent most of our time around Tsim Sha Tsui, one of Hong Kong's busiest neighborhoods, situated across Hong Kong Island's many skyscrapers on Hong Kong's Kowloon Peninsula. The Hong Kong Cultural Center and surrounding Avenue of Stars on Tsim Sha Tsui's waterfront offer a stunning view of Hong Kong island's cityscape. Furthermore, the region has a daily light-show at 8PM, known as A Symphony of Lights; accompanied by an orchestral soundtrack, decorative lights and lasers on both sides lit up in bright neon colors. The nearby Nathan Road and Shanghai Street are central shopping streets with a hectic mixture of high brand mega malls, restaurants and coffee shops, as well as smaller shops and boutiques overrun by tourists and locals alike. One of the other unique locations was Chungking Mansions, an immense building formerly used Wong Kar-Wai's Chungking Express, and known for being a mixture of incredibly cheap and small hostels, Indian, Nepalese and African eateries, electrical stores, foreign exchange offices, but also less legal practices. The brief time we spent walking around inside was unlike any other experience I've ever had. Countless times I was whispered into my ear offers for things ranging from cheap tailored suits to drugs and prostitution, but one time someone offered a compliment on my hairstyle, so that made up for things. Regardless, as a cultural hotpot this place is a goldmine for anthropologists researching glottalization in Hong Kong. Of interest was Yum Cha, a Dim Sum restaurant which, while notorious for its weak customer service, is incredibly popular amongst Japanese tourists for its Instagram-worthy sweet Dim Sums. Macau On my last day before returning to Japan we took a day trip to Macau (澳门), a Special Administrative Region of the PRC and until 1999 a former colony of Portugal. The trip to Macau is quite cheap, and as an incredibly cheap flight destination it was financially more convenient to return to Japan through Macau's airport. Although easily accessible by ferry, Macau is unrelated to Hong Kong and does requires a separate immigration process. Despite Macau's rich history, the region is known primarily as Asia's biggest gambling hub (generation more revenue as in Las Vegas) and attracts mostly tourists from mainland China interested in casino's. Macau is divided into the island Taipa (氹仔, where Macau's airport and The Venetian, world's largest casino, are located) and the Macau Peninsula (澳門半島) connected to mainland China. The various casinos offer free shuttle buses covering both the island and peninsula, rendering public transport almost unnecessary. Casino's offer free luggage service as well, regardless of reservations. The juxtaposing of Portuguese and Chinese culture in Macau's landscape, or at least what remains visible, is fascinating to say the least. My attention however was drawn more towards the contrast of massive themed casino's (such as The Venetian) set against undeveloped ground, construction sites and almost slum-like residential regions undergoing gentrification. Music While traveling, I make it a habit of finding good music by listening to local radio stations or asking recommendations to people I meet in hostels or bars, as my own form of personal souvenir. One of Hong Kong's most popular musicians right now is Eason Chan, famous both in the Mandopop and Cantopop scene, but it's honestly not that my thing. Instead I quite enjoy More Reverb , a young Hong Kong instrumental post-rock band. Conclusion After having lived in Japan for almost 18 months, and coming straight from Taiwan, Hong Kong was quite a surprise, with strong contrasts both in terms of its international atmosphere and division of wealth. While I would've liked to have explored more, I was lucky to be accompanied with friends who showed me around and introduced me various locations and nice food. On a personal note, I had a period where I watched a lot of Hong Kong cinema, starting with Bruce Lee Kung Fu movies during when I practiced Kung Fu myself, to the Hong Kong New Wave and 90's scene with famous directors and actors as Andrew Lau, johnnie To, Wong Kar-wai, Maggie Cheung, Tony Leung and Leslie Cheung. Visiting the location of those movies in real life was really cool! As China's only legal gambling hub, my trip to Macau offered a fascinating glimpse into the world of the rising Chinese nouveau riche class, which by far overshadows the colonial atmosphere the region has. Not being dirty rich myself however, I will probably limit my time in Macau to this one day-trip. Gallery Hong Kong Gallery Macau Hong Kong by Stevie Poppe ( https://flic.kr/p/2fMA1vY - CC BY-SA 2.0) ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2019/05/travels-hong-kong-macau/","loc":"https://steviepoppe.net/blog/2019/05/travels-hong-kong-macau/"},{"title":"Travels: Taiwan","text":"At the end of my third semester in Japan, I took use of an extended school holiday and my last remaining months in Japan to plan a short trip to Taiwan and Hong Kong. During my stay I've had the pleasure to acquaint people from both regions and took this as the perfect excuse for some last minute sightseeing. Brief history The Island of Taiwan (governed since 1945 by the Republic of China, 中華民國) is an East Asian state located southwest of the Japanese Okinawan islands. Dutch colonization and consequent annexation by the Qing dynasty in the 17 th century, as well as annexation and five decades of rule by by the Empire of Japan after the Sino-Japanese war of 1894-1895, have had a drastic impact on Taiwan's demography, and presently the Taiwanese indigenous people make up only 2% of the population. Taiwan's political status and its relation to mainland China, as well as domestic opinions on Japan, remains controversial topics. Nevertheless, the ease of entry for foreign visitors, as well as Taiwan's scenic beauty and vibrant cultural scene have made Taiwan a popular destination for both mainland Chinese and Japanese nationals. Itinerary Due budget and time restraints I spent just two nights in Taiwan: centered around Taipei with one day trip to some of the more famous spots amongst Japanese tourists. I took an early flight from Tokyo's Haneda Airport to Taiwan Taoyuan International Airport and arrived in Taipei Airport around noon. Preliminary preparations including currency exchange and the purchase of EasyCard RFID smart-card (with student discount) for the Taipei Metro transit system. Like Pasmo and Suica cards in Japan this card is widely accepted in convenience stores and retail outlets as well so I ended up using this as my primary means of payment. Day 1: Taipei After a brisk short train ride I arrived at Taipei Station and experienced Taipei's outside air and surroundings for the first time. It was by all means a beautiful, sunny day, but due the time of the year quite humid as well and I realized I'm not particularly good at dealing with tropical temperatures. Luckily my hostel, located near the National Taiwan Museum and Peace Park, was but a short walk from the station. On my way, I was struck by the amount of palm trees and the widespread use of motorcycles. I thought there was some motorcycle event going on but this turned out to be just a daily thing. After checking in and refreshing myself, I went out to explore my surroundings, spending a fair amount of time at the beautiful 228 Peace Park . Some elderly people were practicing taichi and one man appeared to be practicing operatic singing; a bit of an unusual activity in public spaces, but not particularly displeasing. From there I took a subway to Taipei's most well-known temple, the 300 year old folk religious Longshan Temple (龍山寺). The history of the temple, its syncretistic background (the temple originated as a Buddhist place of worship but incorporates elements of Taoism and folk religions as well) and architectural design were fascinating to observe. My next planned destination was the so-called Elephant Mountain (象山); a steep hill and popular hiking trail near the massive Taipei 101 tower . Despite the heat the trail was quite busy; drawing in tourists and locals alike to view the phenomenal sunset over the city. One man, shirtless, took the opportunity to show us his physical prowess by repeatedly jogging up-and-down and performing push-ups every few steps. The view from the top was phenomenal. Several large, climbable stones give the opportunity for tourists to take Instagramable photographs, creating the illusion of peaceful solitude. Said spot was however quite crowded and due the academic holidays in Japan, filled with young Japanese people visiting Taiwan as a graduate trip. After a beautiful sunset, I descended the mountain and headed towards Taipei 101, formerly the world's tallest skyscraper and known for its distinct postmodernist architectural style. While a bit steep, I did enter the building and rode the elevator to the top floors. The skyscraper has a visible ‘tuned mass damper', a massive steel pendulum designed to protect and offset movement from strong winds or earthquakes. Though clearly touristic in nature, the surroundings of the building were quite lovely as well; with a variety of artwork and a cute structure based on Chinese children's book When the Moon Forgot . Finally, I spent the evening by strolling through a nearby night market, the Linjiang Night Market (臨江夜市, also known as Tonghua Night Market 通化夜市 due its proximity to Linjiang Street and Tonghua Street ). Aside from the compulsory Taiwanese bubble tea, I enjoyed popular street food such as stinky tofu , steam fried buns and egg tarts. Day 2: Day-trip - Pingxi Line (平溪線) My day-trip today was centered around probably the most popular tourist destinations near Taipei, often combined in one day trip: the towns of Shifen (十分), which is known for its railway market street, floating lanterns and lush waterfall) along with Jiufen (九份, often associated with Ghibli's award-winning animation Spirited Away). Getting to both places was enough but required a short trip from Taipei Station to a small nearby village called Ruifang (瑞芳), from where one can access the Pingxi Line: a short railway line connecting former mining villages around the Pingxi Valley (平溪). I personally recommend purchasing a cheap Pingxi Line day pass as many of the destinations on the line are worth visiting. Shifen (十分) On my way to Shifen I befriended an elderly Japanese couple who happen to live quite close to my apartment in Tokyo. One of the two actually graduated from Chuo University, a pleasant coincidence. After exiting the station, I landed straight on Shifen's main street, the Shifen Old Street (十分老街). A popular scenic spot, the Pingxi Line runs straight through this pad. When the train passed, paying visitors write their wishes on large sky lanterns and set them off in the sky. North of the Old Street, a scenic path leads to the famous 30-meter-wide Shifen Waterfall (十分大瀑布); a bit hard to portray through just pictures, but this alone was worth the Pingxi Line trip. Houtong Cat Village (猴硐貓村) Like other villages on the Pingxi line, the village of Houtong was traditionally a mining village, and due the well-kept state of its mining infrastructure; offers curious visitors a brief glimpse in its mining history. The primary reason for its touristic growth however is due its immense cat population; claimed to be twice that of its local (human) population. Some boutiques offered quite cute cat-themed hand-made trinkets or desserts, and I couldn't help myself to this amazingly cute cat cake. Jiufen (九份) After my trip through the Pingxi area, I returned to Ruifang and scheduled my next trip, the former gold mining mountain village of Jiufen. On my way to the bus stop, I hoped to purchase a snack at a local food stall. While queuing in line I was however denied by the proprietor who claimed their food to be sold out (没有！没有！). My limited knowledge of the Mandarin language was however sufficient to defend my honor while walking away (真的沒有嗎？好的，我明白了。). Nevertheless it was a bit frustrating to see those behind me in the queue being served after all. Visually, Jiufen reflects the rapid development it underwent as a gold mining town under Japanese occupation, with many wooden structures having a unique colonial-era Japanese feeling. A-mei Tea House (阿妹茶楼) in particular, due its resemblance to Japanese animation Spirited Away's bathhouse, draws huge amounts of Japanese crowds. Facing Taiwan's northeast coast, Jiufen scenery at night is breathtaking. The wooden structures in Jiufen old street (九份老街) undergo a drastic transformation once the many red lanterns are lit at night; and while crowded even during average weekdays, remains magical to say the least. Drained of energy, I lined up at the A-mei Tea House for a break; and while queuing incidentally met the Japanese couple next to me on the airplane. Turns out they live near Ikebukuro, a region of Tokyo I visit on a near-daily base. Not surprisingly, their two-night visit to Taiwan was planned as a graduation trip, with Jiufen as personal highlight. From Jiufen, I took a late night shuttle bus back to Taipei. Two Japanese girls next to me struck up a conversation with me; and again it turned out they live relatively to my region. Starting to feel like a conspiracy theory, but statistically not unrealistic considering I was living in one of the most populous regions of the world. One of the two told a cute story of how she met her boyfriend on a Japanese learning application and planned a working holiday in India to be with him. Unrelated but the brown sugar boba milk tea I order at the Jiufen storefront of popular Taiwanese Bubble Milk Tea brand Xing Fu Tang (幸福堂), was by far the best I've had this trip. Day 3: Taipei Although I'd have loved to spent more time around Taipei or in Taiwan in general, I had a flight to Hong Kong planned at night and the scorching heat this day prevented me from strenuous trips. I passed by the Chiang Kai Shek Memorial Hall (中正紀念堂) before spending my last hours in Ningxia Night Market (寧夏夜市) stocking up on the compulsory gifts for back home, and ordering one last tapioca milk tea. Music While traveling, I've made it habit of listening to local radio stations or ask for recommendations on any local music scene, as a form of personal souvenir. I've become quite fond of the Taiwanese-born A-Mei , one of the most successful Manderin-singing pop musicians alive. The following track is of the 2015 album AMIT2, an album I've listened to many times since my return to Belgium. Conclusion It is difficult to make general conclusions on just several days of sightseeing, but at the very least I was taken a bit aback by the amount of Japanese tourists in Taipei. While I expected Jiufen and the A-mei teahouse to be popular due its association with the bathhouse in Spirited Away, it wouldn't be that much of an exaggeration to claim the language I heard the most was Japanese rather than Mandarin. At some times while eating out or browsing in shops, it was actually easier for me as a foreign tourist to communicate in Japanese rather than in English; something I felt a bit mixed about. Talks with Taiwanese friends revealed a strong positive attitude towards Japan. Younger people care little for its colonial history and are drawn to Japan's soft-power as cultural powerhouse; which was undeniably visibly present anywhere I went. That aside, I had a lovely stay and immensely enjoyed the scenery, cuisine and vibrant atmosphere Taipei and its surroundings offered. Gallery Bubbletea in Ninxia Street Market by Stevie Poppe ( https://flic.kr/p/2fM5qfG - CC BY-SA 2.0) ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2019/05/travels-taiwan/","loc":"https://steviepoppe.net/blog/2019/05/travels-taiwan/"},{"title":"A Quick Guide on Using Anki 5: an efficient vocab mining set-up with Anki and Yomichan","text":"As an undergraduate student relying heavily on Anki 2 for both theoretical courses and for studying Japanese, I wrote several blog-posts throughout 2016 on how I've personally integrated Anki in my daily life. One of those entries extensively documented how I've connected Anki with Firefox to vocab-mine Japanese texts in my browser with JPod101 audio-readings, example-sentences (a method crucial to my language-acquisition) and Japanese dictionary definitions. Although I've had little time to update my blog throughout 2018, those specific posts became somewhat high-ranking when googling things concerning Anki. Unfortunately, Firefox-updates led to Anki-integrated Firefox pop-up dictionary Rikaisama (the plug-in I've relied on for years) becoming obsolete. Thus it was high time to update my humble guide to reflect the current state of affairs on the elusive ‘browser + Anki' integration-methods available today. So it's 20 XX , wasting valuable time on automatable processes bores you and you're keen on integrating technology more efficiently in your Japanese language-acquisition. Concretely you want to: Vocab-mine Japanese texts you're reading both on- and off-line using Chrome or Firefox. Improve your retention-rate with hidden furigana reading-aids and crystal-clear audio-readings. Contextualize your cards with Japanese example sentences. Rely not just solely on foreign-language translations, but use Japanese dictionary definitions for optimized studying. Basically, you want cards like this: What options are available then, sans going all the way and developing said tools yourself? If you've previously used the Rikaisama pop-up dictionary and followed my previous guides, you can continue doing so using a Firefox-derived browser. I still recommend this method for longer texts such as novels. Head to section Rikaisama . If you're new to this site or method, use the Chrome / Firefox add-on Yomichan. It's user-friendly, highly adjustable and has tons of features. Head to section Yomichan . Yomichan Developed by Foosoft Productions - a household name amongst the Anki-utilizing on-line learning community - Yomichan originated as a stand-alone bookreader with Anki-integration, 3 until it in its current form became the most complete alternative available for Rikaisama. In similar fashion to Rikaisama, we'll use Yomichan to vocab-mine Japanese texts and generate extensive Anki-cards with Jpod101 audio-readings, example sentences, furigana , and Japanese definitions. Furthermore, for ex-Rikaisama users, Yomichan definitely has the upper hand concerning user-friendliness and adaptability. Set-up Another benefit of this plug-in is that it's cross-platform, available both on Firefox and Chrome . Pick your platform of choice and after installing we'll start the initial set-up. It's worth checking out Yomichan's homepage as it contains an extensive manual. This guide will focus purely on connecting Yomichan to Anki, setting up a proper template for our cards, and integrate generation of example sentences and Japanese definitions. Note Yomichan can't parse off-line files out-of-the-box. If you're inclined to use Yomichan with off-line files (when reading HTML-formatted novels for example), just head to the Chrome/Firefox settings → extensions page and manually allow the add-on to access file URLs. Dictionaries First, download the libraries to your liking and add them in the Yomichan settings page. Integrating Anki To integrate Yomichan into Anki, we'll need to install Anki-Connect , an Anki plug-in specifically tailored to Yomichan. Open Anki on your desktop, go to Tools → Add-ons → Browse & Install , and copy-paste 2512410601 in the pop-up dialog. Now, we'll create a new set to contain our Yomichan-powered cards. We want cards containing the expression (and hover-able furigana reading), its definition in our language of choice, audio readings and example sentences. For this, we'll create a new ‘note-type' with fields corresponding to the info we'd like to see in our cards. We probably want to be tested both on the Japanese expression (visual recognition), on its meaning (production) and finally on the audio (oral recognition). To achieve this, we'll add three ‘card-types' to our ‘note-type'. Start by creating a new ‘set', eg. yomi-vocab . On Windows, press Ctrl + Shift + N , or click Tools → Manage Note Types , to enter the ‘note-type' management screen. Click add → ok to create a new ‘note-type'. Call it Yomichan . Next, select the new Yomichan ‘note-type' and click Fields . We'll rename the current front and backside as Expression and Meaning , and add three new ‘fields' Audio , Examples and Notes (that latter one could be used for personal notes). Close this screen and select Cards . We'll manage the different cards this note generates as well as its make-up. We'll start with a basic layout. Add the following lines to the bottom of the middle Shared Style screen. For optimal learning, we'll keep kana-readings as furigana displayed only touch/hover. This way learners are less likely to use them as a crutch. 1 2 ruby rt { visibility : hidden ; } ruby : hover rt { visibility : visible ; } Next, click the + button at the top of the screen twice to create two more card-templates. Rename (by clicking More → Rename ) those templates ( Card 1 , Card 2 and Card 3 ) respectively as Recognition , Production , and Audio . Finally, let's wrap up our Anki set-up by creating the actual card's templates. Each card will question you on one field (the Japanese expression, the English translation and the Audio) and show the answer on the backside along with audio and any potential notes, if present. Your card's templates should look as follows: Recognition Front Template 1 < div lang = \"ja\" > { { furigana : Expression } } </ div > Back Template 1 2 3 4 5 6 7 8 9 10 11 12 13 { {FrontSide } } < hr id = answer > { { furigana : Meaning } } < br /> { {#Notes } } < br /> < b > Notes :</ b > < br > { { furigana : Notes } } { {/Notes } } { {Audio } } < br /> Production Front Template 1 { { furigana : Meaning } } < br /> Back Template 1 2 3 4 5 6 7 8 9 10 11 12 13 { {FrontSide } } < hr id = answer > { { furigana : Expression } } < br > { {Audio } } { {#Notes } } < br >< br > < b > Notes :</ b > < br > { { furigana : Notes } } { {/Notes } } Audio Front Template 1 Listen .</ br > { {Audio } } Back Template 1 2 3 4 5 6 7 8 9 10 11 12 { {FrontSide } } < hr id = answer > < div lang = \"ja\" > { { furigana : Expression } } </ div >< br /> { { furigana : Meaning } } < br /> { {#Notes } } < br >< br > < b > Notes :</ b > < br > { { furigana : Notes } } { {/Notes } } Mapping When you're finished, don't close Anki just yet. Our final step for now is to map our Firefox or Chrome Yomichan settings to Anki so that our generated cards will be using the correct card-template and imported data gets saved to within correct set. Open the Yomichan options; scroll down to the ‘Anki Options' category and change accordingly: Deck → Yomichan , Model → yomichan , Field Expression → {furigana-plain} , Meaning → {glossary-brief} , Audio → {audio} . Testing Now let's try out our new set-up! Note Anki has to remain open whenever we vocab-mine using Yomichan, so just keep Anki opened in the background for now. Press and hold Shift or your middle-mouse-button and hover over the words below. A yomichan pop-up should appear. 理解 ( りかい ) | 暗記 ( あんき ) (I'm not obsessed, I swear~) Just press the green Add Expression icon on the pop-up or use the key short-cut Alt + E to import these words into your set. 1 2 3 4 5 (I'll add my copy soon, promise) I've exported my own copy of this example and uploaded if you want to compare or save yourself the work of creating the note template manually: * **Download**: [Example set Yomichan <i class=\"icon-download-alt\"></i>](https://www.dropbox.com/s Japanese Example Sentences Our current set-up is pretty useful already, but as seen in the GIF at the top of the page, some further effort will allow us to add proper example sentences and their translations to our mined vocabulary on-the-fly. We'll use an existing Anki add-on called Japanese Example Sentences to add Japanese sentences taken from Tatoeba.org , a \"collaborative, open, free and even addictive\" community on producing example sentences (the Internet is can at times be a wonderful environment). Download The Japanese Example Sentences add-on has a page on Ankiweb's plug-in page , but as usual we'll install the plug-in using the Desktop Anki application. Go to Tools → Add-ons → Browse & Install, and copy-paste 2413435972 in the pop-up dialog. Restart the application to complete the installation. This example-set is fairly outdated. Follow the instructions left in the plug-in comment section to update to the most recent set. Set-up In its current stage, this plug-in can contextualize existing cards using Edit → Bulk-add examples . There are two requirements so-far: the ‘note-type' should have both an examples field and expression field, and the expression should be just the Japanese expression without square brackets containing furigana . If those requirements are met, we can contextualize existing cards using Edit → Bulk-add examples from the card-browser, or contextualize new cards when creating a new card of a note-type that contains the examples and expression fields. However, we do have square brackets containing furigana in our expression fields, and contextualizing cards generated using Yomichan is not yet possible. Thus finally, we want to alter out cards' lay-out to display our example sentences in a responsible manner. We'll need some further edits outlined below. Card Lay-out Add the code below to the bottom of the middle lay-out screen. It'll display the example sentences to the left of your cards. 1 2 .examples {font-size:75%; text-align: left;} .title {font-size:16px; color: #999999;} Back Template Add the HTML code below to the bottom of your different cards' back templates. 1 2 3 4 5 6 7 {{#Examples}} < br /> < div class = \"examples\" lang = \"ja\" > < span class = \"title\" > Sentences: </ span >< br /> {{furigana:Examples}} </ div > {{/Examples}} Usage with Yomichan / Anki-Connect For proper integration with Yomichan, I've made some adjustments to both its Anki-Connect add-on and the Japanese_Examples add-on. I've added the edited files on a new repository on my GitHub so go ahead and replace the existing add-on files on your Anki's add-on folder on your computer (e.g. C:\\Users\\USER_NAME\\Documents\\Anki\\addons or C:\\Users\\USER_NAME\\AppData\\Roaming\\Anki2\\addons ) with the ones on there. I've described all my edits below in case you'd prefer to do this manually. Feel free to skip this part if you're not technically inclined. japanese_examples.py We've set up our notes to show furigana on our vocabulary. This requires square brackets (eg. 気象庁[きしょうちょう] ). The Anki plug-in for Japanese examples however does not support this out of the box. For that reason, I've added a regular expression to only use the contents up to the first square bracket as expression. 1 2 3 searched = re . search ( r '&#94;[&#94;\\[]+' , expression ) if searched : expression = searched . group ( 0 ) If you'd like to edit this yourself, add that piece of code to the find_examples class right after it defined examples as a list. it should be around line 139 ( def find_examples ( expression , maxitems ): examples = [] ). Secondly, I've edited the mark-up of the example sentences to hide the English translations unless hovered above (or on press on smart-phones). Locate the examples.append call in the find_examples method. You'll want to replace it (I commented it out) with the code below. It'll be around line 174. 1 examples . append ( \"<div id='eng_test'> %s <span id='eng_sentence'> %s </span></div>\" % tuple ( example . split ( ' \\t ' ))) In the find_examples_multiple method (around line 214), replace the current return with the line below. We'll want just one break between example sentences. 1 return \"<br>\" . join ( examples ) Anki-Connect.py This extension calls on Anki's API to create new notes. All my edit does is call japanese_examples at run-time. First, we'll have to import the japanese_examples extension to be able to call its methods. If you're doing this set-up manually, add the line below near the top, along with the other includes (around line 30). 1 from japanese_examples import * Next, I've written a few lines in the addNote method to call Japanese-examples' find method at run-time and, if examples were found and our note-type has the correct destination field, add these to our newly created card. This should come before collection = self . collection () around line 700. 1 2 3 4 5 6 # for use with Japanese examples examples = find_examples_multiple ( ankiNote , MAX_PERMANENT ) # if field is empty and examples exist if examples and not ankiNote [ DEST_FIELD ]: ankiNote [ DEST_FIELD ] = examples DEST_FIELD is a global variable defined in japanese_examples.py . If Anki gives an error, there might be a mismatch in naming. Check the naming in japanese_examples.py and change DEST_FIELD accordingly. Note For this to work, you'll need to make an edit in your note template's layout. As usual enter the note-type management screen by pressing Ctrl + Shift + N , or click Tools → Manage Note Types . Select the note-type of the cards you'd like to contain example sentences and click fields . From there, select the Yomichan note, click cards and add the following CSS-code to the bottom of the shared lay-out screen in the middle. 1 2 # eng_sentence { display : none ; } # eng_test : hover # eng_sentence { display : inherit ; color : #eb4c42 ;} Usage Contextualizing existing cards If you've set up Yomichan to include ‘yomichan' as tag on new cards, this should be a breeze. Open Anki's card-browser (by clicking Browse or pressing B ), select the yomichan tag in the left column, and press Ctrl + A to select all your cards. Next, click Edit → Bulk-add examples . Contextualizing imported cards If you've followed above steps, every time you add a new word on-line, it'll automatically contain example sentences as well. Try it out on our next \" Words of the Day \": 文脈 ( ぶんみゃく ) | 語彙 ( ごい ) Example Set COMING SOON! 1 2 3 As usual, I've exported my own copy of this tutorial and uploaded it in case you'd like to compare or save yourself the work of editing the note template yourself. * **Download**: [Example set Sentences <i class=\"icon-download-alt\"></i>](https://www.dropbox.com/s/2lq0d7cn2rqywih/Rikai%20vocab_context.apkg?dl=0) Sanseido Japanese Definitions Note: if you're using Anki 2.1+, this feature is unsupported yet. Support for on-line monolingual dictionary definitions is a feature often requested of Yomichan. Right now, Japanese EPWING dictionaries are supported out-of-box, but these are propriety software and not easy to get a hold of. The author has however expressed some reluctance to adding support for such on-line dictionaries due the instable nature of accessing definitions: through web-scraping. The slightest update to the structure of an on-line dictionary would render the feature unusable. That said, I've being doing exactly that, automatically web-scraping Japanese dictionary Sanseido when vocab-mining with Rikaisama / Yomichan, for the past three years without any issues. This part of my post will guide you through setting this up; technically it's incredibly similar to the Example Sentences sub-guide, so if you've come this far it should be a breeze. The result of our work will look as follows: Download Again we'll use an existing Anki add-on: Sanseido Definitions . As usual we'll install the plug-in using the Desktop Anki application. Go to Tools → Add-ons → Browse & Install , and copy-paste 1967553085 in the pop-up dialog. Restart the application to complete the installation. Set-up The ‘note-types' of the vocabulary you want Japanese definitions for will need a new field to contain the definition. On the desktop Anki application, press Ctrl + Shift + N , or click Tools → Manage Note Types , to enter the ‘note-type' management screen. Select the ‘note-type' of the cards you'd like to contain example sentences and click fields . On the next screen, click add and call the field Sanseido . Now close this screen. On the previous ‘note-type' management screen, click cards . We'll edit our lay-out and display the new field in our cards. Anki Note Lay-out Lay-out Add the following CSS code to the shared style screen of your note's ‘card-type' template. 1 . title { font-size : 16 px ; color : #999999 ;} Templates Add the following code below the line displaying your vocab expression's definition (e.g. {{Meaning}} ). You'll do this in Expression → Back Template , Meaning → Front Template and Audio → Back Template . 1 2 3 4 5 6 7 < br /> {{#Sanseido}} < br />< div id = \"japanese_meaning\" > < span class = \"title\" > Japanese: </ span >< br /> < span id = \"japanese\" class = \"sanseido\" > {{furigana:Sanseido}} < br />< br > </ span ></ div > {{/Sanseido}} Bulk-edit Open Anki's browser (by pressing B or clicking Browse from the main window). The left of this browser has an overview of all your different sets and tags. Select the deck or tag containing the cards you'd like to edit. Press Ctrl + A to select all of those and click Edit → Regenerate Sanseido Expression . It will crawl the Internet for each new definition so this might take quite a while. Note The plug-in expects your vocabulary to be contained in a field called Word . If your field is called Expression, you'll have to edit this in the plug-in's python file on your Anki's add-on folder (e.g. C:\\Users\\YOUR_USERERNAME\\Documents\\Anki\\addons\\sanseidoDefsForAnki.py ). Open it with your text-editor of choice and change expressionField = 'Word' to expressionField = 'Expression' . Finally, as of July 2017, Sanseido's domain-name changed from .net to .biz. You'll have to manually edit sanseidoDefsForAnki.py again; just search for sanseido.net ( Ctrl + F ) and replace it with sanseido.biz . Alternatively, replace the file with the one on my github repository linked below. Usage with Yomichan / Anki-Connect As with the section on example sentences, a drawback to this plug-in when used in combination with our set-up is that it does not support vocabulary formatted to use furigana , and neither can Japanese definitions be added automatically on adding new cards with Yomichan. We'll have to make some further adjustments. I've added the edited files on a new repository on my GitHub so go ahead and replace the existing add-on files on your Anki's add-on folder on your computer (e.g. C:\\Users\\YOUR_NAME\\Documents\\Anki\\addons ) with the ones on there. I've described all my edits below in case you'd prefer to do this manually. Feel free to skip this part if you're not technically inclined. sanseidoDefsForAnki.py We've set up our notes to show furigana on our vocabulary. This requires square brackets (e.g. 気象庁[きしょうちょう] ). The Anki plug-in for Sanseido definitions however does not support this out of the box. For that reason, I've added a single line of code that uses a regular expression to only use the contents up to the first square bracket as expression. 1 term = re . search ( r '&#94;[&#94;\\[]+' , term ) . group ( 0 ) If you'd like to edit this yourself, add the following code as first line in the fetchDef class - it should be around line 27 ( def fetchDef ( term ): ). Anki-Connect.py Again, Anki-Connect will run sanseidoDefsForAnki at the point of creating new cards. First, we'll have to import that extension to be able to call it's methods. If you're doing this set-up manually, add the following line near the top, with the other includes (around line 30). 1 from sanseidoDefsForAnki import * Next, I've written a few lines in the addNote method to call sanseidoDefsForAnki's glossNote method at run-time and, if examples were found and our ‘note-type' has the correct destination field, add these to our new card. Again this should come before collection = self . collection () around line 700. 1 2 3 4 5 6 7 8 9 # Create sanseido definitions try : glossNote ( ankiNote ) except Exception as e : QMessageBox . critical ( self . window (), 'AnkiConnect' , 'Error, could not create sanseido definition.' ) raise e try : ankiNote . flush () except Exception as e : QMessageBox . critical ( self . window (), 'AnkiConnect' , 'Error, could not create sanseido definition.' ) raise e Imported cards If you've followed above steps, every time you add a new word with Yomichan, it'll automatically contain a Sanseido dictionary definition as well. Try it out on our next \" Words of the Day \"! 逆転 ( ぎゃくてん ) | 裁判 ( さいばん ) Download Example Set As usual, I've exported my own Anki copy of this tutorial and uploaded it in case you'd like to compare or save yourself the work of editing the note template yourself. This one follow up on the last parts of the guide and is identical to the template I'm using myself. COMING SOON 1 **Download**: [Example set Sanseido <i class=\"icon-download-alt\"></i>](https://www.dropbox.com/s/9wrpciaawfxeuo5/Rikai%20vocab_jj.apg) Applications I've covered several ways of using these tools back in 2016 . Rikaisama and Yomichan cover any format that displays actual Japanese text on your browser. This goes from reading news-articles, social media or HTML-formatted novels (I personally use this method to read Kindle-bought literature), but also HTML5 video-games, or viewing anime on Animelon's multi-layer subtitled streaming web-app (no affiliation, its free and I just think it's a pretty cool service). Rikaisama Rikaisama is dead, long live Rikaisama! While Firefox broke support of (unsigned) XUL-based add-ons and thus Rikaisama, further usage is still possible through XUL-supporting Firefox-derivatives such as Waterfox, Pale Moon and Basilisk. 4 For day-to-day practices, installing an additional browser seems like an unnecessary bloated approach, but despite its wonderful successor, I'm still quite partial to Rikaisama solely for reading longer texts such as Japanese novels, since Rikaisama's one-button solution to importing cards into Anki is frankly just more time-efficient as Yomichan. Finally, if you've already painstakingly gone through my previous tutorials, this method will require the least amount of additional effort. Set-up All this step requires is downloading one of those Firefox-derivatives and following my prior guides (I've gone through them again while setting up Waterfox and they're still applicable). Personally, I use Yomichan with Chrome on a day-to-day basis, and Rikaisama in combination with Waterfox while reading large texts. As for the previous guides on Rikaisama, I've added links in the next section. Wait! There is more! Although I've extensively covered Anki/browser-integration in my 2016 blog series, Rikaisama's status as legacy software rendered some of my previous writings rather obsolete and it was thus high time for an update. While I'm still keen on Rikaisama using Firefox-alternative Waterfox in certain situations (namely, handling large texts such as novels), I've switched to using Yomichan in day-to-day situations. Along with this new post, I've slightly updated my previous posts and still recommend them to the completionist. If you've any further questions, feel free to check out the other articles in this series on Anki, or to leave a comment below. A Quick Guide on Using Anki (effectively) (in an academic context) Setting up a perfect vocab-mining environment with Anki and Rikaisama Using Anki's API to contextualize your vocab cards with example sentences Making the switch: J-J definitions in your vocab cards Image taken from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under Fair Use doctrine. ↩ If you're relatively new to the application, I recommend my 2016 post A Quick Guide on Using Anki (effectively) (in an academic context) . Although visually bare-bones, Anki has vast possibilities and is both extremely versatile and, due to its many user-made plug-ins, adjustable. ↩ Granted, the off-line Yomichan-application that predated the current Yomichan Chrome plug-in covered most of these bases, but in my opinion reading novels on Yomichan was less as ideal and the project has been rendered obsolete since development of the Chrome plug-in took off. ↩ It must be said that there's benefits and downfalls to each of those derivatives and I don't specifically recommend you to replace your current browser with one of these unless you know what you're doing. ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2019/04/a-quick-guide-on-using-anki-5-an-efficient-vocab-mining-set-up-with-anki-and-rikaisama-yomichan/","loc":"https://steviepoppe.net/blog/2019/04/a-quick-guide-on-using-anki-5-an-efficient-vocab-mining-set-up-with-anki-and-rikaisama-yomichan/"},{"title":"Study-trip around Fukushima with Chuo FLP Seminar: Fukushima Hope tour","text":"At the start of the new academic year in Japan, early April, I joined a music circle and formed several bands with other members of the circle. During the summer break, one of my band's members invited me to join his class' study trip to Fukushima; gaining first hand experience at methods Fukushima's civilians are employing to revitalize Fukushima's tourist industry after the 2011 earthquake. Brief history The third largest prefecture in Japan, Fukushima is a northeastern region popular amongst Japanese not just as winter resort, but for its Japanese sake, food and many natural hot springs. Historically, Fukushima is known for it's role in the last feudal Samurai war, the 1868 Bosshin War. Today however, the region remains heavily associated with the 2011 earthquake and subsequent nuclear disaster. An earthquake on 11 March 2011 led to a tsunami disabling most of the Fukushima Daiichi Nuclear Power Plant (located on the Pacific Ocean coastline of central Fukushima) and subsequent nuclear meltdowns due insufficient cooling. Despite strong criticism on government-level handling of the situation, much has been done on a local scale to restore Fukushima to its former level. Itinerary A system quite common to Japan and less so where I'm from is the zemi system (derived from the seminar system in Germany); classes of around 10 students working together with the professor in interactive ways about a specific topic the professor is interested in. Usually groups become quite tight and occasionally join the professor for some food and drinks at night, as well as go on study trips (read: glorified city-trips). Earlier this year I had the opportunity of joining my German professor and three other students from KU Leuven studying in Japan for such a study-trip in Matsumoto, but this was my first opportunity doing in a full Japanese context. Although I was not part of the class, my friend and band-mate mentioned that that shouldn't deter me from joining; in fact due the topic in question, input from a non-Japanese student would actually be highly appreciated, or so he assured me. Taught by Dr. Prof. Hosono (細野教授), the class itself (FLP, or Faculty-Linkage Program) is open for students of different academic backgrounds and often focuses on international issues. This semester, students worked on city-planning in disaster-struck areas (まちづくり) with a case-study of tourism in Fukushima. The students worked around the concept of a Fukushima Hope tour (福島県HOPETOUR). How can Fukushima citizens work bottom-up towards reigniting Fukushima tourism? How should they utilize technology such as social media and translation tools to draw in foreign tourists; how can they shift the focus point from so-called disaster (dark) tourism? Are there any routes throughout Fukushima offering starting points to Fukushima's history, cuisine and natural beauty? What strategies are appropriate to attract previous tourists in different seasons? The three-day itinerary planned around Fukushima would first visit the formerly abandoned villages of Namie (浪江) and Tomioka (富岡町), passing closely to the Fukushima Daiichi Nuclear Power Plant itself. During this time, students have the opportunity to talk to some of the locals who've decided to return there and reopen businesses. Afterwards, the bus will head for a japanese-style inn (旅館) at Iizaka Hot Springs, near the centrum of Fukushima, were we will spend the night. The next two days were spent in the Aizu (会津) area, specifically Aizu-Wakamatsu (会津若松), Kitakata (喜多) and Ouchijuku , focusing more on the historical aspects of Fukushima as well as natural beauty and cuisine. HOWEVER! I just purchased a new smart-phone several days earlier and I found out the hard way my alarm app did not function properly. I woke up two hours too late and missed my train to Tomioka. I was forced to buy a new bullet train ticket and catch up with the group in Iizaka, missing the first part of the itinerary. (ू˃︿˂ ू) Fukushima City After a brief mental breakdown over oversleeping and missing my train, I ended up going directly to Fukushima City by bullet train and spent a few hours walking around the city, having a brief conversation with an old baker who'd never talked to a foreigner before and was interested in my opinion on the texture of his melon pan (decent). After a further uneventful trip I headed towards Iizaka through the cute Iizaka Line (飯坂線) and met up with the FLP class who'd arrived around the same. Not the best first impression but it didn't really bother anyone. iizaka onsen Iizaka Onsen (飯坂温泉) is a quaint hot springs resort in the central area of the Fukushima prefecture (Nakadori, 中通り). Coming straight from Tokyo, the town does have a bit of a rustic look, somewhat shabby even at times, but that's part of the charm for sure. The town is historically based around its natural hot springs and there are many public baths, traditional Japanese inns, as well as free foot-baths around. Although we ended up using the hot springs at our inn, we passed the oldest public baths in Japan, Sabakoyu (鯖湖湯), famous for having been visited by famous poet Matsuo Basho and reaching temperatures up to 50°C. These public baths form a social function and even now the free foot-baths around are used by weary passengers taking a break and having a chat. The inn we stayed at, Horieya inn (ほりえや旅館), was established in the 1815 and is still run by the same family. While a bit modernized to accommodate modern necessities, The wooden architecture and base of the building has remained completely intact. Before dinner, one of the proprietors briefed us on Horieya Inn, Iizaka Onsen, and the methods Iizaka locals are employing to revitalize tourism in the area. Everything is done on a community-scale, including digital techniques such as home-pages and social networks as Twitter. Even the website (linked above) is made in-house, and while very much bare-bones, displays the tenacity and perseverance of a mostly aging community to utilize new technology, as well as a lovely sense of humor . Do note the English, Chinese and Korean accommodation of the website using Google Translate. On that topic, it is worth checking out the official Iizaka Onsen's website as well. Despite similar design choices, the tourist association has taken much care in providing handmade translations in English, both traditional and simplified Chinese, Korean, Thai and Russian, as well as providing maps throughout the town highlighting natural beauty and cuisine throughout the four seasons. Bicycles and umbrella's are available for free at the station, by the way. Our meal, made with fresh community-driven Iizaka ingredients was delightful, as was the Onsen and our room (I stayed in a room with another graduate student and my band-mate, nicknamed 赤パンツ or ‘red pants' for his daily choice of trousers, by the way). At night, we joined several other students for some drinks and a few rounds of party-game Werewolf (jinro, 人狼 in Japanese). Tips for future players, abusing your status as Japanese-learning foreigner is an easy way to feint innocence. One of the fourth year students, responsible for some of the trip's organization and financial aspects, found out about gathering and got rather angry that we weren't sleeping yet, going so far as to insinuate we were irresponsible. A bit of a mood-killer, we returned to our rooms and slept briefly but well. Aizu-Wakamatsu Our next day was spent around the castle town of Aizu-Wakamatsu (会津若松), as well as the nearby Ouchijuku (大内宿) and Iimoriyama (飯盛山). Historically, Aizu-Wakamatsu is known as a castle town of the Aizu Domain and the main setting of the Boshin War, a civil war between those in support of maintaining the shogunate system (the feudal Japanese military government), and those trying to overthrow the shogunate after discontent regarding Japan's opening to the west. While 2018 marks 150 years since the Meiji Revolution, the Aizu region aims to focus rather on 150 years since the Bosshin War, hoping to attract tourists for its samurai history. We headed to Aizu Bukeyashiki (会津武家屋敷), known as a residence or mansion for high ranking Aizu samurai. The building is quite extensive and attempts to sketch daily life of Aizu samurai in the middle ages. During our talk with one of the proprietors, we learned of the Aizu-Wakamatsu community to in similar fashion utilize technology to attract and re-attract both national and international tourists; teaching staff how to use voice-translation software and implementing credit-card payment systems around town. The following Youtube video attempts to portray the natural beauty of Bukeyashiki during all four seasons. We stayed in a hostel near Nanokamachi Street (七日町通り), close to the famous Tsuruga Castle . While the current one is a concrete replica, the original castle ( was demolished after the Boshin war) served as administrative center of Aizu and has ties to the famous Date Masamune. The inside of the building has been rebuilt as museum with some interactive elements and various video-material displaying the history of the castle and Aizu samurai. The view from the top is worth the entrance. Iimoriyama Iimoriyama (Mount Iimori, 飯盛山) is a hill most famous for a famous media event related to the Bosshin War. The story tells the tragic suicide of a group of 19 teenagers (belonging to the Byakkotai 白虎隊 or White Tiger Company military unit) who had retreated here to assess the situation, and (wrongfully) believing the castle had fallen, committed ritual suicide as final means to defend their honor. This story is one of many tragic so-called bushido -related stories immortalized as war-time propaganda, and actually impressed Italian fascist leader Mussolini to the extent he donated a Pompeii archaeological column to the site. A 1930's donation from German side can be found here as well, in the form of a black granite plate with an Iron Cross inscription. Another interesting construction is the Aizu Sazaedo (会津さざえ堂), a completely wooden hexagonal Buddhist pagoda built in 1796, known for its ‘double-helix' structure. The building is devoted to Kannon, and worshipers descent the building through a different way as the ascent to avoid disturbing each other. Aside from the striking wooden construction, visitors will immediately notice an enormous amount of labels with calligraphy attached pretty much everywhere. This was done by Edo period ‘tourists' as proof of one's visit. Some things never change. Honestly, the lush nature juxtaposed with the plentiful faded gravestones, the wooden Aizu Sazaedo and the oddly befitting Pompeii eagle make Iimori an absolutely beautiful, must-visit area when in Fukushima. Ouchijuku Our next visit was the former post town of Ouchijuku (大内宿), which connected Aizu with Nikko during a period in which restrictions by the shogunate let by frequent obligatory travels to the capital of Edo (modern-day Tokyo). The area has been redeveloped as popular tourist destination for those looking for an authentic view of Japan's feudal history. The region thrives on tourism, with many shops selling handmade trinkets or grilled fish. One of the proprietors of a tatami-floored restaurant we had dinner took some time to explain some processes of their local cuisine ( negi soba , scallion buckwheat noodles) The well preserved buildings, with thatched roofs made from dried straw, dirt tracks, and untouched natural beauty (electricity cables are buried underground) have made this place a highlight of my stay in Japan. We spent the remainder of our day around this area before taking our bus back to our hostel. Before dinner, professor Hosono held an impromptu class on the topic of city planning, and asked the students to present their findings. As a foreign student, I was asked of my experience as well and had to present some ideas on revitalizing tourism in Fukushima, drawing attention away from the 2011 earthquake and disaster tourism. After dinner, I joined most of the students for a final drink (お疲れ会) and some games before calling it a day. Kitakata The next morning, we visited Kitakata (喜多), a city in Aizu today known particularly for its many ramen restaurants (over 130 stores!), sake and historic warehouses ( kura , 蔵) used for storing dried foodstuffs. It's commonly said that the high quality water in the neighborhood lead to its famous production of sake and noodles, but before we could try this ourselves we were led to a completely different shop: the Okuya Peanut Factory Shop . Apparently, Kitakata is rich in peanuts. We had an interview with the proprietor and received a tour around the stock-house, detailing both the business' history as well as the technical aspects of processing peanuts. The things I'm learning on exchange in Japan. Okuya Peanut Factory Shop has grown to the point this originally one-man-business is now a strong exporter of both refined and unrefined peanuts and became a local hot-spot for peanut soft-serve ice cream. Peanut Ice Due the clear, high quality waters nearby, the town of kitakata has plenty of premium sake breweries as well. We visited the Yamatogawa Sake Brewery , dating back to 1790 and using only in-house cultivated Kitakata water and rice. We had the opportunity to visit their storehouse and engage in some tasting. After our sake tasting, we had just an hour for lunch before ending our study-trip, and hurried to arguably the most famous ramen restaurant in Kitakata: Abe Shokudo (あべ食堂). On our way, we actually passed a small shrine dedicated to ramen. No time for distractions though; there was some serious eating to be done. That's right. Two pictures. My band-mate is a known ramen-aficionado and we ended up using our limited time during lunch to visit a second ramen restaurant right after, Makoto Shokudo (まこと食堂). Although we were several minutes late to our bus back to the station and thus again got scolded by the fourth year student in charge of organization, I can't say I regret making the decision to have a second portion. Conclusion I'm still a bitter I missed the first part of the trip, but nevertheless it was a wonderful experience joining my friend on a school-trip and something I hadn't expected to happen while studying in Japan. A good combination of studying, tourism and playtime amongst other students, I didn't get too much sleep this trip but instead I got first-hand experience in Japanese student-life and good memories. Many talks with local citizens collaborating in revitalizing Fukushima's tourism helped me get a deeper insight into aspects of marketing as well as the great extents of community-driven development. The prefecture of Fukushima is in the public mind still, and especially amongst foreign tourists, deeply connected to the 2011 earthquake and nuclear disaster; needless to say dramatically affecting Fukushima tourism. Regardless of my opinion on nuclear energy and the handling of the actual incident, international research does show radiation levels on average in Fukushima, especially in distant areas like Aizu, to be equal or even lower than the average of unaffected major cities around the world. While tourists are primarily drawn to regions as Tokyo, Osaka, Kyoto and Nara; there is much to be discovered in Fukushima, a region of exceptional natural beauty, local cuisine, hot spring baths and well-maintained historical elements in daily life scenery. I thus sincerely do recommend future travelers to travel outside of this fixed ‘Japan-itinerary' and enjoy the lifelong efforts of local communities such as those of Fukushima to promote their homes to travelers. Gallery Ouchijuku by Stevie Poppe ( https://flic.kr/p/2euNnmK - CC BY-SA 2.0) ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2018/09/study-trip-around-fukushima-with-chuo-flp-seminar-fukushima-hope-tour/","loc":"https://steviepoppe.net/blog/2018/09/study-trip-around-fukushima-with-chuo-flp-seminar-fukushima-hope-tour/"},{"title":"Study-trip around Matsumoto","text":"Last April, I had the pleasure to my professor from KU Leuven on a study-trip to Matsumoto, accompanied by several KU Leuven schoolmates also present in Japan, as well as a Japanese historian and professor at Matsumoto's Shinshū University, and his seminar students. On our study-trip, we would first visit the Old System Matsumoto High School Museum ( Kyūsei Matsumoto kōkō kinenkan 旧制松本高校記念館) and received a guided tour to a tunnel in Satoyama , said to be dug out by forced laborers during the last few months of the Pacific War. Later on we would visit Shinshū University and hold an impromptu seminar on the topics covered that day, before the obligatory nomikai (飲み会). On our second day, we would join our professor in the morning for brief breakfast and recap, before spending the rest of the day visiting the center of the city. Brief history A historic castle town and city in Nagano Prefecture, Matsumoto is known as a popular destination for those looking to visit the mountain scenery or traditional hot spring resorts located around the Northern Japan Alps . Matsumoto Castle in particular is a well-known attraction due to its reputation as one of the most beautiful castles in Japan. Matsumoto is the birthplace of famous artist Yayoi Kusama, and in more recent history, Matsumoto has also attracted attention for having being targeted by doomsday cult Aum Shinrikyo in 1994, leading to eight deaths by a Sarin gas terror attack. Itinerary I met up with a junior student of KU Leuven studying in Tokyo ( kohai , 後輩) for our highway bus from Shinjuku to Matsumoto, were we met up with a good friend and KU Leuven classmate of mine studying in Kyushu University. Our first evening was fairly uneventful as it was too late to explore the city. We ended up joining one of the Shinshū University students for some snacks and a few drinks in a Japanese-style bar, mostly catching up on life, before calling it a day and heading towards our hostel, close to Matsumoto Castle. Matsumoto High School Museum Having met up with our professor and a KU Leuven PhD student currently doing research in Kobe University, we joined the Shinshū University group on a bus ride to our first destination, the Matsumoto High School Museum . Located in an impressive forest park, Agatonomori Park (あがたの森公園), the museum is set in the historical Matsumoto High School, a Taisho-era elite prep-school for Imperial university entrance examinations, and the only one still completely intact today. The building itself, as well as the remaining artifacts displayed throughout the museum, gave some insight in pre-war elite Japanese eduction system, and especially of note was the high levels of expertise these students had over the German language. Satoyama Tunnel Our next visit was to Satoyamabe (里山辺): a guided tour through a mountain tunnel (地下壕) that our guide exapleined to be dug under horrible conditions by mainly Korean and Chinese forced laborers in the last months of the Second World War. The tour was held by a local non-profit organization called the Matsumoto Forced Labor Investigation Team (松本強制労働調査団), who organize these tours out of the principle idea that such history too belongs to the legacy of Matsumoto, and should therefore not be erased from public memory, lest similar tendencies occur again. The tunnel was constructed under order of Mitsubishi Heavy Industries, Ltd. Nagoya Aviation Works (三菱重工業株式会社名古屋航空製作所), whose factories were moved to Matsumoto City after an earthquake in December 1944 struck their main facilities in Nagoya. According to our guide, the company planned to built an underground plant to build parts for Mitsubishi's successor to its Mitsubishi A6M Zero fighter aircraft, the Mitsubishi A7M Reppū (烈風). Local records of the town note approximately 7000 citizens, although official records contain only several hundred people, leaving much of Matsumoto's wartime situation unknown. It should be noted that the billboard outside of the tunnel has been partly vandalized to hide the reference to forced labor, a topic sensitive amongst certain Japanese nationalists. Shinshū University Although Matsumoto is not a particularly large city, it is home to several universities, including Shinshū University. Founded in the 19 th century and nowadays focusing primarily on interdisciplinary research, the university has a fairly good reputation within Japan, and offers a beautiful setting for any potential exchange student. Indeed, one of our undergraduate students studying there as a short-term exchange student, was highly enthusiastic about her experience and education. After reflecting, in an ad-hoc seminar, on our brief experience with a local example of pre-war elite education, as well as but one local artifacts of daily life for forced laborers during the Second World War, we concluded our evening with an expected dinner and some drinks. Matsumoto Come next day, we joined our professor for breakfast at a vegan bakery maintained by an alternative lifestyle couple he acquainted as PhD student in Tokyo. I'm hoping one day I too can have such experiences, visiting lifelong friends in Japan made during my period as student (perhaps even as PhD student) in Japan. One can dream… After having said our goodbyes (my professor had duties in Tokyo), we headed to the nearby Matsumoto Folkcraft Museum , which had an interesting display of Meiji and Taisho-era billboards ( kanban , 看板). After a brief detour to a local hot spring (my friend from Kyushu University, despite now living in Fukushima, had never had the experience), we headed to the center of the city, to Matsumoto Castle . Built in the late 16 th century, Matsumoto Castle is one of few castles in Japan remaining almost completely intact, retaining its feudal era wooden interior rather than being rebuilt using concrete such as Osaka Castle. Designed as a fortress, much of its design caters to the use of long-range weaponry or entrapment, such as the hidden sixth floor used for unexpected attacks and hiding supplies. The idea that those without a visible Asian ethnic background are able to converse in Japan is not quite widespread, especially so in more touristic areas such as, take for example, Matsumoto Castle. While reading out loud a warning sign written in Japanese (warning visitors to be careful not to hit their heads upon ascending the stairs), an elderly man working there as local guide repeated, to my amusement (and very much to my friend's annoyance), in rather incomprehensible English (not intended negatively, his effort to cater to foreign tourists is commendable) the exact same message. Upon responding in Japanese, we were once again replied to in English. Thanks. Our final stop was the Nakamachi District and in particular its Nawate-dōri (なわて通り), a quaint shopping street running along a river. The street is commonly known as Frog Street, selling various frog-related trinkets due its historic association with, well, frogs 🐸. The story goes that a typhoon destroyed parts of the area, leading to the local frog population seeking shelter elsewhere. A symbol of this street, citizens built a ‘frog shrine' in hopes of the frogs' safe return (return and frog are both pronounced kaeru , 帰る and 蛙), but alas. Regardless of the lack of actual living frogs, Nawate-dōri is incredibly charming and a pleasant area to spent our last hours in Matsumoto at. Gallery Matsumoto Castle by Stevie Poppe ( https://flic.kr/p/25gYEw4 - CC BY-SA 2.0) ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2018/05/study-trip-around-matsumoto/","loc":"https://steviepoppe.net/blog/2018/05/study-trip-around-matsumoto/"},{"title":"Travels: Golden Week at Okinawa","text":"During this year's golden week in Japan, I joined several of my friends for a short trip to Okinawa Island, the main island of the Okinawa prefecture. Due its pleasant tropic weather and beautiful beaches at this time of the year, Okinawa is one of the more popular destinations during Golden Week, and for us too a good occasion to finally visit Japan's tropics. Brief history One of 47 of Japan's prefectures, Okinawa consists of over 100 small islands located between the island of Taiwan and Kyushu. Historically, the Okinawan Islands were home to the Ryukyu Kingdom and despite strong pressure from both Japan and China maintained a certain degree of sovereignty until complete annexation during the Meiji Restoration in the late 19 th century. For this reason, Okinawa does feel culturally distinct to the rest of Japan even today and, based on my conversation with Okinawans, it appears many Okinawans take pride in their identity as being Okinawan first, over a national pride of being Japanese. Due's Okinawa tragic role in the Pacific War (with war casualties accounting for over a quarter of its civilian population) as well as its subsequent U.S. occupation until 1972, there is a certain animosity amongst Okinawans against both the United States and the Japanese government. That animosity is further fueled by the continuous role of Okinawa as location for U.S. military bases, hosting approximately half of US military personnel in Japan. News stories of harassment by US military, as well as rising concerns over pollution, have led to a strong politically active base and frequent protests held against the Japanese government. On a more positive side, Okinawa is considered a tropical paradise and attracts tourists both Japanese and foreign. Okinawa's population is often mentioned in international news as having long record-spanning lifespans and living healthy lifestyles. Itinerary We took a short local flight from Narita airport to Naha, the capital of Okinawa, followed by a convenient monorail ride directly to the city. Although we hoped to explore some of the nearby islands for scuba diving, we arrived during the rainy season and limited our activities to the main Okinawan Island. Our first days would be spent around the city of Nago, on the northern part of the island, while we'd spent the remaining days around the capital. Nago We spent the first night in a small vacation house close to several beaches and the city of Nago (名護). Due the rain, we deciding on spending our first day around the Ocean Expo Park , famous for its Churaumi Aquarium (which held the record of largest aquarium in the world until 2005). The Kuroshiu Sea, a 35 meter long tank, is particularly impressive, hosting whale sharks as well as several large manta rays. The Park's massive surroundings are beautiful as well and perfect for spending a day walking around at (or taking the cute shuttle buses between different attractions instead). On our way back from the aquarium, we entered a nearby hotel, Marriott Resort, for dinner. Although reserved for guests of the hotel, we were invited to a small introduction of Okinawan dancing organized by a primary school class and their teachers. On our second day there, a Spanish diving instructor picked us up for some lessons on surfing. After we finished changing gear, two of our friends, from mainland China, shockingly revealed they had never been in the ocean before or even had any experience swimming. Nice. Myself, I'm not particularly known for having a good sense of balance, and unsurprisingly I never actually managed to stand upright on the board for longer as several microseconds. At least I tried. Naha Our next two days were spent in the main capital of Okinawa, Naha. Our hostel in Naha, somewhere around Ebisu Street (えびす通り), was located right next to a small arcade with famous '80s and '90s arcade games such as the Total Slug series; a steal for only 50 yen per round. In celebration of Children's Day (子供の日), there were various activities around Naha, including dance events at the popular central Kokusai Street (国際通り). The Okinawan cuisine (沖縄料理) is popular all over Japan (one of my professors was particularly fond of the kitchen and during 飲み会 would often take us to his favorite Okinawan restaurant), so we spent plenty of time around trying local delicacies. Okinawan soba, Rafute and Gōyā chanpurū are some famous local dishes, and I personally loved the Beni-imo (紅芋) ice-cream. Habushu (ハブ酒), a form of Okinawan liquor (awamori) mixed with the venom of a pit viper, is often seen for sale as well. Another must-visit was Shuri Castle (首里城), the historic main palace of the Ryukyu Kingdom, dating back to the early 14 th century. The current castle is however a reconstruction; the original was almost completely ruined after the Pacific War. Nevertheless the building is magnificent and unlike any castles I've seen in Japan, yet distinct enough from what I know of Chinese royal architecture of this period. The surroundings are extraordinary as well, with a stunning view on the city below and the distant sea. On our last day, we visited Naha's main beach, Naminoue (波上). There were some festivities with local musicians as well as a large picnic co-organized by both Japanese SDF members and US military. On our last evening, we visited Club Epica , a nightclub in the outgoing district, specifically popular amongst foreign tourists and US military. Due the holidays, there were several dance events and concerts inside the club, spotlighting some of Okinawa's traditions while embracing modern elements resulting in a form of psychedelic Okinawan trance musical style. We didn't stick around too long after the events were finished, however. Unfortunately some of the military staff present were quite drunk and started harassing some of the girls around, including two girls in our group; a sober view on what for some might by daily reality in Okinawa. Music While traveling, I make it a habit of finding good music by listening to local radio stations or asking recommendations to people I meet in hostels or bars, as my own form of personal souvenir. Although Okinawa is famous for ‘Queen of Japanese Pop' Namie Amuro and musicians like Mongol800, traditional Okinawan music finds its roots in Ryukyu culture and is both melodically and aurally distinct. Although I've known the song for years, I have no choice but to select The Boom's Shimauta (島唄), as arranged by Okinawan singer Natsukawa Rimi. I had the honor of rearranging and performing this song with several classmates at the emeritus celebration of Dr. Prof. Vande Walle , one of my best memories as undergraduate student Japanese Studies at KU Leuven. Conclusion It would've been fun to spent some more time around the other islands near Okinawa Island, and scuba diving in Okinawan waters was something I did look forward to. Nevertheless it was a fun, relaxed trip with friends and based on dialog with local Okinawans, some readings and my own experiences there, I did to a certain degree get a bit more insight in local problems plaguing the island. Gallery Okinawa by Stevie Poppe ( https://flic.kr/p/25h8ojM - CC BY-SA 2.0) ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2018/04/travels-golden-week-at-okinawa/","loc":"https://steviepoppe.net/blog/2018/04/travels-golden-week-at-okinawa/"},{"title":"Travels: South Korea","text":"As an undergraduate student at my Belgian university, I took several courses related to the Korean language and Korean history, which in turn made me quite eager to visit the country, given that I'd be living relatively close for the next year and a half. During my first semester studying in Japan, I was fortunate enough to make several good friends from South Korea. One of them had already left Japan by the end of the semester, and two of friends still remaining took the academic break in March as an opportunity to return to Seoul for several weeks, inviting me as well: an excellent way of celebrating the successful completion of my first semester in Japan. Brief history South Korea, officially known as the Republic of Korea, is an East Asian country east of Japan. Despite being sandwiched in-between China and Japan, the Korean Peninsula historically did well to repel repeated attempts of invasion and form an own distinct culture. With evidence of Korean settlements (confederacies) going back many centuries BC, and after hosting three major kingdoms ( Baekje , Silla and Goguryeo ) as well as smaller confederacies for seven centuries, the peninsula first reached unification in the 7 th century, leading to new peaks in cultural development. In recent history, Japan's victory in the 1905 Russo-Japanese War led to Japanese occupation, with official annexation in 1910; until Japan's defeat in the Second World War. The Korean Peninsula subsequently suffered the effects of the Cold War in an ideological proxy war between the North and the South. Although an armistice has been reached, the conflict remains far from solved. The topics of Japanese occupation and war crimes remain highly sensitive and are strong political weapons amongst nationalists in both countries. Nevertheless, the steady growth of Republic of Korea's worldwide soft power (韓流 / 한류, the Hallyu Wave) has warmed the younger Japanese generations to renewed relations with the Republic of Korea. Studying the Korean language is rapidly gaining traction, and many Japanese citizens visit the Republic of Korea for tourism or as exchange students. Likewise, under influence of decades of Japanese soft power (\"Cool Japan\"), Japan is an increasingly popular destination for Korean tourists and exchange students. 2 Itinerary When first planning my trip, I intended a fairly active trip covering Seoul, Sokcho and Busan, with a day-trip to Gapyeong. Right before departure however, my friends warned me of a sudden increase in yellow dust (PM2.5) from the Chinese mainland; it would be best to stay indoors as much as possible and make proper use of dust masks. I had never been confronted with air pollution to such an extent before, and on arrival I really was aghast. Without exaggerating at times it was difficult to see the actual color of traffic lights in front of me, and despite sufficient use of masks, I often felt sore in my throat or lungs. Needless to say, I altered my itinerary to reflect these changes and make the best of my time with my friends; centered mostly around Seoul instead, with a brief detour to the north for hiking, and to the DMZ. Seoul: Hongdae & Sincheon I stayed in a hostel in Hongdae , a ridiculously trendy, lively neighborhood centered around the prestigious Hongik art university ; in close vicinity to Ehwa Woman's University and Yonsei University . The growth of K-pop and the idol industry, with many starting idols performing in this region, has made Hongdae famous amongst Koreans and foreign tourists alike. At night, the Hongdae public space transforms into something more suitable for wild nightlife; with many clubs and cheap bars open until early in the morning. After dropping of my backpack, I met up with Suna, my Korean friend and senpai at my Japanese university, and visited the Songpa district, known for the Olympic Park , the Lotte World department complex and the nightlife district of Sincheon (신천). As far as first impressions go, Seoul felt spacious with careful city-planning centered around accessibility. Due the air quality, the massive Lotte World Tower landmark felt quite ominous however; according to my friends it is often referred to tongue-in-cheek as Sauron's Tower (사우론의눈). After we met up with Suna's childhood friend, we decided to have dinner in Sincheon, with my friends largely in control of the evening. They ordered a bunch of dishes I should try, and although I never actually asked for any alcohol drinks, ordered several pints of beer and a bottle of Korean Soju on top. Apparently it is a common habit to pour Soju into one's drink, known as Somaek (소맥). What a way to learn new cultural traditions. Like the izakaya culture of drinking and eating at Japanese pubs, it appeared quite common too in South Korea to spent the night hopping around various eateries until early in the morning, which ended up exactly how I spent my first night in South Korea. DMZ & Seoul Although I've enjoyed little sleep, I had a reservation for a tour to the Korean Demilitarized Zone . 3 Not a trip one does for sightseeing, a visit to the DMZ and most militarized region in the world is quite confrontational; the trip across the Freedom Road itself, a barren landscape with the occasional military vehicle passing by, a sign for what is to come. Through the tour our group had the opportunity to walk through The Third Tunnel , one of several tunnels discovered in the ‘70's and designed by North Korea for an invasion to the south until timely discovery (although North Korea claims the opposite). The proximity of the tunnel to Munsan and Seoul shows the reality of how close both countries were to renewed conflict. Next, we had a short stop at the Dora Observatory , an observatory on the demilitarized zone and the furthest foreign tourists are allowed to go without entering the Joint Security Area. binoculars offer a view on North Korean's so-called propaganda village as well as the city of Gaesung, but air pollution levels hindered our view this day. The final stop was a visit to Dorasan Station , passing the Unification Village. Dorasan Station is the northernmost station of South Korea and will serve as railway connecting the North and South when political relations improve. While the trip conveyed to a certain extent the tragedy of a people, families, being torn apart, I was moved by the message of hope and expectation towards future reunification. Shortly after noon I met up with Hani, a close friend, classmate and colleague at my Japanese university for a visit around the historical core of the city. First on the list was Gyeongbokgung , the most famous of five Joseon-era palaces in Seoul and a stellar example of traditional Korean architectural principles and Joseon court life. Of note were both the stories of several young volunteers guiding English-speaking tourists around 4 and the many English-language signs around the palace detailing the Japanese assassination of Empress Myeongseong. 5 We passed the nearby palaces of Changdeokgung and Changyeonggung , as well as the UNESCO designated Confucianist Jongmyo Shrine , before reaching the beautiful housing area of Bukchon . Still a residential area today, Bukchon is a prime example of traditional Korean houses known as hanok . We finished our day with a stop as the nearby Insa-dong market street for some shopping and dinner; as well as a brisk walk around the Cheonggyecheon creek flowing through downtown Seoul. Yellow dust reached peak levels the following day, and thus I was forced to remain indoors for most of the day. The National Museum of Korea was definitely a highlight however, and gave to certain degrees more context to what I had formerly been taught in Korean History and Korean Culture classes. In contrast, I must admit that the actual exhibitions held inside the National War Memorial , specifically the English explanations, were a letdown. That is to say, I felt the narratives the exhibitions were trying to portray lacked nuance and relied heavily on simplified, patriotic good versus evil rhetoric. This of course triggers questions on the actual social function of museums or memorials, let alone one on a war that is technically still on-going. Having just visited the DMZ myself the day beforehand, I experienced strong messages of reconciliation and unification, again exemplified through the statue of two brothers outside of the War Memorial. This contrasted at times with the contradictory narratives of the exhibitions itself. Anyway. I met up with Hani again in the afternoon, shortly before she would leave to Tokyo again. We spent some time in the massive Lotte complex; which houses one of the largest permanent cinema screens in the world. We were unfortunately not able to fully enjoy said screen to its full potential; having wasted time and money on the abysmal sequel to Del Toro's Kaiju-homage 6 Pacific Rim. Despite the bad air quality, people were relatively hesitant to wear protective masks. Instead, it was more common to see young people sit outside smoking and drinking all day, without a care in the world. My friend somewhat pessimistically taught me about a popular satirical term on the South Korean Internet: Hell Joseon , a term indicating the reality of South Korea's current socioeconomic state, with many facing intense pressure to achieve, harsh working conditions and rising unemployment. At this time, a popular meme on Japanese social media portrayed working conditions for millennials in South Korea to be of such drastic levels that one will either end up collapsing of stress ( Karōshi 過労死) or end up working in a fried chicken store. 6 Sokcho One of my good Belgian friends planned a two-week trip to Seoul and Japan. After meeting up at the station, we left to Sokcho early in the evening, hoping to spent a day trekking through the famous Seoraksan National Park . After an initial cable car ride to the Gwongeumseong Fortress ; we enjoyed more impressive views hiking to Ulsanbawi Rock (passing the beautiful Sinheungsa Temple on our way), tracing the Cheonbuldong Valley trail and viewing the Daeseung Falls . Evening fell before we realized, and the hike back was a bit difficult due the lack of light, and more worryingly, noises serving as stark reminders that we were alone in wilderness, surrounded by plenty of wildlife. Last day Sokcho is a seaside town, so how could we not spent our last hours in Sokcho visiting the local fish market? A tad overwhelming for sure, but after one of the employees saw our clueless foreign faces and helped set us up, I enjoyed by far one of the best fish-based meals I've ever had. I was however a bit taken aback by a cute, young Korean couple sitting near us. Reflecting the drinking culture I've experienced on my first night in Seoul, the couple must've drunken a dozen pints of beer with additional soji shots. A scene which felt rather out of place for a late-morning Wednesday. We returned early in the afternoon, in time to meet up with Ina, my Japanese language classmate and part of my main friend circle during the first semester. While I brought my Belgian friend (who does not speak Japanese or Korean), she brought one of her friends, who speaks neither Japanese nor English. Nonetheless it was a happy reunion and after a few drinks language barriers were of little importance. Although it was my last day in South Korea, my Belgian friend would stay a bit longer for sightseeing. We had however, under influence of the Hallyu Wave ourselves, agreed many years ago to one day go clubbing in Seoul together. The perfect occasion to make true on that agreement, I spent my last night in Seoul with my Belgian and Korean friends enjoying Seoul's nightlife; before heading straight to the airport next day; feeling dead, but satisfied. Music While traveling, I've made it habit of listening to local radio stations or ask for recommendations on any local music scene, as a form of personal souvenir. One of my Korean friends introduced me to a lot of k-pop idols and due sufficient exposure I've to a certain degree started to appreciate some releases. This trip was marked by GAIN's (가인) Paradise Lost. Conclusion Despite the bad timing of my visit (periods between December and March are notorious for declining air quality and a rise in PM2.5; often attributed to mainland China's relocation of its environmentally worst offending factories west of the Korean Peninsula), I've had a wonderful trip. It was a joy staying in Seoul, meeting my friends and experiencing the things I've learned about in classrooms in real life for the first time. Furthermore, I fell in love with the Korean kitchen 7 and have started cooking various dishes myself afterwards. I definitely look forward to visit again and resume my original itinerary. Gallery Seoul by Stevie Poppe ( https://flic.kr/p/2fSmbwX - CC BY-SA 2.0) ↩ Loose from an already present diaspora with ethnic roots stemming back to the Korean peninsula. ↩ Ordering a tour in advance is the only way one can visit the area. I went with VIP Travel and would recommend that one. Due military drills it was not possible to visit the JSA, the Joint Security Area, on the border of South and North Korea. ↩ A common way to gain real-life English practice. ↩ A tragic historical event often considered as the start of contemporary anti-Japanese sentiment in Korea. Empress Myeongseong's story has been adapted in a variety of media including an extremely popular 2001 South Korean television series. ↩ A Japanese genre of film featuring large monsters, popularized through the Godzilla series. ↩ ↩ Seriously, while known for being rather spicy, the richness of Korean cuisine is amazing. while I definitely enjoy such dishes as Samgyeopsal (삼겹살), Bulgogi (불고기), Gimbap (김밥), Cold Noodles (Mul Naengmyeon 물 냉면), and Gogigui (고기구이), with a side serving of Kimchi (김치) or Buchimgae (부침개), my favorites are by far Dak galbi (닭갈비, quite popular on Japanese social media), Kimchi Jigae (김치찌개), Bibimbap (비빔밥) and Crispy Korean chicken (Yangnyeom tongdak, 양념통닭). For those in the Tokyo neighborhood, I recommend Shinchan (辛ちゃん) for its chicken, Shijan Dakgalbi (市場ダッカルビ) for its Cheese Dak galbi, and Saemaeul Sikdang (セマウル食堂) for Kimchi Jigae. I've eaten in those places more than I care to admit. Don't forget to try the Korean sparkling rice wine makkoli (막걸리)! ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2018/04/travels-south-korea/","loc":"https://steviepoppe.net/blog/2018/04/travels-south-korea/"},{"title":"Going Abroad as a Research Student","text":"It's been a while since I've posted my last blog, and while I've had some ideas on new posts, I had to prioritize schoolwork, including finishing my graduation thesis and preparing my cultural exchange. Courtesy of my university I received the opportunity to study and live in Tokyo for a year and half at Chuo University with MEXT scholarship, as part of my Master degree in Japanese Studies. I'm writing this small blog post exactly a week after arriving in Tokyo, in order to talk a bit about my university, my first impressions of my surroundings, and my personal experience with the application process for my scholarship. Chuo Universty Chuo University (中央大学) is a private-run university with several campuses spread throughout Tokyo, and containing a plethora of faculties, graduate schools and researchers. It's mostly known for its legal studies, dating back to the 19 th century, and has many well known alumni ranging from economics and politics (including former Prime Minister Toshiki Kaifu) to arts and letters (including popular director Makoto Shinkai of Kimi no Na wa 君の名は。 fame). I'm studying at the largest campus, Tama Campus, which is located in Hachioji: a city in the western portion of Tokyo, and about an hour by public transport to more central parts of Tokyo such as Shinjuku. As a research student I have some liberty in the classes I can take up; they're either thematically similar to master classes of Japanese Studies at Leuven, or support the global understanding necessary to write my master thesis. Thus I'll mostly be taking classes from the Graduate School of Letters (specifically the Socio-Informatics course) and Graduate School of Policy Studies, as well as an introduction to Japanese Law from the Graduate School of Law. If time permits, I hope to continue my Korean studies as well. I was pretty overwhelmed by the size of the campus. It has eleven main buildings, along with a variety of student facilities for after-school activities such as clubs (クラブ) and circles (サークル), sport rooms, several restaurants, etc. I haven't gotten the hang of it yet and I'm sure I'll get lost a lot over the next year and a half. I'm pleased to see these amount of facilities available however, and I really look forward to the start of this school semester (21th of September). Although it is fairly remote (田舎), it's definitely a beautiful, lush location, with a backdrop of mountains and forests. Finally, the food at our main school restaurant and nearby surroundings is quite diverse, tasty and incredibly cheap (approx. 300 yen or 2.5€ for a large plate of カレーライス Japanese curry? Count me in! ). Mext I was lucky enough to receive a MEXT (also known as Monbusho) Research Scholarship, offered by the Japanese Ministry of Education, Culture, Sports, Science and Technology, which will cover most of my major expenses in Japan. I applied for the scholarship through university recommendation, after a new collaboration between KU Leuven and Chuo University had been made. It was a rather last minute opportunity, and I had to write my research proposal and fill out the MEXT application in a matter of days; something usually requiring weeks to months of preparing. Luckily, I had already started to prepare my research for my undergraduate thesis, and with some supports from my professors had a good idea on how to extent the scope. In my application, I had to select two professors at Chuo University whose bodies of work most strongly fit how I envisioned my research. After one of them agreed to support me, we held a brief interview before I could be officially accepted. This took place about two months after my initial application, and it took another five months until I received positive news from the Japanese Ministry of Education. Finally, my status as MEXT-supported scholar is that of Research Student, rather than Japanese Studies student or Undergraduate university student, as was often the case before exchange projects at KU Leuven became part of the Master's Degree. This status gives research students the opportunity to extend the scholarship after entering a local Graduate Degree Program or even further as PhD student. There are several extensive blog posts on writing your research proposal and filling out the application. If anyone stumbled upon this blog is interested in further details on the process, feel free to leave a comment. But for now I recommend these blogs: Embassy-Recommended MEXT Scholarship 2018: How to Get Started Going to Graduate School in Japan on a Monbushō Scholarship How I got the Monbusho Research Scholarship (Part 1 of 4) Logistics After an excruciating seven months of waiting, I got my confirmation and realized I'd actually be living in Japan for at least 1.5 years. As a MEXT-recipient, I discovered a great deal of work was done for me, and my application at the Japanese embassy for a student visa was accepted almost immediately, without the usual checks for solvability. Finally, matters of lodging and booking flights were swiftly dealt with by the government and university as well. My flight included two pieces of baggage as well as the usual on-board bag, so I packed about 50KG worth of clothes, cooking materials, electronics, omiyage , … I imagine I'll need in my time here. Others before me have written extensively on preparing for long exchange periods, including one of my senpai at my university. I'll link them below as well. The Ultimate Packing List For Japan 2 Suitcases + 1 Year = Study Abroad Packing List Financially, it's definitely recommended to make a local bank account. Transferring money from one's home country to Japan will be much cheaper in the long run; especially since the scholarship's monthly stipends are deposited on a local Japan Post Office account. Similarly, I'll also be getting a local SIM card. The internet taught me that data-only SIM cards for accessing 4G are fairly cheap, around 1000 - 1500 yen (approx. 9€ - 14€) on a monthly base, so this looks like the best option right now. Lodging I'm staying at an apartment block privately owned and refurbished by Chuo University in collaboration with other nearby universities. It is pretty small, but definitely sufficient as a student, and luxuries include my own small bathroom and kitchen area, as well as a washing machine. I've been using a 100-yen store to liven it up a bit (still needs some work, definitely need a microwave-oven, water cooker and rice cooker in due time). Location wise, it's not too shabby. It takes about 25 minutes by foot to reach my campus, and similarly it's about 20 minutes to a central part of town called Tama Center; a very pleasant shopping district right next to an apparently famous Hello Kitty themed attraction park . There's supposedly held a lot of concerts and events held here, and in my first week we discovered a local festival with several live performances and an open-air theater piece. As my university is fairly remote, my apartment surroundings feel rather suburb-ish as well; but the green surroundings and lack of commuter passes make up for it, and I'm sure I'll come to appreciate the quiet here. Reaching Shibuya however would take me approximately an hour, and set me back about 500 yen (approx. 4€) for a single trip. As it's still vacation at the time of writing this, I haven't met too many people yet; but through the various ordeals newly-arrived exchange students had to deal with (such as registering at the municipal office, sessions on emergencies and safety, …), I've already had a pleasant time so far. Also while heading for Shibuya this weekend to meet up with schoolmates here on holiday, I had to ask someone the road after getting lost, and randomly received a matcha bagel. It's the little things that count~ Gallery These are only a couple of pictures I've taken so far, it'll grow over time as I write more posts on my exchange period. Hachioji Suburbs by Stevie Poppe ( https://flic.kr/p/XEtf42 - CC BY-SA 2.0) ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2017/09/going-abroad-as-a-research-student/","loc":"https://steviepoppe.net/blog/2017/09/going-abroad-as-a-research-student/"},{"title":"Travels: One Week of Andalusia","text":"Having finished the final exams for my undergraduate degree in Japan Studies, we decided to spend some time off by visiting a close friend doing an internship in Seville, Spain. We used our time there to get a glimpse of beautiful Andalusia and were not disappointed by the stunning beauty of its architecture and nature, its fascinating history and, importantly, its wonderful culinary traditions. Brief history Andalusia, an autonomous region at the most southward point of Spain, has had due to it's geographical location - a gateway between Europe and Africa - strong ties with various cultures such as the Phoenicians, Greeks, Romans, Byzantines, Jews and the Muslim Moors. These cultural influences have, of course, left a strong mark on the development of Andalusia, both in its national identity and through cultural markers such as architecture, music, dance, and cuisine. Itinerary Our friend let us stay in her apartment in Nervion, the shopping district of Seville, approximately 20 minutes by foot from the historical center. Taking her apartment as starting point, we arranged most of our activities around Seville and nearby regions. It was quite a pleasant surprise to be located only a short walk from one of the more appraised tapa 2 restaurants in Sevilla. We spent most of our time in Seville, but used a local carpooling service for two day-trips, to Granada and Ronda. Seville Seville, the capital of Andalusia and Spain's 4 th largest city, flourished in trade during the middle ages and thus has a well-developed and maintained city center. Amongst our highlights there was the 15 th century Cathedral of Seville , the third largest church in the world, built over a late twelfth century mosque. It contains the remains of the historical controversial figure Christopher Columbus. The area around the cathedral (commonly known as the Jewish Quarter ) is considered the historical center of Seville and is a delight to stroll through with it's small cobblestone streets and variety of boutiques and restaurants. Our friend introduced us to a small but crowded bar with late-night flamenco performances, a sure marker of the vibrant nightlife in Seville. To us, the most impressive site was Réal Alcázar , UNESCO World Heritage Site and palace built in Moorish architectural style across the 10 th to 14 th century. It has recently gained new fame as setting of Dorne in the hit-series Game of Thrones. During our trip, we found out that a small festival of late-night concerts were held in the garden of Alcázar and promptly booked tickets. Despite spending over an hour looking for the correct entrance to the event, we were able to join one of the concerts: a performance of baroque pieces for cello, violin and organ. Finally, we visited the Plaza de España , a plaza containing mainly government buildings designed in a variety of architectural styles, including Art Deco, Moorish and Neo-Renaissance, built over period of 15 years as a means of promoting tourism during the 1929 Expo held in Seville. The plaza has gained renewed touristic interests due to being used as set piece in the Star Wars saga. During our trip, we met up with several local people that our friend had befriended during her stay as intern in this city. Unfortunately neither of us spoke Spanish very well, but it was still a pleasant way of learning more of Seville life and some of it's hardships, including low wages, high unemployment, and an unbalanced ratio of man and women. Granada We spent a day in Granada , commuting by car using a local carpooling service. We planned on visiting Alhambra , the Moorish famous palace and fortress complex ended up as Royal Court after the Christian Reconquista. Unfortunately due to an error, our reservation for Alhambra did not go through, but we were however still able to visit the gardens and outer areas of Alhambra, a beauty on itself. Afterwards we spent the evening strolling through town and watching the sun set on Alhambra from a square on a hill. It was a pleasant experience to, after a year of learning the language, speak a few words of Korean with a Korean tourist standing next to us. Ronda Using the aforementioned carpooling service we spent a short day in Ronda , a small city in the province of Málaga , situated on a canyon. We spent part of the day hiking down the canyon until we had a beautiful view on the city and it's bridge spanning the canyon. Music When traveling, I make it a habit of finding new music by listening to local radio stations or asking recommendations to people I meet in hostels or bars - It's my own personal souvenir and sticks better in my mind than regular touristic souvenirs. During our trip we frequently passed street musicians, most of all guitarists performing pieces in Spanish Flamenco style. My father (and as it turns out, my partner's mother too) often played these pieces (having lived in Andalusia himself) during my childhood. Hearing these compositions once again after many years, a sense of nostalgia fell over me, and thus it felt appropriate to pick the following two compositions, Recuerdos de la Alhambra - Francisco Tárrega and Asturias - Isaac Albéniz . In my limited opinion, the following renditions have captured the precision of these pieces' tremolo techniques perfectly. Conclusion We had some misfortune during this trip, including the errors in the booking of Alhambra. The period of our journey also knew a strong heatwave that significantly worsened traveling conditions by day. Yet, the charm and beauty of the Andalusian cities we visited were undeniable. The architecture, the lovely small streets and the lush nature sure were lovely to gaze at while strolling around. But most of all, we were able to get some much-needed rest and splurged a bit on excellent food and wine - definitely a highlight of this trip. Gallery Alhambra by Stevie Poppe ( https://flic.kr/p/WhzXyc - CC BY-SA 2.0) ↩ An appetizer or snack, although nowadays small complicated dishes combinable to a full meal. The origin of \"tapa\" is often explained as meaning \"cover\"; supposedly small plates of snacks were used to cover drinks from fruit flies. ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2017/07/travels-one-week-of-andalusia/","loc":"https://steviepoppe.net/blog/2017/07/travels-one-week-of-andalusia/"},{"title":"New Song: IROHA 【いろは歌】","text":"During a course on classical Japanese ( kobun 古文 ) last semester, we were introduced to a particularly well-known Heian-era poem titled Iroha – famous for using all characters of the Japanese syllabary exactly once 2 . Attributed to Kūkai (founder of the Japanese Esoteric sect of Buddhism), the layered meaning of the poem (and the fact we had to be able to recite it for my exam) struck a chord with me. I wrote a short melody to help me memorize it properly and, after my exam period, spent some time getting acquainted with electronic music composition in order to further expand upon that melody. いろは歌 I begin most of my compositions by playing the piano. I tried to create an otherwordly, esoteric vibe through constant shifts in modulation and relying on ninths and dissonance in the melody. Once I had my base backing track done, I spread the different lines across piano, pan drum and some ambient pads, wrote a sparse drum-track using chimes, woodblocks and taiko and practically drowned the whole in reverb. The vocalist's thin, airy voice works especially well for the kind of setting I hoped to express. The second half in particular used heavily distorted samples of her voice in an attempt to derail and build up tension in similar fashion (hopefully) to the original poem. It also allowed me to become better acquainted with filters and techniques commonly used in electronic music. Several authors debated a hidden message in the original Man'yō-gana version of the text, by putting together the last syllable of each line, which could be read as toka [toga] nakute shisu ( 咎無くて死す , ‘die without sin'). I tried to incorporate this in the song as some form of hidden message, using distorted whispers. For the YouTube video I created a wallpaper using a photograph I once took at the grave of Tokugawa Ieyasu in Nikko. At the time of the picture, a typhoon rapidly approached our region and due to heavy rainfall, we were practically alone. Nonetheless, the storm's influence on the ancient cryptomeria forest and the bronze urn of Tokugawa Ieyasu left for a breathtaking and very humbling sight. I did a bit of editing and used several free Japanese calligraphy fonts to display the contents of the poem. I found the english translation, by Esoteric Buddhist specialist Prof. Dr. Ryūichi Abe, to be particularly poetic and fitting for the kind of atmosphere I tried to create. That's it for now! I'm in the process of rearranging and mixing several compositions in my spare time so I'll be sure to post here when I have something new tangible. I'm still really new to composing music digitally, so all feedback is welcome! Further reading The Japanese Page: Iroha : aside from providing further insight in the history of the poem, the writer made an interesting in-depth analysis of each phrase of the poem, definitely worth reading! Nikko Shrine by Stevie Poppe, edited ( https://flic.kr/p/LeKi4r - CC BY-SA 2.0) ↩ Iroha uses 47 kana, without any form of voicing ( dakuten 濁点), of which two characters (ゐ and ゑ) are now considered archaic and obsolete. The best known example of such a pangram in English would be \"The quick brown fox jumps over the lazy dog.\" ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2017/03/new-song-iroha-uta/","loc":"https://steviepoppe.net/blog/2017/03/new-song-iroha-uta/"},{"title":"織田信長とアルベール一世：我々の国民の共有している歴史","text":"私は先週、ブリュッセルで行われた日本語のスピーチコンテストに参加して、「ベルギー・日本友好１５０周年」のために歴史的なスピーチを発表する機会をいただきました。すると、参加者の他者が非常に高いレベルにあったし、質疑応答もやや恥ずかしくなっても、何とか第三位に入賞しました。このブログのエントリーは私が書いたスピーチの原稿です。 スピーチ 会場にお集まりの皆様、こんにちは！ルーバン大学日本学科3年生のスティービーと申します。私は子供の頃から日本の文化と歴史に大変興味がありました。そして日本学科に入学して2年間勉強をすることで自分の知識をもっと広め、深めることができました。更に、来年日本へ留学する可能性があることや、今年が「ベルギー・日本友好150周年」の年であることから、日本の歴史だけでなく、特に独立前のベルギーと日本の400年も前に遡る両国が共有している歴史への関心も高まってきました。そのため、この発表「我々の国民の共有している歴史」では、二つの歴史的逸話を紹介して、ベルギー人と日本人の友好についてお話したいと思います。 この共有されている歴史、すなわち、両国間の共感は、私たちの国の歴史的展開がほぼ同じように進んだからだと考えます。ベルギーも日本も強大な国々が隣接していたにもかかわらず独立することができました。ベルギーがフランスやドイツの大国に挟まれながらも何とか独立したのと同様に、日本も中国やロシア、欧米の中で、独立国として次第に世界で最大の経済大国の一つになりました。そのため、明治時代に岩倉使節団が派遣された際、「日本人と同じような独立の気概を持っているベルギー王国をモデルとしてはどうか」という記事を書いた日本人記者もいました。また、第一次世界大戦中にドイツ帝国の攻撃を受けたベルギーは「意志が強い、勇敢な国である」と日本で報道されました。 織田信長とアルベール一世 当日のベルギー王アルベール一世がフランスの国境近くにある村から抵抗をし続けていた間、東京と大阪の朝日新聞が協力して、ベルギーを支援するために募金キャンペーンを行いました。更に、ベルギーの国民を励ますために、ベルギーの国王の誕生日に刀を献上する企画も立てました。そのため、朝日新聞記者の杉村廣太郎がベルギーに派遣されました。１９１４年に彼がイギリスを経由した際に、そこで苦しむベルギー避難民を目撃し、戦争の深刻な状況について朝日新聞に報告しました。戦争の影響による難しい旅路を経て、やっとのことでベルギーの亡命政府にたどり着き、国王に謁見する機会を得ました。彼は、朝日新聞の社長が書いた国交の手紙を渡した後、国王に戦国時代の織田信長が用いていたという刀を献上しました。その象徴的な行為がベルギーだけではなく、世界中の報道機関によく受け入れられたのはいうまでもありません。その結果、ベルギーの国民も日本人に対して非常に友好的な印象を持つようになりました。 関東大震災 １９２３年の関東大震災が起こった時、ベルギー人がためらわず、救援活動を行ったり資金集めをしたりしたのは、この友好的な日本との関係のためだと言えます。王族と、新たに設立されたベルギー国内委員会が協力して大規模な救援活動を実施しました。私の故郷アントワープを含むベルギーの全国各地でも教会と戦争の退役軍人が協力して「Japan Day」という催しを行いました。他にも、ベルギー人の芸術家達が作品を集めたり新作を発表したりして特別な貢献をしました。まずはブリュッセルで、次に日本で作品の展示発売や展覧会を開催し、売り上げや入場料の利益は全て救援活動のために寄付されました。日本の展覧会だけでも3万５千人もの観客が訪れたと報告されていて、その上、日本の皇太子様と皇后様が30点もの芸術品をご購入されたと聞いております。 もちろん、多くの国が日本への支援に参加しましたが、ベルギーより多くの資金を集めたのは大国のイギリスとアメリカだけでした。それはベルギー人と日本人の相互の友好関係と共感の証しだと思います。私は、日本学科で学ぶベルギー人の学生として、我々の両国がこの豊かな道を共に歩き続けられるように心から願っています。 以上です。ご清聴ありがとうございました。 参考文献 Japan & Belgium: An Itinerary of Mutual Inspiration, ed. W.F. Vande Walle, pp. 187-213. Tielt: Lannoo, 2016. Japan & Belgium: Four Centuries of Exchange. Brussels: Commissioners-General of the Belgian Government at the Universal Exposition of Aichi 2005, Japan. 在ベルギー日本国大使館の「日本・ベルギー友好150周年」の 公式ロゴ . ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2016/12/oda-nobunaga-to-aruberu-issei/","loc":"https://steviepoppe.net/blog/2016/12/oda-nobunaga-to-aruberu-issei/"},{"title":"Oda Nobunaga and King Albert - The shared history of our nations' people","text":"For this year's Japanese Speech Contest in Brussels, I had the pleasure to prepare and hold a speech in honor of Belgium and Japan's 150 th year of mutual friendship. Despite some hickups and an incredibly high level amongst the other candidates (meeting all these excellent Japanese learners was both an extremely humbling experience as well as a perfect motivation booster for the coming years) I was awarded 3 rd prize. This blog contains my speech's English translation. Speech Hello everyone! I'm Stevie, a Belgian student at the university of Leuven and currently third year student in the faculty of Japan Studies. Lately, through my studies and the realization of 150 years of amity between our two nations, I've gained an increased interest in our mutual history; a long shared history dating from before Belgium was even officially recognized an independent country. That's why, through this presentation, I would like to focus on two historical anecdotes in history that show not just a friendship between our countries' governments, but a friendship between our people as well. It is my belief that the empathy shared between our people stems from similar historical developments. Just as Belgium found its independency while cornered between the great nations of France and Germany, Japan rose as an independent world power despite the large Chinese and Russian empires on one side, and the Western industrial powers on the other. Thus, during the Meiji Period and the subsequent Iwakura Mission, Belgium was occasionally reported as a potential model nation with its people possessing a high spirit of self-reliance and independence, just as the Japanese people have. King Albert and Oda Nobunaga Following this logic, during the first world war, Belgium was reported on a daily base in Japanese media as a brave, strong-spirited country that fell victim to the conquering German empire, while Belgian King Albert continued resistance from a small village near the French border. Eventually, the Tokyo and Osaka Asahi Newspaper launched both a campaign to collect funds for Belgium and planned the presentation of a Japanese sword to the Belgian king on the occasion of his birthday and as a means to encourage the brave resistance of the Belgian people. Sugimura Kotaro, a journalist of the Asahi Newspaper, was sent to deliver the sword and travelled through Great Britain in late 1914. Here, he witnessed a great number of Belgian refugees and reported of their situation to his home country's newspaper. After a difficult route he finally managed to reach the Belgian government in Exile and was allowed an opportunity to meet with the king. He presented the king a message of dedication from Asahi Shimbun's president as well as hand him the Japanese sword, a sword said to have been used by Oda Nobunaga himself; the daimyo known to attempt to unify Japan at the end of the Warring States period. It goes without saying that this symbolic act was very well received in Belgian- and international press, and gave a good impression of the Japanese people in the eyes of Belgians. The Great Kanto Earthquake It could be argued then that when disaster struck upon Japan in 1923 with the devastating Great Kanto Earthquake, Belgium reacted without hesitation to set up its own relief help and fundraisers to support a Japan, a friendly nation in dire need. The royal family co-operated with a newly found committee (\"the Belgian National Committee for Aid to Japanese Disaster Victims\") to oversee a large-scale relief effort. Locally, war-veterans and churches worked together to set up Japan-Day events across the country, including in my own hometown of Antwerp. Belgian artists also made a unique contribution by creating new, or collecting and donating existing, art pieces and exhibiting them first in Brussels, and later on in Japan. All proceeds of sales and entrance fees went to the relief effort. Over 35 thousand visitors in Japan alone came to appreciate the exhibition, and amongst the art-purchasers were the empress and crown prince, buying over 30 pieces in total. Of course, many countries participated in fundraisers for Japan, but Belgium's effort was so strong they were preceded only by the large nations of the United States and the United Kingdom – surely a sign of our people's mutual compassion and friendship, and hopefully a path we're able to continue through the future, together. With that, I would like to end my presentation. I hope you have enjoyed it and thanks for your attention. Sources Japan & Belgium: An Itinerary of Mutual Inspiration, ed. W.F. Vande Walle, pp. 187-213. Tielt: Lannoo, 2016. Japan & Belgium: Four Centuries of Exchange. Brussels: Commissioners-General of the Belgian Government at the Universal Exposition of Aichi 2005, Japan. The official logo in celebration of 150 years of friendship, courtesy of the Embassy of Japan in Belgium . ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2016/11/oda-nobunaga-and-king-albert-the-shared-history-of-our-nations-people/","loc":"https://steviepoppe.net/blog/2016/11/oda-nobunaga-and-king-albert-the-shared-history-of-our-nations-people/"},{"title":"Japan Cultural Festival in Antwerp 2016","text":"This weekend the NHK Culture Center and the Embassy of Japan in Belgium organized the 30 th edition of the Japan Cultural Festival, held in Antwerp in celebration of 150 years of friendship between our two nations. Together with a few classmates and upper class students I had the honor and pleasure of joining this event's staff of interpreter volunteers, spending my last 3 days assisting two incredible (and incredibly kind) artists convey their works to a large crowd of highly interested Japan aficionados. The festival included a wide variety of exhibitions, demonstrations and performances related to Japanese artistry: dances, drum performances, calligraphy, clothing, tea ceremonies, etc. During the festival I was responsible for a stand on artistic calligraphy and a Japanese embroidery technique called sashiko ( 刺し子 ) 2 , two fields of artistry I had no prior experience in and thus learned a lot about through my interpreting those 3 days. Interpreting as barely just a third year student was quite an experience. The amount of technical terms related to embroidery processionally made it pretty difficult to accurately translate everything in real time but I think I somehow managed to convey the overall meaning most of the time. Something that quite surprised me was how engrossed I got in my interpreting as well as the group sense we fostered. I felt a certain sense of pride every time guests complimented on our booth's exhibitions. Another thing that surprised me was the high interest in these art forms I originally thought maybe to be too specific to draw large crowds. Both days of the event were highly visited and left us drowned in work to the point we barely had time to eat a snack at noon. At the end everyone was exhausted, but it was definitely a good kind of exhaustion, and spending the evening together having dinner in the city center of my hometown made it all the more a pleasant memory! (me and the two artists, Hashimoto Shiori and Chiharu Minato, at our booth) (The opening ceremony was extremely formal, but I was pleased to occasionally perceive a little joke in the Japanese speeches that didn't translate well in Dutch) (Alas, due to a dead cellphone battery I wasn't able to take a lot of pictures of the events.) Opening ceremony of the Culture Festival. ↩ Sashiko clothes are commonly made from Japanese indigo blue dyed cotton clothes embroiled with distinct patterns - https://en.wikipedia.org/wiki/Sashiko_stitching ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2016/11/japan-cultural-festival-in-antwerp-2016/","loc":"https://steviepoppe.net/blog/2016/11/japan-cultural-festival-in-antwerp-2016/"},{"title":"Writing a Musical?","text":"Last year, I cooperated with several other students at my music academy to write the music for a small local musical called \" Dating - a musical for the highly educated \" (a supposedly ironic, yet pretentious title I'm less than fond of, honestly). Being slightly unsatisfied with the end-result of my own compositions, I've since started rearranging them. I intend to describe the process in this little blog post. The Idea Under the wings of our arrangement teacher, we collaborated with the Word & Song division of our music academy and the Kontich Orchestra. The W&S division provided the actors, came up with a general concept and devised a tentative track list as well as additional background information we were to use as guidelines in our compositions. We held frequent meetings with the Kontich Orchestra for rehearsals, further fine tuning, and to get a better feel of each musician's capacities and the sound of their instrument. In the end, due time limits, I only contributed two songs but I'm happy to have been part of the project despite the in my opinion somewhat pretentious premise. The following flyer was distributed around town: The Story \" Dating - a musical for the highly educated \" involves a group of supposedly less than highly educated adults from out of town who've met up for a double date using a popular dating site. A couple of teenagers, obsessed with selfies and being young (the average age of the writing team was about 50 years by the way) take note of this and decide to ruin their fun by pretending to be each others date. Somewhere near the end, things get heated up so much that a member of the audience, using classic \" breaking the fourth wall \" 1 techniques, calls the musical to an abrupt end by spouting some idealistic wishes on the concepts of love and relationships. At this stage, Bacchus 2 and Cupido make their entrance in a rather absurd fashion and save the day by convincing the audience that yes, it's important to believe in love. (I just wrote the music! I take no part in this plot!) The Songs The Banana Caves This song is played right at the start of the second half of the musical. As the cast enters the bar and spends a while having a few drinks too many before breaking out into dance, my guidelines were to create a slow \"ballad\" that leads into a funky second half. Unfortunately, I have no proper recordings of the actual musical, but the following track is my work-in-progress rearrangement. As I've yet to run it through a VST it sounds quite synthetic but I hope you enjoy it nonetheless. This might sound silly, but I'm rather fond of videogame composer David Wise's soundtrack to the original Donkey Kong Country games for the Super Nintendo console, games I grew up with as a kid. I tried to create a similar atmosphere in my song and named it The banana Caves as a kind of homage. It's basically done but the part between 3:50 and 4:00 needs some further work. :P Creepy Intermezzo The second piece I wrote was a very last minute composition used during the intermezzo, a time when the actors needed a brief moment to rearrange the decor. I wanted to create something mysterious and film-ish to keep the audience in suspense. At the time I had been playing a lot of Erik Satie's work and based my theme's base on his Gnossiennes. The further arrangements were inspired by the gothic soundtrack of Coppola's \"Bram Stoker's Dracula\" . In retrospect, I should have read the script more carefully; the particular mood I tried to convey with this track wasn't the most suitable for the relative lighthearted musical it was meant for. I do like the piece's main theme but I'm not satisfied with the quick ‘n dirty arrangement so I hope to start over some time. That's it for now! I'm in the process of rearranging and mixing several compositions in my spare time so I'll be sure to post here when I have something new tangible. a theater convention intended to heighten a piece's realism: actors act in such a way that an imagined, invisible wall seemingly separates the audience from the stage. Breaking this wall refers to the act of proceeding contact between the stage and the audience. ↩ Bacchus , the Roman god of agriculture and wine and equivalent of the Greece Dionysus. He's often associated with hedonism. ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2016/10/writing-a-musical/","loc":"https://steviepoppe.net/blog/2016/10/writing-a-musical/"},{"title":"Narrating Newspapers: Weekly Vlogging as a Japanese 練習","text":"Note : currently on hold due my upcoming exams. Priorities! In order to up my Japanese pronunciation and become more fluent in Japanese text recitation, I've tried getting acquainted with vlogging in Japanese. I'll (hopefully) publish weekly videos of myself narrating a newly posted news article published on one of the mainstream Japanese newspaper websites, using a classic streaming-style set-up so any potential viewer can read along. If time permits, I've found it a fun exercise to try and translate the article as well as create subtitles (maybe I should try fan-subbing some anime or drama one of these days, if I ever find the time, that is~). As this will become a weekly thing I won't be posting new blog-articles for every single video I do, but feel free to follow me on twitter or subscribe to my YT channel . For now it's limited to these narrations and ( wip ) music, but actually I've always wanted to try actual vlogging and/or podcasts.... Maybe it'll happen one day (when I'm studying abroadかな～). Videos Week 9: 殺人未遂容疑 This Mainichi Shumbun article is another one related to JR. A guy tried to push two people of train platforms in Osaka and is now wanted for double attempted homicide. The original article can be found on Mainichi Shimbun ( 線路に６３歳女性突き落とす 男逃走 大阪 ). As usual I've included English subtitles. (22/11/16) Week 8: ＪＲ山手線 This week's article is on a small accident on the JR Yamata line caused by a overheated cellphone charger. The original article can be found on Mainichi Shimbun ( ＪＲ山手線 車内で携帯充電器が発煙 ). As usual I've included English subtitles. (22/11/16) Week 7: 織田信長とアルベール一世 Due the Brussels Japanese Speech Contest this weekend I didn't record any news articles, but instead uploaded a recording of my speech entry. I've posted the original speech in Japanese, as well as an English translation on these blogs: Japanese & English ! (27/11/16) Week 6: ＰＴＳＤ治療 This one's an article on a possible treatment to PTSD. The original article can be found on Asahi Shimbun ( 恐怖の記憶、書き換える技術開発 ＰＴＳＤ治療に効果か ). As usual I've included English subtitles. (22/11/16) Week 5: トランプ氏 This time I'm reading a Nikkei article on Trump's first public interview since his election as President of the USA. The original article can be found on Sankei News ( トランプ氏、 犯罪歴 ( はんざいれき ) ある 移民 ( いみん ) を 強制送還 ( きょうせいそうかん ) 最大300万人}} ). (14/11/16) Week 4: トヨタの電気自動車 My fourth reading is on a Toyota press-release regarding its policy on electrical vehicles. The original article can be found on Sankei News ( トヨタ、 電気自動車 ( でんきじどうしゃ ) を量産 平成３２年までに本格参入…バッテリー性能向上で走行距離改善 ). (08/11/16) Week 3: 渋谷のハロウィーン This week's newspaper article is a report on the pre- Halloween festivities around the shopping districts in Shibuya, Tokyo. The original article can be found on Sankei News ( 「安倍マリオ」効果か？ スーパーマリオ仮装売れ行き好調 週末の 渋谷 ( しぶや ) ルポ ). As usual I've added subtitles in CC. (30/10/16) (not sure if that's a bad angle or if I should really lose some weight soon…) Week 2: くまモン My second vlog is a bit more light-hearted and covers the history of the famous Kumamoto Prefecture character Kumamon , again covered in an Asahi Shimbun article: くまモン、やせていた「過去」 失敗糧にキャラ再設定 . Subtitles are available in CC. (23/10/16) Week 1: ブラック企業 My first vlog is on the ブラック 企業 ( きぎょう ) , the so-called Japanese Black companies . I'm reading the Asahi Shimbun article ブラック企業、見抜くには…転職者から情報、離職率調査 and attempted my own translation as seen in the subtitles (I've added them as soft-subs in YouTube, so you can (de)activate them at will). 2 (16/10/16) Photograph of my desktop set-up. My voice-recorder is a Blue Snowball model. ↩ The beginning of the video is a bit rushed, sorry! ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2016/10/narrating-newspapers-weekly-vlogging-as-a-japanese-renshu/","loc":"https://steviepoppe.net/blog/2016/10/narrating-newspapers-weekly-vlogging-as-a-japanese-renshu/"},{"title":"Anki Add-on: Japanese translations for Korean vocabulary","text":"I have recently started studying Korean as part of the minor I took up at my university. While browsing the internet for helpful resources I found that, like us, a remarkable amount of students of the Korean language have knowledge of Japanese and/or study both simultaneously. As anyone browsing my blog might have guessed, I'm pretty fond of using Anki for assisting in vocab acquisition, and thus use it for Korean vocabulary as well, using both existing sets and my own based on our textbooks. 2 Having a broad base in Japanese, I thought it would be more efficient having a link between the two languages in my flashcards. In order to assist myself and other learners of both Japanese and Korean, I wrote an Anki add-on that adds Japanese translations of Korean expressions. Similar to my previous Anki posts, this blog post serves as a brief tutorial on setting up the add-on and adding Japanese translations to our cards. We'll install an Anki add-on, alter our card-layout to display the Japanese expressions in a fashionable manner, and finally bulk-edit our existing cards to actually add them. Anki Korean-Japanese add-on The add-on scrapes Weblio's online Korean-Japanese dictionary for all translations aside from the usually present character decomposition. The expression 개 (transcribed as gae 3 or kae 4 and means either dog or is used as a particular counter), for example, results in the following page : (If I feel like it, or if there's some demand for it, I'll expand the add-on to scrape the example sentences as seen above as well) Download The Japanese Definitions for Korean Vocabulary add-on has a page on Ankiweb's plug-in page , but as usual we'll install the plug-in using the Desktop Anki application. Go to Tools → Add-ons → Browse & Install , and copy-paste 553926167 in the pop-up dialog. Restart the application to complete the installation. Set-up The add-on expects two things: The field containing your Korean expression should be called: Korean The note-type should contain a field called Japanese which will contain the Japanese translations (Both names can be edited in the JapaneseDefinitionsKoreanVoc.py file by opening it with a text-editor and changing the names listed in the beginning. The file can be found on the Anki's add-on folder on your computer (e.g. C:\\Users\\your_username\\Documents\\Anki\\addons )) To create a new field Japanese , open the desktop Anki application and press Ctrl + Shift + N (or click Tools → Manage Note Types ) to enter the note-type management screen. Select the note-type of the cards you'd like to contain example sentences and click fields . On the next screen, click add and call the field Japanese . Now close this screen. On the previous note-type management screen, click cards . We'll edit our lay-out and display the new field in our cards. Anki Note Lay-out (this is a suggested Anki layout as seen in the screenshot below. Feel free to skip if you'd like to do this yourself.) Lay-out Add the following CSS code to the shared style screen of your note's card-type template. 1 2 . title { color : #999999 ; font-size : 80 % ;} . notes { color : #449933 ; font-size : 80 % } Templates Add the following code to the bottom of all your different card templates' back template. 1 2 3 4 5 6 {{#Japanese}} < br >< div style = 'text-align: left; ' > < span class = \"title\" > Additional Information: < br ></ span > < span class = notes > {{Japanese}} </ span > </ div > {{/Japanese}} Bulk-edit Open Anki's browser (by pressing B or clicking Browse from the main window). The left of this browser has an overview of all your different sets and tags. Select the deck or tag containing the cards you'd like to edit. Press Ctrl + A to select all of those and click Edit → Regenerate JK Expression . It will crawl the internet for each new definition and scrape the HTML so if you have thousands of cards this might take an hour or two. Result Screenshot of JapaneseDefinitionsKoreanVoc.py code from Sublime Text's editor. ↩ Cho, Young-mee Yu, red . 2010. Integrated Korean: Beginning. 2 nd ed. KLEAR Textbooks in Korean Language. Honolulu: University of Hawaii Press ↩ According to the official Korean Revised Romanization ↩ According to the widely used McCune-Reischauer transliteration system, which tries not to transliterate but rather represent the phonetic pronunciation. ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2016/10/anki-add-on-japanese-translations-for-korean-vocabulary/","loc":"https://steviepoppe.net/blog/2016/10/anki-add-on-japanese-translations-for-korean-vocabulary/"},{"title":"A Quick Guide on Using Anki 4: Making the switch - J-J definitions in your vocab cards","text":"You've been studying Japanese for a while now and reached a level of Japanese adequate enough to understand (simple) texts. Drilling new vocabulary has become a daily routine so entangled in your lifestyle you barely notice anymore. If you were to describe your current level of proficiency, you'd proudly present yourself as an intermediate level learner. Sounds familiar? Let's take it a notch further by adding Japanese definitions 2 to our Anki-sets and reduce our dependency on English definition crutches! If you've followed my previous tutorial on adding Japanese example sentences to your vocabulary cards, this tutorial will feel very similar: we'll install an Anki add-on, alter our card-layout to display the definitions in a fashionable manner, and finally bulk-edit our existing cards to actually add them. (This might be my first tutorial worthy of the header quick ). Sanseido add-on There are various online monolingual dictionaries for Japanese vocabulary. I personally like Weblio and Goo , but for our Anki cards we'll use Sanseido - their definitions are usually more concise as the other dictionaries and tend to employ more comprehensible vocabulary. Luck has it someone has already written an anki add-on to serve our blog's purpose. The result of our work will look as follows: Download The Sanseido Definitions add-on has a page on Ankiweb's plug-in page , but as usual we'll install the plug-in using the Desktop Anki application. Go to Tools → Add-ons → Browse & Install , and copy-paste 1967553085 in the pop-up dialog. Restart the application to complete the installation. Set-up The note-types of the vocabulary you want Japanese definitions for will need a new field to contain the definition. On the desktop Anki application, press CTRL-SHIFT-N , or click Tools → Manage Note Types , to enter the note-type management screen. Select the note-type of the cards you'd like to contain example sentences and click fields . On the next screen, click add and call the field Sanseido . Now close this screen. On the previous note-type management screen, click cards . We'll edit our lay-out and display the new field in our cards. Anki Note Lay-out Lay-out Add the CSS code below to the shared style screen of your note's card-type template. 1 . title { font-size : 16 px ; color : #999999 ;} Templates Add the following code below the line displaying your vocabulary's translation/English definition (eg. {{Meaning}} ) . You'll do this in Expression → Back Template , Meaning → Front Template and Audio → Back Template . 1 2 3 4 5 6 7 < br /> {{#Sanseido}} < br />< div id = \"japanese_meaning\" > < span class = \"title\" > Japanese: </ span >< br /> < span id = \"japanese\" class = \"sanseido\" > {{furigana:Sanseido}} < br />< br > </ span ></ div > {{/Sanseido}} Bulk-edit Open Anki's browser (by pressing b or clicking Browse from the main window). The left of this browser has an overview of all your different sets and tags. Select the deck or tag containing the cards you'd like to edit. Press ctrl-a to select all of those and click Edit → Regenerate Sanseido Expression . It will crawl the internet for each new definition so this might take quite a while. 3 Note : the plug-in expects your vocabulary to be contained in a field called Word . If your field is called Expression , you'll have to edit this in the plug-in's python file on your Anki's add-on folder (e.g. C:\\Users\\your_username\\Documents\\Anki\\addons\\sanseidoDefsForAnki.py ). Open it with your text-editor of choice (notepad is fine) and change expressionField = 'Word' to expressionField = 'Expression' . EDIT : as of July 2017, Sanseido's domain changed from .net to .biz. As mentioned in the comments section, you'll have to manually edit sanseidoDefsForAnki.py , search for sanseido.net and replace it with sanseido.biz for it to work. Alternatively, replace the file with the one on my github repository linked below. Usage with Rikaisama / Real Time Import As with my previous tutorial, a drawback to this plug-in is that it does not support vocabulary formatted to use furigana , and neither can Japanese definitions be added automatically on adding a new note through Rikaisama. We'll have to make some small adjustments for that. I've added the edited files on a new repository on my GitHub account so go ahead and replace the existing add-on files on your Anki's add-on folder on your computer (e.g. C:\\Users\\your_username\\Documents\\Anki\\addons ) with the ones on there. I've described all my edits below in case you'd prefer to do this manually. Feel free to skip this part if you're not technically inclined. Set-up sanseidoDefsForAnki.py We've set up our notes to show furigana on our vocabulary. This requires square brackets (eg. 気象庁[きしょうちょう] ). The Anki plug-in for Sanseido definitions however does not support this out of the box. For that reason, I've added a single line of code that uses a regular expression to only use the contents up to the first square bracket as expression. 1 term = re . search ( r '&#94;[&#94;\\[]+' , term ) . group ( 0 ) If you'd like to edit this yourself, add the next line of code as first line in the fetchDef class - it should be around line 27 ( def fetchDef(term): ). Real-Time_Import_for_use_with_the_Rikaisama_Firefox_Extension.py This extension calls on Anki's API to create new notes. I haven't found a way to hook japanese_examples to outside note-creation, so instead I've edited the real_time_import plug-in itself to call japanese_examples at run-time. First, we'll have to import that extension to be able to call it's methods. If you're doing this set-up manually, add the next line near the top, with the other includes (around line 30). 1 from sanseidoDefsForAnki import * Next, I've written a few lines in the createNote method to call sanseidoDefsForAnki's glossNote method at run-time and, if examples were found and our note-type has the correct destination field, add these to our newly created card. This should come before dupOrEmpty = note.dupeOrEmpty() around line 100. 1 2 3 4 5 6 7 # Create sanseido definitions try : glossNote ( note ) except : showTooltip ( \"Error, could not create sanseido definition.\" ); try : note . flush () except : showTooltip ( \"Error, could not create sanseido definition.\" ); Contextualizing imported cards If you've followed above steps, every time you add a new word online by pressing r (the Real-Time Import key) while hovering over something Japanese, it'll automatically contain a Sanseido dictionary definition as well. Try it out on this article's \" Words of the Day \"! 1 {{逆転(ぎゃくてん)}} | {{裁判(さいばん)}} Download Example Set As usual, I've exported my own Anki copy of this tutorial and uploaded it in case you'd like to compare or save yourself the work of editing the note template yourself. It includes work from the previous two tutorials. Download : Example set Sanseido Wait! There is more! The main goal of this post was to introduce another less known, yet highly useful Anki functionality for increasing your study efficiency, as well as hopefully create an attitude of self-reliability by pointing out various tools and possibilities. If there are any further questions, feel free to check out the other articles in this series on Anki or to leave a comment below. A Quick Guide on Using Anki (effectively) (in an academic context) Setting up a perfect vocab-mining environment with Anki and Rikaisama Using Anki's API to contextualize your vocab cards with example sentences Image taken from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under Fair Use doctrine. ↩ Google Anki JJ and you'll come across dozens of posts describing the \" leap from the J-E to J-J dictionary \". It appears to be one of the more popular studying methods amongst self learners online, and is highly blogged about on such blogs as Japaneselevelup.com or on the koohii.com forums. ↩ It takes a couple of seconds for one card. At time of writing, I had 20761 cards based on the Rikai note type; parsing these took about an hour and a half. ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2016/10/a-quick-guide-on-using-anki-4-making-the-switch-j-j-definitions-in-your-vocab-cards/","loc":"https://steviepoppe.net/blog/2016/10/a-quick-guide-on-using-anki-4-making-the-switch-j-j-definitions-in-your-vocab-cards/"},{"title":"Travels: a week in Transylvania","text":"With the academic school year rapidly approaching, we decided on a short last-minute trip abroad and booked tickets to Romania. We spent spent a week,traveling through the historic region of Transylvania, commonly associated with vampire folklore, and situated between the imposing Carpathian mountains. A rich history, ranging from a feudal agricultural culture to a post-war communist regime and following shift to a capitalist mode of production, leaves a very layered culture represented in all aspects from architecture to national mentality. This short blog covers our itinerary, with plenty of pictures to go along with it. Brief history Historically, Transylvania, a traditionally more agricultural region, has been conquered by plenty of nations, including the Roman Empire, Bulgaria, Hungary, the Ottoman Empire and the Austro-Hungarian Empire until it's disintegration following the first world war. Transylvania, as the rest of Romania, played an important role in European politics throughout the Interwar period, but suffered greatly through the second world war and the subsequent communist regime (1947 - 1989). The 1989 revolutions brought an end to Romanian Communist leader Ceausescu's repressive regime 2 and led to the implementation of the free market as well as a 2007 accession to the European Union. The majority of the people are ethnically Romanian, but there's a sizable Hungarian and Roma population as well. Especially the Romani people, an ethnic minority originating from northwestern India and historically serving as slaves in Romania, undergo plenty of discrimination both in Romania and throughout Europe, thus leaving them segregated and socially vulnerable. Itinerary As \"poor college students\" without driver licenses, we relied heavily on public transport. Luckily, Romania has an excellent public transport system both within cities as throughout the country. Thus commuting through the city felt as easy as at home. Unfortunately, due to time issues, we had to cut several important destinations (such as Sighisoara) out of our trip. Bucharest We arrived in Bucharest's Otopeni airport on the 18 th of September and after a brief stroll through town headed to Gara de Nord train station for a train to Sinaia. As a West-European citizen, the contrasts were a bit surprising. Balkan and Latin architectural influences contradict strongly with the tight, functional soviet designs, 3 and even the more touristic access able historical center underwent heavy decay and is now mostly covered in graffiti. Sinaia Sinaia is a quaint little town about an hour and half away from Bucharest and close to the medieval town of Brasov. As heavy focus is put on nature preservation and the weather was excellent, we spent most of our day hiking and enjoying our surroundings. We stayed the night in a cute inn near the center of the town, from where we had an excellent view. Sinaia's monastery, built in 1695, contains two courtyards churches built in Byzantine style and is lovely indeed. Nearby the monastery is an impressive castle reminiscent of Disney castles. Peles Castle, built between 1875 - 1914, was designed in a neo-renaissance style combining different West-European architectural styles. We suffered heavy rainfall the moment we reached the monastery, which made photography a bit more difficult (although it did add to the general atmosphere). Unfortunately the way back down took about half an hour of walking through the equivalent of a small river and left our socks in an unspeakable state. Brasov Still soaked from our earlier trip, we left for the nearby medieval mountain resort city Brasov located between the Carpathian Mountains. We booked a small apartment near the historical center for about 5€ per person/per night. The price reflects the quality pretty well. This was our front door: We spent the first day in this town exploring. One of the most impressive sights in this town is the so called Black Church, a large Gothic church containing one of the largest organs in Eastern Europe. Lucky for us, weekly organ concerts are held during the summer months - something I hadn't experienced yet. Bran Castle On our second day in Brasov, we took a short day-trip to the nearby village of Bran visiting the so-called Dracula's Castle, an extremely imposing fortress with, despite any tangible relations with the historical figure of Vlad Tepes, a rich history nonetheless. Sibiu Sibiu, voted European Capital of Culture in 2007, represents Transylvanian Saxon culture and has one of the few German-only schools in Romania. We passed an orthodox church in the middle of a service and were drawn to the sound of a small orthodox choir chanting what appeared to be Russian Liturgical music. I'm not religious, but I do enjoy the ambiance of Gregorian or Russian Znamenny 4 chants. Thanks to the church's acoustics the music really came to life and made it quite a cool experience, even if I felt like an incredibly out-of-place blasphemer at the moment. Turned out September is opera month in Sibiu and this particular evening featured an opera-recital accompanied by piano and clarinet. Being a student meant we never had to pay over 2€ for any activity. As usual, large portions of the town were covered in juvenile graffiti. One of the more interesting and politically engaged examples however, designed as modern art project, resonated with me a bit: \"My GREAT_ friends a referendum will not separate us\" The historical and cultural center has plenty of noteworthy restaurants hidden in basements. We had some terrific traditional 5 food at this place. It must be noted that like most things, food is incredibly cheap for western tourists, ranging around 7.5 - 10€ for a full course dinner including drinks. The ASTRA National Museum Complex is a large (one of the largest in Eastern Europe) open-air museum containing over 300 buildings in traditional Romanian folk style spread across several lakes. Unfortunately visiting at an off-season moment with bad weather meant relatively little to do, but it was worth the trip regardless. Turda Salt Mine The Turda Salt Mine is a hidden gem for tourists visiting Romania. The city of Turda has successfully transformed one of it's old (first mentions are dated May 1, 1271) salt mines into the equivalent of a modern attraction park, going as far as including a ferris wheel at about 50 meters down underground. The deepest point, around 112 meters deep, contains an underground lake navigable by boat (one of the more memorable things I've done during travels so far) as well as some interesting architectural constructions. Cluj Cluj-Napoca, the second most populous city in Romania and the official capital of Transylvania, was the final point of our trip. After the Turda Salt Mine trip, we spent some time strolling around until we took a night train back to Bucharest for our early flight back to Belgium. Music When traveling, I make it a habit of finding new music by listening to local radio stations or asking recommendations to people I meet in hostels or bars - It's my own personal souvenir and sticks better in my mind than regular touristic souvenirs. This trip's band is the Bucharest located Subcarpati , a lovely blend of Romanian folk music and modern hip-hop beats. Conclusion Despite the brief stay and relatively lousy weather we had a great time traveling around. The historic communist mode of production and the post-soviet economic problems have left their mark both geographically as socially, making for some highly interesting and contrasting views; yet some of the more rural villages feel as if they came straight out of our image of medieval fairy-tales. We've never felt unsafe, despite an initial surprise in regards to the impact of ‘80's era western media and its heavy influence in projecting eastern Europe and Soviet countries in a particular bad light. In regards to the people we talked with, there's a strong mentality of kindness through actions over words as we've often noticed during moments of distress. Also, the food was terrific . Gallery Bran Castle by Stevie Poppe ( https://flic.kr/p/LAMBhm - CC BY-SA 2.0) ↩ Ceausescu started of as a relatively progressive Communist leader, going so far to mix North Korean Juche and Maoist concepts while dismissing aspects of soviet ideology, but steadily became more extreme, leading to an oppressive regime and heavy conflicts with Russia's soviet reformist Mikhail Gorbachev. Interestingly, after their arrest, Ceausescu and his wife were the last people to be executed in Romania before the abolition of capital punishment. ↩ https://en.wikipedia.org/wiki/Constructivist_architecture ↩ https://en.wikipedia.org/wiki/Znamenny_chant#Current_usage_and_related_traditions ↩ Traditional Transylvanian food is generally a mixture of Hungarian and Romanian cooking. Read more: https://www.lonelyplanet.com/romania/transylvania/travel-tips-and-articles/a-guide-to-romanian-cuisine ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2016/09/travels-a-week-in-transylvania/","loc":"https://steviepoppe.net/blog/2016/09/travels-a-week-in-transylvania/"},{"title":"A Quick Guide on Using Anki 3: contextualize your vocab cards with example sentences","text":"So you've followed my previous tutorial and set up this wonderful environment for creating vocabulary cards with audio and furigana based on all kinds of Japanese text you find online. Yet, while studying your new set, you feel this nagging feeling deep down inside… There's something missing, still… \" Hm, could it be…? \", you ponder quietly. Yes . You need Japanese example sentences for providing context to your vocab cards ! One of the common pitfalls of learning Japanese vocabulary through Anki cards is memorizing them loose of context. Perhaps you've found yourself in a situation having no particular problems recalling vocabulary whilst reviewing your cards, but less so when confronted with real-life situations. If that sounds familiar, one way to counter this common problem would be to install an additional Anki add-on, and bring our vocab cards to a next level by adding Japanese example sentences and their translations. This blog offers an easy step-by-step tutorial on setting this up. Additionally, for those who've followed my previous tutorial, I add some extra code to combine the example sentence functions with the instant-import features of the Firefox plug-in Rikaisama. Plug-in We'll use an existing Anki add-on called Japanese Example Sentences to add Japanese sentences taken from Tatoeba.org , a \"collaborative, open, free and even addictive\" community on producing example sentences. The result of our work will look as follows: Download The Japanese Example Sentences add-on has a page on Ankiweb's plug-in page , but as usual we'll install the plug-in using the Desktop Anki application. Go to Tools → Add-ons → Browse & Install , and copy-paste 2413435972 in the pop-up dialog. Restart the application to complete the installation. Edit : as user nwt mentioned in the comments, this example-set is fairly outdated. Follow the instructions left in the plug-in's comment section to update to the most recent set. If you want a test Anki deck to work with, I've uploaded an example set, the result of my previous Anki tutorial, containing the necessary notetype and two test-notes for download: Example set Rikai . Set-up The note-types of the vocabulary you want sentences for will need a new field to contain these. On the desktop Anki application, press CTRL-SHIFT-N , or click Tools → Manage Note Types , to enter the note-type management screen. Select the note-type of the cards you'd like to contain example sentences and click fields . On the next screen, click add and call the field Examples . Now close this screen. On the previous note-type management screen, click cards . We'll edit our lay-out and add the sentences in our actual cards. Anki Note Lay-out Add the code to the bottom of the middle lay-out screen. It'll display the example sentences to the left of your cards. 1 2 . examples { font-size : 75 % ; text-align : left ;} . title { font-size : 16 px ; color : #999999 ;} Back Template Add the following HTML code at the bottom of your different cards' back templates. As pointed out in the comments, lang=\"ja\" serves to render characters correctly, rather than in simplified Chinese due Han unification. 1 2 3 4 5 6 7 {{#Examples}} < br /> < div class = \"examples\" lang = \"ja\" > < span class = \"title\" > Sentences: </ span >< br /> {{furigana:Examples}} </ div > {{/Examples}} When done, close both this and the previous screen. Let's try this plug-in out now! Usage Contextualizing existing cards We'll start by adding example sentences to our existing cards (of the note type we've just edited). Press B or click on \"Browse\" to open Anki's Card Browser. The left of this browser has an overview of all your different sets and tags. Select the deck or tag containing the cards you'd like to edit. Press Ctrl + A to select all of those and click Edit → Bulk-add examples . Most, if not all, should contain sexy new example sentences by now. Contextualizing new cards While creating a new card of a note-type that contains the Examples field, the add-on will automatically fill in that field as soon as you've entered a Japanese expression in the Expression field. No further set-up is required for this. Usage with Rikaisama / Real Time Import A drawback to this plug-in is that it does not support vocab expressions using furigana , and neither can example sentences be added automatically on adding a new note through Rikaisama. I've you've read my previous blogpost , you'll probably want to to follow these next steps as well. I've made some adjustments to both the Anki Real-Time Import add-on and the Japanese_Examples add-on. I've added the edited files on a new repository on my github so go ahead and replace the existing add-on files on your Anki's add-on folder on your computer (e.g. C:\\Users\\your_username\\Documents\\Anki\\addons ) with the ones on there. I've described all my edits below in case you'd prefer to do this manually. Feel free to skip this part if you're not technically inclined. Set-up japanese_examples.py We've set up our notes to show furigana on our vocabulary. This requires square brackets (e.g. 気象庁[きしょうちょう] ). The anki plug-in for Japanese example sentences however does not support this out of the box. For that reason, I've added a regular expression to only use the contents up to the first square bracket as expression. 1 2 3 searched = re . search ( r '&#94;[&#94;\\[]+' , expression ) if searched : expression = searched . group ( 0 ) If you'd like to edit this yourself, add that piece of code to the find_examples class right after it defined examples as a list. it should be around line 139 ( def find_examples(expression, maxitems): examples = [] ). Real-Time_Import_for_use_with_the_Rikaisama_Firefox_Extension.py This extension calls on Anki's API to create new notes. I haven't found a way to hook Japanese_examples to external note-creation, so instead I've edited the real_time_import plug-in itself to call japanese_examples at run-time. First, we'll have to import the japanese_examples extension to be able to call its methods. If you're doing this set-up manually, add the line below near the top, along with the other includes (around line 30). 1 from japanese_examples import * Next, I've written a few lines in the createNote method to call Japanese-examples' find method at run-time and, if examples were found and our note-type has the correct destination field, add these to our newly created card. This should come before dupOrEmpty = note.dupeOrEmpty() around line 100. 1 2 3 4 5 6 # for use with japanese examples examples = find_examples_multiple ( note , MAX_PERMANENT ) # if field is empty and examples exist if examples and not n [ DEST_FIELD ]: note [ DEST_FIELD ] = examples Finally, I've edited the mark-up of the example sentences themselves to hide the English translations unless hovered above (or on press on smart phones). Locate the examples.append call in the find_examples method. You'll want to replace it (I commented it out) with the line below. It'll be around line 174. 1 examples . append ( \"<div id='eng_test'> %s <span id='eng_sentence'> %s </span></div>\" % tuple ( example . split ( ' \\t ' ))) In the find_examples_multiple method (around line 214), replace the current return with the following line. We want only one break between example sentences. 1 return \"<br>\" . join ( examples ) Important! For this to work, you'll need to make an edit to your note template's layout. As usual enter the note-type management screen by pressing Ctrl + Shift + N , or click Tools → Manage Note Types . Select the notetype of the cards you'd like to contain example sentences and click fields . From there, select the Rikai note, click cards and add the following CSS code to the bottom of the shared lay-out screen in the middle. 1 2 # eng_sentence { display : none ; } # eng_test : hover # eng_sentence { display : inherit ; color : #eb4c42 ;} New Usage Contextualizing existing cards If you've set up Rikaisama to include ‘rikai' as tag on new cards, this should be a breeze. Open Anki's browser (by clicking Browse or pressing B ), select the rikai tag in the left column, and press Ctrl + A to select all your cards. Next, click Edit → Bulk-add examples . Contextualizing imported cards If you've followed above steps, every time you add a new word online by pressing R (the Real-Time Import key) while hovering over something Japanese, it'll automatically contain example sentences as well. Try it out on our new \" Words of the Day \"™! 1 {{文脈(ぶんみゃく)}} | {{語彙(ごい)}} Download Example Set As usual, I've exported my own copy of this tutorial and uploaded it in case you'd like to compare or save yourself the work of editing the note template yourself. Download : Example set Sentences Wait! There is more! The main goal of this post was to introduce another less known, but highly useful, Anki functionality on increasing your study efficiency, as well as hopefully create an attitude of self-reliability by reaching out various tools and possibilities. If you've any further questions, feel free to check out the other articles in this series on Anki, or to leave a comment below. A Quick Guide on Using Anki (effectively) (in an academic context) Setting up a perfect vocab-mining environment with Anki and Rikaisama Making the switch: J-J definitions in your vocab cards Further reading The 10 000 Sentences Method : one of the more popular online self-study methods seem to be the complete Japanese immersion ( All Japanese All The Time ) method and its 10 000 sentences style of learning: increasing the feel of a language by assimilating and internalizing 10 000 different sentences. To be honest, the arbitrary number of sentences and clickbaitfish descriptions make it pretty gimmicky (\" Learn Japanese in just 18 months !\" 2 ), but its popularity should give some validity on the importance of example sentences in your vocab-learning. Image taken from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under Fair Use doctrine. ↩ I've read enough blogs on learning Japanese or language acquisition to come up with my own gimmicky clickbait articles as \" Use these methods to watch your Japanese learning abilities soar \" or \" Break the Plateau: 10 easy tips to Master Japanese \" and cash in on ads or selling e-books, but honestly I'm just some guy on the internet learning Japanese and occasionally blogging about cool, potentially useful features. Maybe if I actually sticked on studying instead of needlessly browsing the internet I could've actually made such claims, but no… It's stronger as myself… ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2016/09/a-quick-guide-on-using-anki-3-contextualize-your-vocab-cards-with-example-sentences/","loc":"https://steviepoppe.net/blog/2016/09/a-quick-guide-on-using-anki-3-contextualize-your-vocab-cards-with-example-sentences/"},{"title":"Waarom Japanologie?","text":"Zoals velen in mijn richting ontstond mijn interesse in Japan al op jonge leeftijd onder invloed van het zogenaamde Cool Japan : de globale verspreiding van Japanse popcultuur (muziek, televisie, games, …) die sinds de jaren ‘80 geleidelijk aan begon door te sijpelen in ook onze nationale media. Het zou, zo blijkt, zelfs een interesse worden die zodanig groeide dat ik er jaren later mijn studiekeuze door liet beïnvloeden. Vaak krijg ik bij vermelding daarvan echter nog, zoals mijn klasgenoten ongetwijfeld kunnen beamen, een hoop verbaasde blikken toegesmeten en dus ook dé klassieke hamvraag voorgeschoteld: Waarom nu juist Japanologie ? Als tweedejaars werd ik eerder dit jaar gevraagd om op een opendeurdag van onze richting een kort woordje over mijn persoonlijke ervaringen met Japanologie aan de KU Leuven, waar ik aldos ook bovenstaande vraag probeerde te beantwoorden. Toch werd ik daarna door verschillende bezorgde ouders aangesproken met enkele niet geheel onterechte vragen als \" Ja maar, wat zijt ge daar nu mee? \", \" Leert ge dan enkel over Japan? Oeioei, is da ni héél beperkt? \" en \" Is Japan nog wel relevant? \". Om beter de oorspronkelijke vraag \"Waarom Japanologie?\" te beantwoorden lijkt het mij het beste om te vertrekken vanuit juist die drie bovenstaande vragen: Vragen \"Japanologie? Dan leert ge enkel over Japan? Oeioei!\" Als je naar de onderwijsaanbod-pagina van Japanologie aan de KU Leuven gaat krijg je het volgende te zien: Bachelor in de taal- en regiostudies: japanologie (Leuven) Bachelor of Arts In de bacheloropleiding Japanologie bestudeer je in de eerste plaats het moderne Japans op een intensieve manier. De opleiding plaatst het Japans steeds centraal tegenover de achtergrond van de Japanse cultuur, geschiedenis, filosofie, economie en politiek. Je kiest een minor, wat je de mogelijkheid geeft om je eigen interesses beter in te werken in je studie. Het spreekt voor zich dat de Japanse taal en theoretische vakken gericht op Japan centraal staan in onze richting, maar bovenstaande tekst zegt toch wat meer: Japanologie aan de KU Leuven is een onderdeel van taal- en regiostudies, en levert een Bachelor of Arts op: een diploma binnen de geestes- of sociale wetenschappen. Concreter betekent dat dat een student Japanologie een hoop vaardigheden onder de knie hoort te krijgen die op eerste zicht geen rechtstreeks verband met Japan lijken te houden maar de student vormt om genuanceerder en meer kritisch te kijken naar niet enkel Japan, maar de hele wereld . Dat lijkt me ook logisch. Enkel kennis van een bepaalde taal of wat feitenkennis van één bepaalde regio, losstaand van diens rol tegenover de rest van de wereld, lijkt me zonder vaardigheden om te kunnen vergelijken niet bijzonder veel waard. Ondertussen worden computers steeds efficiënter in het instant -vertalen van zowel tekst als audio, en zijn er enorme digitale databanken met historische feiten die gratis geraadpleegd kunnen worden. Wat computers en software echter (nog) niet kunnen is het betekenis geven aan die data. De vaardigheden die we tijdens onze studies leren zorgen er voor dat wij uiteindelijk die leegte kunnen invullen en aldus ook als een soort ambassadeur een culturele brug tussen de verschillende natiestaten kunnen vormen. Enkele voorbeeldjes : om de Japanse economie en politiek te kunnen begrijpen is het niet alleen belangrijk om op theoretisch vlak iets te weten over die velden, maar ook om de politieke en economische situatie uit de eigen leefwereld te begrijpen. Om ontwikkelingen in de Japanse hedendaagse pop-cultuur te begrijpen is het dan zeker weer niet mis om kennis te hebben over de historische ontwikkeling van (Japanse) literatuur, of om zich te verdiepen in antropologie, in ontwikkelingen in het terrein van geschiedschrijving, enzovoort. We kijken eens naar een verkort lijstje van zowel verplichte- als keuzevakken binnen de optie culturele minor 2 . Vakken die rechtstreeks verband hebben met Japan of het Aziatisch continent (vakken als \"Inleiding tot de Oost-Aziatische kunst\", vakken uit Sinologie, de Koreaanse taal, enzovoort) heb ik even weggelaten. Verplichte vakken Inleiding tot de wijsbegeerte, Religie, zingeving en levensbeschouwing, Informatiekunde, Sociologie, Recht, Inleiding tot de sociale en culturele antropologie Optie Geschiedenis en Literatuur Inleiding tot de studie van de Europese literatuur en cultuur: na 1800, Geschiedenis van de nieuwe tijd, Geschiedenis van de middeleeuwen, Geschiedenis van de nieuwste tijd, Nederlandse letterkunde I: moderne literatuur, Algemene literatuurwetenschap I Andere keuzevakken Mediacultuur, Inleiding tot de beeldanalyse, Taalverwerving, Algemene taalwetenschap I, Inleiding tot de economie, Geschiedenis van Rusland, Inleiding tot de Slavische wereld, Consumer Behaviour, Academisch Nederlands, Human Rights, Economische ontwikkelingen van Japan, Griekse mythologie en godsdienst, Geschiedenis van de internationale betrekkingen, Hedendaagse politieke en sociale theorieën: marxistische en socialistische stromingen, Sanskriet, East-West Perspectives in Philosophy, Genderstudies Een van onze docenten haalde in de eerste les van het vak \"Ruimtelijke Organisatie van Japan\" een metafoor aan die me sterk bijbleef: waar studenten aan richtingen binnen de exacte wetenschappen zich omhoog werken door een ladder te beklimmen binnen het eigen domein, worden studenten binnen studiedomeinen zoals Japanologie verwacht omhoog te klimmen door middel van het bouwen van een piramide. Elk nieuw veld waarover geleerd wordt representeert een nieuw bouwblok aan die pyramide. Door het bestuderen en beheersen van een hoop verschillende velden (zij het nu politiek, geografie, literatuur of filosofie) creêert men paradoxaal juist een verdere specialisatie in de eigen richting. Deze stelling sluit naadloos aan bij de volgende vraag: \"Wat zijt ge daar nu mee?\" Het cliché gaat dat richtingen binnen Letteren, waartoe Japanologie behoort, het niet goed doen op de arbeidsmarkt. Met een verzwakte economie is er minder vraag naar dergelijke diplomas, en cijfers worden dan naar hartelust rond onze oren gesmeten: zo zouden bijvoorbeeld volgens dit vacature.com artikel uit 2010 , gebaseerd op cijfers van de VDAB, 25% van de afgestudeerden met een academische bachelor taal-en letterkunde nog steeds werkloos zijn na 1 jaar. Ook een recenter jobat.be artikel uit 2013 plaatst Masters in Oosterse talen en culturen als een slechte keuze. De werkelijkheid is uiteraard meer complex dan aangegeven. Uit diezelfde studie van de VDAB stond in 2011 één schoolverlater met een master in \"Taal en regiostudies: Oude Nabije Oosten\" geregistreerd. Diezelfde schoolverlater stond tevens na 1 jaar, in 2012, bij de VDAB geregistreerd als werkzoekende, waardoor het percentage werkzoekenden na 1 jaar binnen \"T&R: Oude Nabije Oosten\" op dat moment als 100% werd geclassificeerd. in 2014 waren er dan weer 3 schoolverlaters in die richting, die in 2015 niet stonden geregistreerd als werkzoekenden: een percentage van 0% werkloosheid bij afgestudeerden \"T&R: Oude Nabije Oosten\". 3 De situatie is vergelijkbaar voor Japanologie (zie foto beneden), dus staar je zeker niet blind op dergelijke cijfers. Uiteraard houdt de richting Japanologie aan de KU Leuven daar wel rekening mee. Een van de twee minors, de economische minor, richt zich sterk op het verwerven van economische vaardigheden rechtstreeks inzetbaar op de arbeidsmarkt. 4 Binnen Letteren heeft Japanologie een uitstekende reputatie op gebied van het verwerven van digitale geletterdheid, en sinds het schooljaar van 2016-2017 wordt ook de master Japanologie een tweejarige master, met meer nadruk op stages en het opbouwen van praktische vaardigheden. Daarnaast is het mogelijk om tijdens een van de masterjaren al een attest als leerkracht Japans te bemachtigen. Als laatste wordt het behalen van een extra diploma - een extra specialisatieveld - ook sterk aangemoedigd. Over ervaringen in de bedrijfswereld kan ik zelf nog niet meespreken. Wel lijkt mij, zoals in elke richting, het profileren tijdens de studiejaren en het kweken van extracurriculaire competenties iets dat zeker niet verwaarloosd mag worden. Waar ik eigenlijk eerder op wil focussen binnen de context van deze vraag is de waarde van onze richting (en andere richtingen binnen Letteren of zelfs breder, binnen arts ) niet op professioneel gebied, maar op persoonlijk gebied. Door het bestuderen van andere landen in al hun facetten, maar ook dankzij vakken als filosofie, antropologie, sociologie, enzovoort, leer je niet alleen meer over de wereld, maar ook over jezelf. Hoewel we gezien de neo-liberale waarden die gelden in onze maatschappij geneigd zijn te kijken naar de rechtstreekse toepassing van studies op de arbeidsmarkt, mogen we niet vergeten dat universiteiten meer nog een rol hebben in het ontwikkelingen van kritische, analytische denkers. \"Is Japan nog wel relevant?\" Enkele recente artikels over Japan die de Belgische mainstream media haalden 5 : Japanse keizer Akihito zinspeelt op aftreden President Obama bezoekt Hiroshima uitbreiding bevoegdheden Japans leger premier Abe verkleedt zich als Super Mario op slotceremonie in Rio Het klopt dat Japan niet meer wordt aanzien als het economisch powerhouse dat het ooit was, en sinds het barsten van een economische zeepbel vast lijkt te zitten in een langdurende recessie. Niettegenstaande blijft Japan de derde grootste economie van de wereld, 6 en wordt Tokio, juist zoals Londen in Europa en New York in de Verenigde Staten, gezien als het ingangspunt in het Aziatisch continent. Weeral blijkt de werkelijkheid genuanceerder dan de overheersende stereotypes, en een sterkere focus wordt daarop gegeven in vakken zoals Japanse Politiek en Japanse Economie. Studenten Japanologie vandaag kiezen uiteraard ook niet langer uit dezelfde beweegredenen van studenten Japanologie 30 jaar geleden, die toen eerder economisch van aard neigden te zijn. De blijvende populariteit van de richting verraadt echter wel een aanhoudend belang en een niet onbelangrijke shift van economische naar culturele supermacht. Conclusie Er kunnen nog zoveel redenen voor- of tegen het volgen van Japanologie zijn, als je al gebeten bent door de Japan-microbe en je overtuigd bent dat dit de richting voor jou is zal het heel moeilijk worden om een andere richting met dezelfde passie en motivatie aan te vangen. Andere richtingen kunnen profijtelijkere carrièreopties opleveren, maar een richting als Japanologie biedt dan weer, hoe cliché het ook klinken mag, een heel andere soort rijkdom: die van persoonlijke verrijking. Het niet volgen van je hart, in functie van betere carrièreopties, kan leiden tot een spijt achteraf die niet meer in te halen valt. Niettemin, wees het nu een job in een Japans bedrijf in België, een functie ergens in Japan zelf, of zelfs iets geheel ongerelateerd aan de oorspronkelijke studies: een pad vind je zeker. Met een diploma Japanologie toon je niet alleen aan een hoop academische vaardigheden en een uitstekend aanpassingsvermogen te bezitten, maar meer nog, je hebt jezelf kunnen ontwikkelen tot een rijper persoon die analytisch en met nuance rondom zich kan kijken.Sterker nog, misschien hebben we in deze tijden van groeiende onverdraagzaamheid rondom ons zelfs juist méér Japanologen en mensen met een Bachelor en/of Master in Arts nodig! Graag wil ik concluderen door te stellen dat mocht je toch kiezen voor Japanologie – ongeacht je oorspronkelijke beweegredenen –, en openstaat voor wat ik hiervoor beschreef, je in het gezelschap van gelijkgestemden enkele van de beste jaren in je leven kan beleven, juist zoals de voorbije twee jaar al voor mij bleken te zijn. Meer lezen? Nippaku: Japanologie aan de KU Leuven, wat houdt dat nu precies in? : gedetailleerde beschrijving van onze richting door een van mijn senpai , zeker eens lezen! Japanse Studies, KU Leuven : de homepage van onze richting, met allerlei informatie voor (potentiëel) nieuwe studenten. Fushimi Inari Shrine by Stevie Poppe ( https://flic.kr/p/M4Kdmk - CC BY-SA 2.0) ↩ De optie die ik zelf volg. Daarnaast is er ook een economische minor met een hoop verplichte en keuzevakken rond economie, maar daar ben ik minder in thuis. Een meer complete lijst van vakken staat op de onderwijsaanbod pagina van KU Leuven . ↩ Cijfers gebaseerd op een grootschalig telling van de VDAB. Bekijk een interactieve grafiek over alle richtingen heen op: https://www.vdab.be/trendsdoc/schoolverlaters/detail/default.shtml ↩ Aan de andere kant levert de culturele minor, met vakken rond Koreaanse geschiedenis, cultuur en taal, ook een eerste aanzet tot een Koreanologie in België. Naast Cool Japan kent ook Korea al een geruime tijd een sterke shift naar soft power met haar export van pop-cultuur, beter bekend als de Korean Wave. ↩ Met de hashtag Japan lijken online kranten zelfs op dagelijkse basis te rapporteren over het land: http://www.standaard.be/tag/japan?page=1 ↩ Volgend op de Verenigde Staten en China, en gevolgd door Duitsland, Groot-Britannië en Frankrijk. Meer: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal) ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2016/09/waarom-japanologie/","loc":"https://steviepoppe.net/blog/2016/09/waarom-japanologie/"},{"title":"A Quick Guide on Using Anki 2: an efficient vocab mining set-up with Anki and Rikaisama","text":"UPDATE 2018~: Rikaisama has been rendered obsolete since 2018 updates broke Firefox' support of unsigned XUL-based add-ons. Nevertheless, this guide can still prove useful in two ways: further use of Rikaisama through XUL-supporting Firefox-derivatives such as Waterfox, Pale Moon and Basilisk, as well as the list of applications for Browser/Anki-integration listed in the second half of this guide. As I personally use Waterfox, I'll refer to Waterfox as main option hereafter. Another option is to use Yomichan for Chrome, described in this updated blog . So you want to: read a Japanese novel without ending up crying yourself to sleep realizing you know nothing , Jon Snow? learn Japanese by watching anime, but like, for real? create beautiful Anki sets filled with handpicked vocabulary, hidden furigana reading-aids, AND crystal-clear audio pronunciations, all without spending days of mind-numbing copy-pasting? Welcome to the lovely world of Integrated Anki & Rikaisama ! The hove-over Japanese dictionary plug-in Rikaichan and its Chrome variant Rikaikun have already been widely established as incredibly helpful tools for reading Japanese online. Nevertheless, one major function remained missing still: a seamless integration with Anki for efficient vocabulary mining. A recent expansion of the original Rikaichan, aptly named Rikaisama (it's a pun !), adds several new features including communication with the Anki API, Sanseido Web Dictionary ‘s J-J dictionary definitions, access to J-Pod101 ‘s audio-files, 2 and more. 3 Concretely, this means audio playback of tens of thousands of words and expressions as well as easy integration of new vocab into Anki. Despite these incredibly useful features, Rikaisama remains fairly about. To counter that, this blog serves as a brief tutorial on setting up a Rikaisama/Anki environment as well as demonstrating some useful real-life use-cases. Rikaisama Download Like Rikaichan, Rikaisama is a Firefox Waterfox plugin, available for download on their sourceforge download page (free, of course). If you're an avid Chrome supporter, this one might be worth switching for. After installation you'll have two new buttons at your disposal, either already visible at the top-right of your menubar, or in Firefox Waterfox's toolbar. The left one activates Rikaisama. I just leave it permanently open. The right one switches the Rikaisama lookupbar. I don't use this one often so I hide it in the toolbox myself, but for now we'll need it to access Rikaisama's settings. Open the toolbar and click the right-most icon to open those (after we're done, click the leftmost icon to deactivate the toolbar). Rikaisama's base settings are pretty self-explanatory, and I've left most of them to their default values. For our purpose, the Clipboard & Save and Anki tabs will be the most relevant. Set-up Rikaisama's homepage contains a crude, bare-bones set-up guide covering the basics of linking Rikaisama with Anki. It does however expect technical knowledge of it's readers, which I feel will averse some. I try to simplify things while still explaining why we're doing things, since it really is pretty easy to set-up. If you'd rather get straight to the point, just ignore the reasoning parts (or skip to 1.2.4. Usage to download a pre-made Anki-set with matching note-type: all you'll have to do then is to change a few Rikaisama settings). Anki First, Anki will need an additional add-on as well in order to allow real-time import. It's ‘available' at Ankiweb's plug-in page , although actual installation is done in-app. Open Anki on your desktop, go to Tools → Add-ons → Browse & Install , and copy-paste 2512410601 in the pop-up dialog. Now, we'll create a new set to contain your Rikaisama-powered cards, a new note-type with fields corresponding to the info you'd like out of rikaisama, and finally a few card-types to test us on on visual recognition, on oral recognition, and on production. 4 Start by creating a new set, e.g. rikai-vocab 5 . On Windows, press CTRL-SHIFT-N , or click Tools → Manage Note Types , to enter the note-type management screen. Click add , then ok on the next screen, to create a new note type. Call it Rikai (do note my excellent graphical skills as seen in the screenshot below ( ͡° ͜ʖ ͡°) )! Next, select the new Rikai note-type and click Fields . We'll rename the current front and backside as Expression and Meaning , and add two new Fields Audio and Notes (this one could be used for personal notes or example sentences). If you're interested in pitch accents, add a fifth field Pitch . Next, close this screen and click on Cards . We'll manage the different cards this note generates as well as it's make-up. We'll start with some basic layout. Add the following lines to the bottom of the middle Shared Style screen. For optimal learning, we'll keep word readings in kana as furigana displayed on touch/hover. This way learners are less likely to use them as a crutch. 1 2 ruby rt { visibility : hidden ; } ruby : hover rt { visibility : visible ; } Next, click the + button twice to create two more card-templates. Next, rename (by clicking More → Rename ) templates Card 1 , Card 2 and Card 3 respectively as Recognition , Production , and Audio . Finally, let's wrap up our Anki set-up by creating the actual card's templates. Each card will question you on one field (the Japanese expression, the English translation and the Audio) and show the answer on the backside along with audio and any potential notes, if present. Your card's templates should look as follows: Recognition Front Template 1 {{ furigana : Expression }} Back Template 1 2 3 4 5 6 7 8 9 10 11 12 13 {{FrontSide}} < hr id = answer > {{Meaning}} < br > {{Audio}} {{#Notes}} < br >< br > < b > Notes: </ b > < br > {{furigana:Notes}} {{/Notes}} Production Front Template 1 {{ Meaning }} Back Template 1 2 3 4 5 6 7 8 9 10 11 12 13 {{FrontSide}} < hr id = answer > {{furigana:Expression}} < br > {{Audio}} {{#Notes}} < br >< br > < b > Notes: </ b > < br > {{furigana:Notes}} {{/Notes}} Audio Front Template 1 {{ Audio }} Back Template 1 2 3 4 5 6 7 8 9 10 11 12 13 {{FrontSide}} < hr id = answer > {{furigana:Expression}} < br > {{Meaning}} {{#Notes}} < br >< br > < b > Notes: </ b > < br > {{furigana:Notes}} {{/Notes}} If you added a fifth field Pitch , add <br><br>{{Pitch}} to the back templates of those cards. Mapping Now we'll enter our Rikaisama settings on Firefox and ensure our newly created note-type's fields match the ones in Rikaisama's anki settings. You'll have to make some adjustments in both the Anki and Clipboard & Save tabs, as documented below: Anki tab Save format : $d[$r]$t$n$t[sound:$a] Field names : Expression Meaning Audio If you added the pitch field, this becomes: Save format : $d[$r]$t$n$t[sound:$a]$t$p Field names : Expression Meaning Audio Pitch Make sure to mark the save audio check-box . The field names field is pretty forward, it contains the field names of the note we just created in Anki. The Save format field however is a bit more complicated. It'll contain tokens or variables (similar to fields in your Anki note) available for each Japanese word hover-able through Rikaisama. Each field name must have both a corresponding token, marked by a dollar sign and a single letter (eg. Expression → $d ) as well as a token for empty spaces in between. To get our furigana , the kana reading must be encapsulated next to our expression (eg. $d[$r] ). The $a token contains just the MP3 filename. To get it to play in our cards, we require a correct format (e.g. [sound:ちょうつがい - 蝶番.mp3] ), hence the [sound:$a] . Clipboard & Save tab Anki keeps all of its assets, like audio or images, in a specific folder. Thus, we'll have to adjust the path these audiofiles are saved in rikaisama's settings as well. On a clean Anki install on windows, this folder will probably be something like C:\\Users\\your_name\\Documents\\Anki\\anki_username\\collection.media . When located, use that folder's path for your Saved Audio path. We'll also have to confirm our save format again in this tab. Just copy/paste the one you used in the other tab. I personally tag all rikai-created ankicards as such, this isn't necessary but it might come of use in your Anki's browser one day. Dictionaries A final step in case you haven't used Rikaichan before, is to install some dictionaries files, available at Rikaichan's homepage . I personally use the Japanese - Dutch , 6 Japanese English and Japanese Names dictionaries, and sort them in those order in Rikaisama's configuration. Usage Now let's test our set-up! For Rikaisama to recognize which Anki set and which notes to use, we'll have to open anki and enter our set every time we're on hunting spree. If you get a mismatch error, just click add and ensure that the selected type is Rikai by clicking on the current type and selecting the proper one (just click close again afterwards). Hover over these two words. A rikai pop with definitions should appear. 理解 ( りかい ) | 暗記 ( あんき ) (I'm not obsessed, I swear~) Now, while still hovering over them, type r (the Real-Time Import key) to import these words into your set. Great! (Note that Rikaisama by default does not allow duplicates, so if you try to add an already added word, you'll get an error message.) I've exported my own copy of this example and uploaded if you want to compare or save yourself the work of creating the note template manually: Download : Example set Rikai If there's one downside to this method, it's that an audio reference will be added to your cards regardless if there's a preexisting audio recording online. This is particularly the case with lesser known words or idioms: while reading Harry Potter I've added a few hundred of such cards. To remove these, just open Anki on your desktop computer and select Tools → Control Media . It will clear all the non-existent audio entries from your cards' Audio fields. Afterwards, click Tools → Empty Cards to remove all the empty oral-recognition cards. Appliances Internet This one goes without saying as that's pretty much the prime objective of this tutorial. If you're a relatively new learner, NHK provides simplified news articles and videos at NHK Easy News . I used these a lot near the end of my first year studying, and it's a neat way to stay up to date with what's going in Japan, or more generally in the world (frankly speaking in this time and age it's best to arm yourself with knowledge anyway). Novels 「言い方がまちがってるわ。ウィン、ガー・デイアム レヴィ・オーサ。『ガー』と長一くきれいに言わなくちゃ」 7 JNovelFormatter JNovelFormatter is a neat little tool by the developer of Rikaisama 8 that converts Japanese literature in .txt format into clean parsed HTML files. Layout is fairly customizable, although I think the original settings are easy on the eye as-is (I like dark backgrounds when reading for hours at a time, makes me feel less like I'm gazing straight into a light-bulb). End of sentence dots get turned into book-markable anchors so you won't lose track of your progress. I highly recommend this method! I've used this approach while reading The Girl Who Leapt Through Time ( 時 ( とき ) をかける 少女 ( しょうじょ ) ) during our 読書 ( どくしょ ) クラブ ‘s book-reading sessions and just last week, as a kind of summer project, worked my way through the first Harry Potter (a surprisingly high difficulty but I know the original by heart so that helped heaps context-wise). As for resources, there's plenty to be found online. Light novels are commonly uploaded in text format and probably a good starting point for the more advanced learner. 9 I've heard Zero no Tsukaima ( ゼロの 使魔 ( つかいま ) ) and Kino no Tabi ( キノの 旅 ( たび ) ) are relatively easy reads, for example. 10 Aozora Aozora , pretty much the Japanese Gutenberg project, freely hosts tons of public-domain books online. 11 Check out Natsume Soseki's classic Botchan as example . Their literary standards are a bit higher as the typical light novel so it's quite an adjustment, but great for those who're looking for easy-to-find, legally obtainable literature. Anime A really cool concept recently released is Animelon's multi-layer subtitled anime streaming web-app, akin to the Erin's Challenge video's (except with arguably way more interesting content: as of date there are over 60 series hosted, mostly all big names ranging from more recent hits as SAO and Attack on Titan to classics as Fullmetal Alchemist: Brotherhood and Clannad). If I were to recommend one to start with, I might pick Clannad. 12 The high-school setting means relatively easy and casual vocabulary (as opposed to, for example, SAO's highly technical and fantasy-related vocab you'll barely ever encounter in real life), while the voice-acting's pacing is calm enough to pick up as relative beginner, and devoid of difficult accents. Check out their video demonstration below to see the various possibilities: What bothers me a bit is the legality of it all. They're currently keeping this site up as proof-of-concept in search of potential buyers but meanwhile they're hosting full series in HD quality \"for educational purposes only\". Gaming This one's a bit tricky still. The new web-possibilities since the implementation of the latest HTML-standard, HTML5, have given rise to a broad range of games running natively in your browser without the need for additional clutter such as Adobe Flash. Visual novels, in particular, would be most suitable for our purpose considering the dense amount of words in each game. Unfortunately so far the online playable ones I've found were either already translated into English, or else of the early '90s abandonware eroge kind, so not what I was looking for (you'll easily find that online if that's your cup of tea). Another option is to set up your gaming environment in such a way it enables you to use these features. There's an active community of technically capable Japanese learners at Reddit and Koohii who came up with such a thing. I've yet to try this myself, and honestly, it's a bit bloated, but if you're dying to play visual novels and learn Japanese at the same time, this'll do the trick: Get Visual Novel Reader : it appears to be a mask to be run over an already installed visual novel, using OCR to capture Japanese text, parse it, and run it through some dictionaries ( Tutorial ). Use the Furigana Inserter Firefox Plug-in to automatically copy-paste dialog from your clipboard, obtained dynamically through OCR by above means, to an empty Firefox screen. If you're running Firefox and your game side-by-side you'll be able to max up your visual novel ankisets in no time, just like this guy! Wait! There is more! The main goal of this post was to introduce a less known, but highly useful, Anki functionality by linking Rikaisama's excellent 13 pop-up dictionary with Anki's API, as well as show several possible appliances. If you've any further questions, feel free to check out the other articles in this series on Anki, or to leave a comment below. A Quick Guide on Using Anki (effectively) (in an academic context) Using Anki's API to contextualize your vocab cards with example sentences Making the switch: J-J definitions in your vocab cards Further reading AwesomeTTS for Anki : My approach uses JapanesePod101's vast audio library. Occasionally however I encounter words or expressions that have no native spoken audio recorded. If you want audio-completion, this open-source tool allows for text-to-speech in your Anki cards. I use this occasionally on full sentences or korean vocabulary, but the quality of the free TTS engines remains fairly primitive, so your mileage might vary. Learn Japanese Through Videogames : speaking of videogames, you could also look up videogame scripts ( セリフ 集 ( しゅう ) ) and vocab-mine from those. This guy wrote a decent blog on that. The recent Phoenix Wright spin-off Dai Gyakuten Saiban (a series I'm particularly fond of) has yet to be confirmed to receive a western release, so I'll have to brush up my legal vocab this way before playing. Image taken from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under Fair Use doctrine. ↩ A popular online language course podcast. They have podcasts ranging from newbie (mostly English) to upper intermediate (100% Japanese). I used to use these in the beginning as the added scripts are quite helpful for raising listening skills. The male, English speaking is notoriously difficult to stand, however, and despite most content being free, they have some iffy marketing techniques. Regardless, they recorded a dictionary's worth of vocabulary in high quality audio for Jim Breen's dictionary, so that's hella cool. Read more: http://blogs.japanesepod101.com/blog/2009/04/20/biggest-announcement-ever-edict-japanese-dictionary-now-with-audio-for-every-clip-must-hear ↩ Other useful features include pitch order, updated frequency lists, and EPWING dictionary support if you're into that. The full list is on their homepage. ↩ I'll assume concepts like notes and types are somewhat clear to the reader. If not, I refer to that particular section of my previous Anki article Creating your own cards… Efficiently ! for a more detailed guide on notes and setting up templates. ↩ I personally keep several sets containing rikai-powered cards. A set for each new book I read, sets for vocab related to my studies, and a set for whatever I come across online. I do this to keep my priorities in check: during exams I give less priority to keeping up to date with my books/misc vocab. ↩ Courtesy of Leuven University's very own Japanology department! More info on the Waran Jiten Japanese-Dutch dictionary at http://japansnederlandswoordenboek.org/index.php/Hoofdpagina ↩ The single most satisfying scene from the Japanese edition of the first Harry Potter novel. ↩ Actually, this guy made a lot of surprisingly helpful tools for learning Japanese, including the more popular subs2srs for creating Anki sets based on subtitles and video, and OCR Manga Reader, a manga reader with Rikaichan-like functionality using optical character recognition. Full list is at http://rtkwiki.koohii.com/wiki/Community_Tools ↩ I'll refrain from linking here as that's legally gray territory. ↩ Then again, Joyce's Ulysses is probably an easy read as well compared to the likes of Proust's In Search of Lost Time, so I'll come back to this statement when I've judged for myself. ↩ According to Wikipedia, they host over 10.000 works including both out-of-copyright works or those made freely available by the authors. Read more: https://en.wikipedia.org/wiki/Aozora_Bunko ↩ If you're watching Clannad: After Story afterwards, keep tissues at hand. ↩ Actually, while Rikaisama improves greatly upon Rikaichan, there's still some features I'd like to see implemented such as the ability to display sanseido-mode J-J as well as default J-E at the same time, or to select which definition to ankify. The developer set up a to-do list of sorts, but as it's a one-man hobbyist project I'll expect these to be implemented rather slowly. ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2016/09/a-quick-guide-on-using-anki-2-an-efficient-vocab-mining-set-up-with-anki-and-rikaisama/","loc":"https://steviepoppe.net/blog/2016/09/a-quick-guide-on-using-anki-2-an-efficient-vocab-mining-set-up-with-anki-and-rikaisama/"},{"title":"A Quick Guide on Using Anki (effectively) (in an academic context)","text":"Those who know me well might accuse me of preaching popular memorization tool Anki as a revolution in studying, and they wouldn't be that far off. Over the past year I have extensively integrated this software in my studies, with (to me personally) remarkable results in both productivity and academic grades. There are, however, some pitfalls for the casual user to take heed of, and getting the full benefit of your time requires cultivating a proper mindset. Considering the amounts of time I mention Anki abundantly on this site, I decided to write my own short guide on setting up and using Anki efficiently, as well as describe how I've personally integrated Anki in my studies. Anyone with an interest in studying Japanese will surely have heard of Anki. It's covered extensively on all major learning resources on-line, and the canonical self learner's beginner path seems to be that of Heisig's Kanji method , Tae Kim's online Grammar Guide , and Core 2K/6K/10K, 2 all three heavily relying on Anki. Language acquisition certainly remains Anki's most popular usage, although the software has become broad enough to find it's way in various other branches (it's become an integrated routine for medical students, for example, as well as math students thanks to LaTeX support). While it sounds like I'm being paid to advertise Anki, the application remains free and open source (and if you don't like synchronizing your Anki progress over Anki's servers you could even set that up on your own as well). 3 What is Anki? Why is Anki?! How is Anki?!? To recap: Anki is an extremely customizable application designed for memorization, available for free on nearly all the major devices 4 as well as being accessible online. Being able to study on the road (I commute by train, taking me up to three hours of total traveling time every day) and synchronizing your progress at home is a major boon on using your time practically. There are plenty of studies on methods for efficient learning, and not surprisingly, classic late-night cramming, along passive studying methods as extensive marking and rereading of texts don't score very high. On the other hand, active recall testing (questioning yourself by actively trying to come up with the answer to a question) and spaced learning (spreading information absorption over long periods of time) are considered key instruments. 5 Anki is an application that achieves both through timed flashcards. The concept is simple: the front of the card displays a question and the backside displays the answer. After showing the answer, you decide whether, and to what extent, you've passed. A highly optimized algorithm uses said information to determine the card's next appearance. The Guide Setting up! Download & Install Download the application for whatever system you'd like on Anki's homepage and follow the installer's steps. I highly recommend investing into a lower-end smart-phone if you don't own one already and installing a mobile variant of Anki as well. Might be somewhat of a steep investment, but I believe it's definitely worth the cost in the long run (not just for Anki but plenty of other learning tools such as dictionaries or OCR tools). 6 Anki's layout, especially the desktop application, is pretty bare-bones (although functional and self-explanatory enough to get used to in a couple of minutes). Account & Syncing To synchronize your progress over different devices you'll have to register an account on Anki's web service (I definitely encourage doing this regardless: just imagine the horror if your device breaks down after 3 years of having intensively studied thousands upon thousands of flashcards). On your desktop Anki, open Settings → Preferences from the menu, then select the Network tab and insert your user credentials. From the Anki's main menu, select the syncing icon. Now do the same on your mobile device. On Ankidroid this is Settings → AnkiDroid → AnkiWeb Account . Make sure the \" Download Media \" check-box below \" Synchronize \" is marked ! Now try syncing from this device as well. If everything goes well, we'll start with our first set right away. Creating your first set and cards! It's best to consider Anki's main interface as a container for your decks (also called sets) of flashcards. Aside from a filler deck called Default , there's no content as to speak of yet on a fresh install. Feel free to remove that one (set-specific options such as deletion are accessible through the icon on the right of each set). There's a lot of pre-made content on the internet, but in order to get used to the interface and learn some of the base features, we'll create a small set, set up a template for our notes, and create some cards all by ourselves . Don't worry if the terminology doesn't make sense yet, we'll cover that over this guide. To demonstrate, we'll create a set containing some important dates, events and notables in Japanese history. Click the Create Set button in the bottom bar, and feel free to name it something along Japanese History 101 . We'll create two more sets, 0 - Important dates and 1 - Meiji . Click and drag these right below the original Japanese History set to make ‘sub-decks' (child-nodes) out of them, as shown below. Anki orders sets alphabetically, so I often use numbers to realize my intended structure. Now click on \"1 - Meiji\" to enter that set. As there's no content yet you'll get an appropriate congratulatory message. Doesn't matter, we're on a different mission now, on a mission to create . Click on Add to start on your first card. This is how it will look like in the end. While it might sound somewhat contradictory, I should mention you'll never actually create ‘cards' (at least, in Anki-jargon) directly. Instead, what you're actually making now is a ‘note' , a kind of blueprint containing all information relevant to a certain topic. This is useful because you'll often want different cards questioning you on the same subject. A so-called ‘note' on a particular foreign word might contain said foreign word, its translation, an audio reading and an example sentence. Based on this note, you'd be able to generate several cards questioning based on the foreign word, on the translation, or on the audio. This requires more jargon. Following my explanation, ‘notes' are blueprints for creating cards and so-called note-types are blueprints for ‘notes'. If we want ‘notes' with data on a foreign word, as well as its translation, an audio reading and an example sentence, we'll need a note-blueprint (note-type) with four fields: ‘term', ‘translation', ‘audio' and ‘example sentence'. Based on this note-type, we'll be able to create notes on foreign vocabulary to our heart's extent. Generation and styling of the actual ‘cards' will also be set up in said ‘note-type'. Following through our language example, we might want to generate three cards: one questioning you on visual recognition (a certain term), one on oral recognition (audio), and one on production (translation), with all three displaying an example sentence along your answer. I made a brief, crude diagram trying to visualize the result of this tutorial. We're almost done. Earlier, we made a ‘set'. A ‘set' (or it's child-set) contains ‘notes'. As mentioned above, those ‘notes' belong to a certain note-type and include settings like the amount of fields it contains. In a ‘note-type', you'll also specify ‘card-types' . ‘Card-types' serve as blueprint for the actual ‘cards' you'll study: they determine the lay-out side of things: which fields are shown and how are they made up. To summarize in this article's most confusing sentence to date: when create a new ‘note' , it will generate ‘cards' based on the ‘card-types' contained in your note's ‘note-type' . If this doesn't make sense yet, don't worry, you'll get it soon enough by doing this tutorial. Type:Basic means we'll create, unsurprisingly, a note of the basic type. These notes have only two fields (a ‘front' and ‘back'), and only one generated card: one that shows you the ‘front' as query, and the ‘back' as response. We'll create a new note-type in a second, but for our first note this is fine: a card on Ito Hirobumi , an important figure in the modernization of Japan during the 19 th century, using the brief summary below. Inserting images is through copy-pasting. As example, copy Ito's portrait on the right and paste it in your Create Note screen. Who Lower-rank Samurai in Choshu - politician - 4x PM - Resident-General of (protectorate) Korea - founding father of Modern Japan - assassinated by a Korean nationalist. When 1841 - 1909 Where Born in Choshu, studied in Universy College London (1863) together with Choshu Five . Spent 18 months in Europa studying different constitutions. What Originally a sonno joi movement member before foreign studies. On return, warned Japan against Shimonoseki passage war. Set up a cabinet and tax system, became prime minister and set up Meiji Constitution . Also important for the removal of several Unequal Treaties. It's a good practice to properly ‘tag' your cards as well. You can add multiple tags, separating them with spaces. This allows for quick sorting in Anki's card browser. You'll end up with something like the screen-shot to the right. Finish up by clicking add . Congrats, keep this up and you'll be breezing through your exams in no time! Creating your own cards… Efficiently ! Our first ‘efficient' card will be one on what is considered a tipping point in recent Japanese history: the Meiji Restoration in 1868 . First we'll create a new ‘note-type' to generate two ‘cards', and next, alter the basic template of the ‘card-type' belonging to the ‘note-type' to allow pop-up furigana on touch. This one's meant to get a feeling on Anki's diverse extension options. You'll end up with these cards: First, switch sets to the ‘ 0 - Important dates ‘ subset. If you're still on the previous add-screen, switch sets by clicking on the corresponding set button near the top right. There are various ways of creating a new ‘note-type'. For now, just click on the basic type, then manage on your next dialog screen, and add on the one thereafter. Select the first option, and name it something like DateEventDetail . Return and select this new type as your active ‘note-type'. Next up, you'll be editing the this note-type to contain three fields. You'll want a ‘note' that: contains the event's specific date ( 1868 ), the event's name ( Meiji Restoration ) a field for more detailed information . Thus in the field screen, add a new field called Detail , and rename the other two to Date and Event . Let's close this screen and add same data to our fields. For the Detail field, copy-paste the following text. Copy-pasting doesn't include our preferred layout yet; we'll do this manually using the tool-bar demonstrated in the screen-shot below (or using classic shortcuts as ctrl-b used in other text-editors). Date and Event will respectively contain 1868 and Meiji Restoration . When finished, don't click on add just yet! Cause Commodore Matthew C. Perry (Convention of Kanagawa) - unequal treaties (Harris Treaty) - sonno joi movement Rise Satsuma-Choshu Alliance - resignation of Tokugawa Yoshinobu (15 th and last Tokugawa Shogun) - Boshin War Key points Restoration of imperial rule : return of the Emperor's practical abilities The end of Sakoku policy , as well as the Bushi dominated Bakufu, and 265 years of rule by Tokugawa Shogunate Five Charter Oath promulgated at Emperor Meiji's enthronement. \" one reign, one era name \" ( 一世一元 ( いっせいいちげん ) ) system adopted: the start of eras named after their emperor. Effect Meiji constititution ( Ito Hirobumi , 1890) - abolition of the han system ( 廃藩置県 ( はいはんちけん ) ) - abolition of the four class system ( 士農工商 ( しのうこうしょう ) ) - land and tax reforms - nationwide subscription - satsuma rebellion - Rapid industrialization - 富国強兵 ( ふこくきょうへい ) and militarization Finally, we'll change the ‘cards' generated out of this ‘note'. Click on the Cards button and take a look around. Terms in double brackets (eg. {{Detail}} or {{Date}} ) are variables and represent either your note's fields or its front/backside (eg. {{FrontSide}} ). Through tags as <br> and <hr> , you might notice cards use HTML (a markup language used in web-development), and the style-screen in the middle of the window defines your card's style using CSS (a language standard for web design): colors, font-types, etc. Note that although it could help, you really don't need any experience in these to make useful cards. Start by editing your current card. Name it date by clicking more → rename . Next you'll want to add the contents of your detail field to your backside's template. <br><br> creates two line breaks in HTML, adding sufficient room between the content of the two fields. 1 2 3 4 5 6 7 {{FrontSide}} < hr id = answer > {{Event}} < br >< br > {{Detail}} This card will question you on the date: What happened in 1868? That's right, the Meiji Period. Have some extra information! Now click on the + button in the upper-right corner to add a new card to generate. Name it Event . This time you'll edit the templates of the front and back by switching the {{Event}} and {{Date}} variables around; this will question you what happened on the event's date. Meiji Period? Oh yeah, 1868. When finished, you should now have something similar to the screen-shot on the left. Useful, right? That's just the start of how extensible this tool really is! If you're still with me, I'd like to introduce you one last useful feature (for now). This one might be less relevant for those not studying Japanese, but honestly, it's a neat one to keep in mind when designing your cards (I occasionally use it outside language-learning as well). Remember those Japanese characters up there? They had furigana 7 on top. We'd probably want this in our cards as well. Luck goes it setting this up is extremely simple and support comes out-of-the-box: just add furigana : (all lower-caps) in front of your field variables in the card-note template screen you're in, eg. {{Detail}} becomes {{furigana:Detail}} (you won't see any visible changes yet as we haven't added the actual furigana in our text itself yet). Actually, to accommodate Japanese learners, we'll take away the crutches of kana-reading aids and let it display only on click/touch. Add the CSS below to the bottom of your note's style (the one in the middle). This hides all furigana (or rather, ruby annotations) by default and displays them upon touch or hover. 1 2 ruby rt { visibility : hidden ; } ruby : hover rt { visibility : visible ; } Now, add the actual furigana (contained in square brackets) next to their respective words: eg. 一世一元[いっせいいちげん] , 廃藩置県[はいはんちけん] , 士農工商[しのうこうしょう] , and 富国強兵[ふこくきょうへい] . When done, let's wrap things up by clicking add . Now you've created your first set(s) and a couple of cards, I highly recommend changing the studying settings tied to your sets as well. Out of the box, Anki will add 20 new cards and allow only 100 repetitions a day . This is highly inefficient. We'll want to remove the top barrier and be in control of the amount of new cards we study. Open your main deck's settings by clicking the icon to the right and selecting settings. Don't worry, these settings pertain all sets fixed to this \"option group\", so you won't have to do this manually for all your new sets. Edit these three elements: New Cards : new cards/day -> 0 Repetitions : maximum reps/day -> 9999 Mistakes : action on difficult cards -> tag only Instead of an arbitrary number of new cards every single day, you'll be in control yourself by clicking \" customized study \" from the set's menu, and choosing the amount of new cards you'd like to study. I've exported my own copy of this example and uploaded if you want to compare or save yourself the work doing it manually: Download : Example set Japanese History Your first set… Efficiently ! If you're starting on a new language, you'll probably want to start by learning basic vocabulary common in every language (eg. words such as \"person\", \"name\", etc). If that's the case, there's no need to reinvent the wheel: look for pre-made decks and if you will, adjust the card lay-out to your own liking. Anki's own shared deck repository is a good place to start. If there are no pre-made decks to your liking, but you have access to large data files such as large word lists, another solution would be to import CSV 8 files or other forms of text separated by tabs or semicolons and mapping the contents to your note's fields. An added benefit of this approach is it's suitability for group (or class) work, by setting up a googledoc sheet, and through a joint-effort creating a large spreadsheet everyone could export as CSV. If there's demand, I could hack up a small (we'll see) guide on setting this up. Personally, I've set up my environment to import any new Japanese word I encounter on-line and would like to study straight into Anki with audio and dictionary definitions. As this goes beyond the scope of this article, I've written another tutorial on that, and definitely recommend that approach for Japanese learners. Structuring Data \" But Stevie \", I hear you ask, \" how do I study less structured information with Anki? \" Good question. This involves some thinking on your part: what is it that you'd like to reproduce? What information would you like to memorize? If you're a student, think creatively on your course and how you could structure its contents in bite-sized pieces to feed your Ankiset. 9 Courses are designed with a certain logic in mind, after all. You could start off by making subsets on each of the big chapters of your textbook, or create the answer to potential questions first, mark the important data next, and Ankify 10 this. This process is an important step in grasping the big picture and part of the actual studying . This is the exact reason why I'm reluctant to share some of my own sets that don't involve the need for just rote memorization. Over the past two years at university I've devised and adjusted my own method to structure and create Anki-sets. Feel free to try it yourself (your mileage might vary). It goes somewhat as follows: Structure : create a structure based on the course's structure (this could be as simple as copying the Table of Contents) or on repeating elements (certain theories, models, periods in time, artists, their works, etc). If the content of a single class is relatively self-containing, this is even easier. Use this as template for your set. Analyze : analyze all your available sources: your textbook, own notes, power-point presentations, etc. Find repeating elements, highlighted or marked parts, and everything that strikes as important. There's a reason for that and those should be given priority on memorization. Divide : try to divide and rephrase that information into questions suitable for flashcards. Use the Five Ws 11 as guideline if stuck (What, Who, When, Where, Why). Try to remain brief in the backside of the flashcard: I often break this rule myself, but, if you have to scroll down through your card it's usually too long. Style : Different information requires different styling. Alter your templates and note types to reflect these by using graphical elements, cloze-encounter, audio, different directions, etc. A simple example. To memorize important dates, I use a card with three fields (Year, Event, Information) and generate two notes on these: one displaying the year in front, one displaying the event. Both show further details on the back. Study : You've analyzed and processed the contents in a self-containing Anki-set and thus done half the studying already. Congratulations. Now get started on those new cards and drill that knowledge . If I studied something marketing-oriented I'd probably call this the SADSS-model and write an e-book called \" 5 Steps To Master All Knowledge (and make you a more successful person in life) \", free of charge if you subscribe to my non-existent newsletters. If I were a social media marketeer I'd add \"Number 5 will shock you!\" as subtitle. (Don't worry. I won't do either.) Spaced Learning \" Stevie! If I'm a college student studying one, or several, language(s) alongside other theoretical classes (and I most likely am since I'm reading this blog), won't this mean I get to repeat hundreds to thousands of cards every single day?! \" Yes. Yes it does mean that. Think of it this way. You'll have to study either way. Language acquisition comes with it's sacrifices: you can't just stop studying for a few weeks and expect to come out unharmed. Building up an internal dictionary containing tens of thousands of words you recognize on sight or produce actively, utilizing grammatical structures on the spot (often under severe pressure), or comprehending speech at real-time; all of these require an effort not just to memorize, but to maintain as well. Same goes for any other class, you can't cram your way through university. What this method offers is a way of structuring your data and studying efficiently: paradoxically by studying more you'll actually save time, as there's no need to repeat what you already know. If you're utilizing Spaced Repetition 12 , the data you need to know will be served (roughly) the moment you're about to forget it. \" Even if I'm on a holiday? This makes me very sad! :( I'll just pretend this method of studying is not suitable for me and dismiss anki forever! \" Sadly, this does mean maintaining your cards even when you'd prefer spending your day in a different way. There's no pause button to put your cards on hold, so if you fail to go through them several days in row, you'll have piled up all those lost days and take forever to get through them again. Of course, once the set has finished it's purpose (usually after preparing for an exam), you could just as well delete it. It does feel as a waste (after all, what's the point of studying in the first place?), but sadly it's just unsustainable to maintain everything, all of this deep knowledge, till the end of times. Another approach here would be to suspend the more trivial cards and just stick on the broad outlines. The choice lies in your own hands!™ I spent about a month on a road trip in the USA this summer; unforgettable memories aside I did maintain my Ankisets the whole time. But as I removed all of my non-Japanese sets and maintained the bare necessities of studying the language, this meant only about half an hour of studying each day. That's not too bad, huh? Anyway. As stated earlier, actively studying will be unavoidable as a student. It is entirely possible that the method Anki and it's spaced repetition provide don't actually work for you, but just don't lie to yourself either . 13 Find another way that does work for you. There are plenty of studying methods that could be suitable (including some interesting new ones 14 gaining popularity), but more likely than not, reading through your textbook a few times and cramming a summary the night ahead of an exam won't be one of them. \" But wait Stevie hold up, if I get this right, I'll be forced to use Anki, like, forever? Won't I ever be able to just quit and bask in my new-found fluency? \" This one's a bit difficult. I've heard of people using Anki for over 8 years , applying it solely as a language maintenance tool and only very occasionally adding new vocab cards when appropriate. This rings more true to me. Spaced learning means they're only getting like 5 - 10 minutes worth of repeating old cards by now anyway so why not? If there's ever a time, however, when you're comfortable enough to process media as-is, and you feel you've been a slave to Anki for far too long, don't let me stop you; uninstall Anki and bask in your newfound freedom! Just… don't let all that progress go to waste! Maintain your language skills! Wait! There is more! The main goal of this post was to introduce Anki both in it's core usage as well as the wide range of appliances in the field of both linguistic and academic studies. If you've any further questions, feel free to leave a comment below. That being said, Anki is customizable to the point books have been written on the topic. Trying to contain everything in a single post (\" A quick guide to Anki \") is an impossible mission. There are several other features I feel are useful (and underrepresented) enough in mainstream language blogs to warrant further writing though (namely Anki's API, statistics, useful plug-ins, and integrating Rikaisama and J-pod101's audio). Check these out in my next blogs: Setting up a perfect vocab-mining environment with Anki and Rikaisama Using Anki's API to contextualize your vocab cards with example sentences Making the switch: J-J definitions in your vocab cards Further reading Anki Manual : the official Anki manual. If you're into that kind of thing, theirs is excellent and very comprehensive. I recommend saving these for after you've gotten used to the software a bit. Japaneselevelup.com : one of the most popular Japanese self-learner resources out there, themed after a roleplaying videogame (a bit gimmicky but there's a large audience for that stuff). Probably mentions Anki more as I do ( my god ). Twenty rules of formulating knowledge : an article on making quality flashcards by Dr Piotr Wozniak , the author of the very first SRS algorithm as used in Anki predecessor SuperMemo . Anki Essentials : a 100+ page guide on using Anki. Told you there were books written on Anki. I'm not much of a fan on the downloadable self-help e-book hype but for what I've read, this one's surprisingly good. Image taken from the 2012 Japanese animated film Wolf Children by Mamoru Hosoda, used under Fair Use doctrine. ↩ Flashcard sets consisting of respectively 2000, 6000 or 1000 words with example sentences, all accompanied with high quality audio. They're available both as Ankisets or on Anki alternative Memrise. As I've started learning Japanese through university I can't attest for it's helpfulness yet, but the addition of spoken sentences for context is a major advantage indeed. ↩ This is for more technical users and there really is no reason not to use AnkiWeb's servers, unless they're permanently down for some reason. Read more on https://github.com/dsnopek/anki-sync-server . ↩ Except if you're on iPhone, but if you're able to afford Apple products that shouldn't be much of a problem. Aside from donations that's the only income the developers get anyway, and it's license is peanuts compared to buying any triple-A video game on current-gen consoles. ↩ Karpicke, Jeffrey D., en Henry L. Roediger. 2008. \"The Critical Importance of Retrieval for Learning\". Science 319 (5865): 966–68. ↩ OCR : O ptical C haracter R ecognition: basically recognizing text or characters through an image (like a scan or photograph taken with your cellphone). I recently found this free manga reader for Android with OCR and dictionary which looks pretty cool. Check it out at http://ocrmangareaderforandroid.sourceforge.net/ ↩ A Japanese reading aid by printing the word's pronunciation in syllabic characters above the original word. A more general term for this is ruby, and such annotations are commonly used for the romanization of other languages as well. Read more: https://en.wikipedia.org/wiki/Ruby_character ↩ C omma- S eperated V alue files: plain text containing data formatted as table by separating them through commas. ↩ The danger in this is learning information loose from it's context. Be sure to structure your ankiset accordingly as well to remain aware of the big picture. ↩ to ankify : \" to process large sets of information into small, managable flashcards prepared for the memorization tool Anki \". At least, that's how I'd expect to see this term if it were to actually exist. ↩ A popular formula of questions, whose answers are considered as base intel, in any type of problem-solving, research or journalism. Read more at https://en.wikipedia.org/wiki/Five_Ws ↩ SRS or S paced R epetition S ystem is how the Anki/learning community commonly referred to spatial learning. ↩ Actually there are several notables arguing against Spaced Repetition, but honestly, the real value in these articles lies in the counter-arguments provided in the comments. Example: https://www.scotthyoung.com/blog/2012/08/05/forgetting-is-good/ ↩ Memory Palaces (also known as Method of Loci ) are gaining some new popularity thanks to TV series Sherlock, and rely on spatial memory. Venn Diagrams and other kinds of graphical tools also target our visual memory, and are quite helpful, but these could surely be combined with SRS (in fact, I believe visual stimuli are fundamental to your sets anyway). ↩","tags":"Studies","url":"https://steviepoppe.net/blog/2016/09/a-quick-guide-on-using-anki-effectively-in-an-academic-context/","loc":"https://steviepoppe.net/blog/2016/09/a-quick-guide-on-using-anki-effectively-in-an-academic-context/"},{"title":"Parsing Japanese Text in Markdown-Python for Stylizing and Semantic Purposes","text":"Due to my studies I (will) often use Japanese in my blog. As I gave some thought to typography and readability, I found the default appearance of Japanese text to be in stark contrast with the rest of my design. 1 To target specifically Japanese text, I wrote a small Markdown-Python extension for use in static blog generators as Jekyll and Pelican (or pretty much anything that utilizes Markdown-Python to parse Markdown in HTML) and embed such text in a span with the language attribute set to Japanese. The added, and probably more important bonus, aside from styling and semantic reasons, is that this method counters the negative effects of Han unification in so-called CJK-languages. Download I've added the extension on its own repository on my GitHub for anyone interested, but as it serves its purpose for me as-is I have no further interest in maintaining it at the moment. 2 Installation Copy the japanese.py script into your python-markdown extension directory. If you're using Pelican as static site generator, open your project's pelicanconf.py and add 'japanese' to the MD_EXTENSIONS list: 1 MD_EXTENSIONS = [ 'japanese' ] Usage Using a simple regular expression (\\{\\{)(.+?)(\\}\\}) , the extension treats double {} brackets as span tags with a lang=\"ja\" attribute. 1 {{読書クラブ}} will output 1 < span lang = \"ja\" > 読書クラブ </ span > Example 1 (fonts): just compare 読書 ( どくしょ ) クラブ (custom) to 読書 ( どくしょ ) クラブ (Meiryo) to 読書 ( どくしょ ) クラブ (MS Gothic default). 3 Example 2 (unihan): compare the Chinese to Japanese characters: 隆 ( 隆 ), 誤 ( 誤 ), 直 ( 直 ). 4 Styling Although it's a bit of a risk performance-wise, I'm quite a fan of Google's free web-fonts. 5 Due the complexity of the Japanese character-set, development on these have been slow 6 , but Google's Noto Font is getting quite efficient and with the Japanese font set supporting near 7000 characters, it should pose no problem for most web-projects. Since it works better, typography-wise, with the rest of my fonts, I use this one over fonts as Meiryo that are more widespread across all platforms. Using the CSS below, I ensure max compatibility by using Meiryo and others as fall-back if the page can't connect to Google's font API. 1 2 3 4 5 6 7 8 @ import url ( https :// fonts . googleapis . com / earlyaccess / notosansjapanese . css ) ; [ lang = \"ja\" ] { font-family : \"Noto Sans Japanese\" , \"メイリオ\" , \"Meiryo\" , \"ヒラギノ角ゴ Pro W3\" , \"Hiragino Kaku Gothic Pro\" , \"ＭＳ Ｐゴシック\" , \"MS PGothic\" , Sans-Serif ; font-weight : 100 ; font-size : 95 % ; } Further reading http://www.growingwiththeweb.com/2014/03/languages-and-chinese-characters-on-the-web.html http://nimbupani.com/declaring-languages-in-html-5.html https://design.studio-umi.jp/blog/google-web-font-japan http://kanjialive.com/2013/04/selecting-a-better-japanese-font-for-windows-web-browsers/ This is less so on mobile devices. Most Windows web browsers default to MS Gothic, lacking anti-aliasing found in newer fonts as Meiryo, and require some manual adjustments. For maximal compatibility, I prefer to do this in-site. If no further customization is necessary, just adding Meiryo as fall-back font in the page's font-family is sufficient, e.g. font-family: Arial, Helvetica, Meiryo,sans-serif; . ↩ A possible extension could be one where different regular expressions test for different languages and thus deliver different lang attributes. If I find the need for that on my own blog (eg. Korean), I'll update this. ↩ For furigana support I use a slightly edited version of an existing MD extension available at https://github.com/djfun/furigana_markdown . ↩ If a multilingual page uses only Japanese, it's sufficient to put a Japanese font as fall-back in the body's font-family. If occasionally Chinese or Korean characters are used as well, this approach, aside from semantic benefits, remains more recommended. ↩ I use Quicksand and Poirot One for all my latin-based text on this page, for example. ↩ Adobe's competing, subscription-based Typekit apparently offers a wider range of Japanese web-fonts for anyone interested: https://typekit.com/fonts?script=japanese . ↩","tags":"technical","url":"https://steviepoppe.net/blog/2016/09/parsing-japanese-text-in-markdown-python-for-stylizing-and-semantic-purposes/","loc":"https://steviepoppe.net/blog/2016/09/parsing-japanese-text-in-markdown-python-for-stylizing-and-semantic-purposes/"},{"title":"Visiting a Japanese Elementary School - An (Awkward) Experience to Remember","text":"Earlier this year us second year Japanese Students were invited to participate in an international speech contest (theme: \"tell an interesting story\"), organized by Kobe University. Just this week we were informed that the video-recordings of those entries had been uploaded to the contest's website and were available for all to watch. 2 My story is a classic \"awkward Japanese language mix-up\" anecdote from a school trip to a Japanese elementary school in Brussels, which I'll touch upon in this post, alongside the embedded video. ( 日本の方、あまり自信がないのですが、もしよろしければ是非ともご覧ください！ ) Edit : I just found out that I somehow managed to win first prize out of all the foreign contestants!! This is so cool to hear! Thanks for the cookies, Kobe University! :D The Video This video, along with my participation entry, is available at speech-data.jp . The story The story isn't that interesting and a bit embarrassing, but here goes anyway! As second year students Japanese Studies, our class participated in an excursion to a Japanese elementary school in Brussels: a fun opportunity to practice our language skills as well as have some fun playing games and learn about cultural differences and customs. The event even warranted a small post on the JSB 3 section on yahoo.co.jp ! In advance of this trip, we split our class in several groups, each preparing a presentation or activity related to our country. Several students in our group live in Antwerp, thus we thought it fun to hold a small presentation on the famous Antwerp folklore story of Druon Antigoon , an evil giant guarding a bridge and exacting an exorbitant toll of those wishing to cross. Failing to cross meant getting your hand chopped off and thrown into the river. 4 A tad morbid, but it worked well with a popular Belgian children's game we thought fun to teach them, and in addition, there's some really nice chocolate treats based on this tale we wanted to distribute afterwards. The game The game is a variation of a classic tag game called \" schipper, mag ik over varen? \" (Eng: \" Skipper, may I cross? \") and usually played on a school's courtyard or in a larger PE classroom. To play, the territory gets divided into two and the students line up on either side. One or two volunteers play the role of \"skipper\" or \"boatman\" and line up in the middle (I drew a quick MS paint diagram below, don't judge). Each round, those on the border are expected to reach the other side. Those playing the role of skipper make up a rule that has to be upheld to attain passage. Rules to pass could be anything: those wearing glasses, those skipping on one leg, etc. Not being able to uphold these rules however means running past the skipper and not being caught: if you're tagged, you become a skipper as well. The more skippers, the more difficult the game becomes. The shame A classmate and me played the role of skipper and did pretty well making up rules in Japanese. Near the end however, I noticed several of the remaining players wearing pink pants. \" I should definitely use this information for the next skipping rule \", I thought to myself. Now, the common word for pants in Japanese is ズボン ( zubon ), probably one of the first words we learned. Except during the heat of the moment I mixed this one up with another Japanese word very similar to the English \"pants\". The end … Long story short, in front of not just my own teachers but all of the Japanese staff present, I asked a bunch of 12 year old girls to pass if they wore pink パンツ ( pantsu ), a word commonly reserved for underwear. Hope I won't make that mistake again. 5 Nikko Bridge by Stevie Poppe ( https://flic.kr/p/LKfDn7 - CC BY-SA 2.0) ↩ These videos can be found @ http://www.speech-data.jp/chotto/2015F_sub/ ↩ Short for the J apanese S chool of B russels, read more on this on Wikipedia ! ↩ Lucky for us Antwerp citizens, a young soldier named Brabo cuts of the giant's hand in return and casts it into the river, leaving us with some nice statues near the city square and an interesting story to tell tourists. Read more on this on Wikipedia ! ↩ I was confronted with a loud 「えええええええぇぇぇぇぇぇッ！！！」 coming from these girls' general direction. Everyone else looked particularly amused however, so that's something at least. ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2016/08/visiting-a-japanese-elementary-school-an-awkward-experience-to-remember/","loc":"https://steviepoppe.net/blog/2016/08/visiting-a-japanese-elementary-school-an-awkward-experience-to-remember/"},{"title":"Travels: an Icelandic Winter","text":"After a long, stressful first semester exam-period, I spent my brief time off traveling to two completely opposite places I've been wanting to visit for a very long time: Rome, Italy and Iceland : one of the coldest winter destinations I could imagine, but a place of stunning natural beauty. Over the past years, Iceland has gained somewhat of a reputation for its hipster trends and excellent music scene (both true), but its biggest attraction to me personally remains its wild, barren landscapes: sceneries that appeared downright alien at times. What follows is our brief itinerary through this beautiful country. Brief history Iceland, a volcanically active island close to the Arctic Circle, was first inhabited by in the 9 th century by, traditionally claimed, vikings or Norwegian settlers. Most of present-day Icelandic culture finds its roots in its Scandinavian heritage, and even now is mostly famous for its medieval sagas and traditional literature and cuisine, as well as its language closely related to Old Norse. As the Icelandic plateau is one of the newest in the world, the surroundings closely resemble what our continents must've looked like millions of years ago. Agriculturally wise, almost everything is imported from countries as Canada, as the only native growths here are limited to bleak bushes. Itinerary Having only a few days off in-between my exams and the start of my next semester, we were very limited in our itinerary. Even more, neither of us had a driver license at this point, so we were forced to book several tours to show us around (a thing I generally dislike, but actually ended up really pleasant). We stayed several nights in Reykjavik as day-trips to iconic views were easiest to reach. Golden Circle Tour On our first proper day, we took a tour leading us through the Golden Circle , the most popular tourist route in Iceland. the host driving us around was an elderly man taking great pride in his viking roots and often talked of bygone farmer days and Icelandic folklore. On our way to our first hallmark, we encountered these beauties: Geysir Geysir is the geyser the English word is derived from and erupts about every 10 minutes, hurling boiling water up to 70 meters in the air. Gulfoss Gulfoss , the Icelandic term for \"Golden Falls\", is an iconic waterfall located in the midst of a large canyon. National Park Parliament Thingvellir , a national mark in southwestern Iceland, lies in a valley known for its Althing , the national Parliament. It was established in 930 and is generally considered as the founding point of the Icelandic nation. It held sessions until 1798, and is thus one of the longest-running parliaments in the world. Northern Lights In the evening, we took a bus to a more desolate spot outside of the city and waited for polar activity. We were extremely lucky to have a clear sky in this period (heavy snowstorms hit Iceland shortly before our arrival as well as on our last day) and after a few hours of waiting were graced with ghostly green shimmmers dancing in the dark (excuse my attempt at sounding poetic). Alas my camera was not strong enough to record any of this, so you'll just have to take my word for it's magic. Second Tour The next day, we took another tour driving us around several other landmarks, including the famous volcano Eyjafjallajokull which erupted in 2010. Our host, an extremely pragmatic man, felt no need to recite nostalgic childhood memories and instead informed us on current-day issues such as the economic crisis, the political system and farmers' daily lives. Eyjafjallajokull Vatna Glacier Fans of Game of Thrones might recognize Vatnajökull , the largest ice cap in Iceland, as a scene in the series' second season. The barren surroundings contrasted strongly with the bright, blue reflection of the enormous mass of ice in front of us. Black Sea Our last stop was the Reynisfjara shore , a black pebble beach near an impressive cliff. A snowstorm hit us around this period and walking around was extremely difficult. Reykjavik We spent one day walking around Iceland's capital, a city with a population of around 130,000. Bearded men walk around wearing lumberjack shirts in the middle of the winter, and blend in well with the picturesque and colourful houses as well as the various boutiques and stores that could only be referred to as being hipster. We visited spots as the Reykjavik Concert & Conference Center, known for its beautiful glass architecture, and the Church of Hallgrimur, the largest church in Iceland. We had some great food and shopping around this place, but by the time we were ready to depart to our final destination, another heavy snowstorm made its appearance. Thanks to a kind by-passer we reached our bus-stop in time for a drive to Kevlavik, a small village close by the airport. As our airplane left quite early the next morning, we felt it'd be safest to spend our night nearby. Kevlavik Unfortunately, our bus stop was pretty far from our actual destination, and as Kevlavik was formed around and out-of-use military base, nothing really stood out as bed & breakfast kind of place. By this time, the storm reached a peak with winds reading over 80KM per hour, and walking through either 30cCMof snow or slippery ice made this an impossible feat, especially since we were all by ourselves. At one point we were literally clinging to the pole of a traffic light for approximately 10 minutes (felt longer) until a car passing by was kind enough to give us a ride to our B&B. Safe to say, we didn't go out again for sightseeing that day. Music When traveling, I make it a habit of finding new music by listening to local radio stations or asking recommendations to people I meet in hostels or bars - It's my own personal souvenir and sticks better in my mind than regular touristic souvenirs. Despite a relatively small population, Iceland (particularly Reykjavik) is well known for having an excellent music scene, with such world-class acts as Bjork , Sigur Ros and Of Monsters and Men . I came across this trip's band, Kiasmos , while visiting a bookstore and swiftly fell in love with the duo's experimental, almost drone-like ambient electronic music. One of the duo's members, Ólafur Arnalds , has already built a name for himself mixing his classical piano background with electronic influences and is definitely one to remember as well! Conclusion It's a shame we had such a short stay, as Iceland came over as a beautiful country unlike any other, with a rich and fascinating history and splendid alien-like landmarks. Our personal biggest drawbacks were a lack of money and driving licenses; I would definitely return if neither of these formed a problem. Gallery Iceland Winter by Stevie Poppe ( https://flic.kr/p/NskHGR - CC BY-SA 2.0) ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2016/08/travels-an-icelandic-winter/","loc":"https://steviepoppe.net/blog/2016/08/travels-an-icelandic-winter/"},{"title":"Travels: Romance in Rome","text":"After a long, stressful first semester exam-period, I spent my brief time off traveling to two completely opposite places I've been wanting to visit for a very long time: Iceland and the historic city of Rome, Italy, cradle to one of the most well-documented civilizations in western history. In order to recharge our batteries, we decided on this 4-day city trip and, looking back, it really did turn out to be a city that could only be described as a living, breathing open-air museum. What follows is our brief itinerary through this charming city. Brief history Where does one start! Rome, Italy's capital and largest city, has been an extremely influential city for nearly three-thousand years. What is now a global city popular for it's fashion and design was once a small village on top of a hill that through a series of fascinating military and political events became center of a vast empire and location of Catholicism's founding. Plenty of historical locations have been destroyed following Italy's rise of Fascism and role in the second World War, but the historic center (Unesco World Heritage) remains as if untouched by time. Itinerary Having limited time, we spent all our time exploring the historical center of Rome and the Vatican. We spent three nights in a lovely bed & breakfast around 15 minutes away from the historical center; which was, due being located in what might might be perceived as a shabby neighborhood, extremely well priced. Day 1 We spent our first day strolling around the city's historical center and experiencing some of the more popular destinations such as the famous Spanish Steps square and Trevi Fountain . As fans of English Romanticism (including poets as Byron , Shelly and Keats we spent a bit of time in what is now a museum dedicated to Keats: the Keats-Shelley Memorial House museum . We concluded our evening having the most classic dinner one might expect of tourists in Italy: pizza and tiramissu (no regrets). Day 2 Our second day was mostly centered around Rome's ancient Colosseum , Palatine Hill and the massive Roman Forum . Beautiful ruins rich in history made this area an unforgettable experience. It turns out this area is popular amongst conservatory students as well. Excellent, sunny weather (in the middle of winter!) and a classical violist nearby reciting Vivaldi made for a lovely picnic~. Later in the evening we visited the Pantheon , a basilica dating back to 27 BCE, and architecturally quite impressive. Day 3 Our third day was mostly focused on the Borghese Gardens (we ended up renting a rowing boat!) and its excellent art museum including works of Bernini . Turns out ordering a chocolate milk in Rome will net you a solid chocomousse-like dessert. Cultural differences! Day 4 Our fourth and last day, a Sunday, was an excellent time to visit the Vatican City and gain a potential bar-bragging story by experiencing a Pope Francis speech live. The Sistine Chapel ‘s queues are legendary and despite being out of season, today was not an exception. We could've easily queued for 4 hours so we wisely decided on admiring St. Peter's Basilica instead. I'm not religious, but the general atmosphere in Vatican City was quite enjoyable and reminiscent of that often felt at music festivals: a small band was performing live music while an enormous crowd, including a surprising amount of young people, lined up to see the Pope's speech. Not speaking Italian or being religious made me feel slightly out of place however. Music When traveling, I make it a habit of finding new music by listening to local radio stations or asking recommendations to people I meet in hostels or bars - It's my own personal souvenir and sticks better in my mind than regular touristic souvenirs. This trip's band is Zu , an experimental band hailing from Rome and often defined as being a mix of dark jazz (whatever that might be~), math rock and noise. It's definitely not an easy listen, but highly atmospheric and I've grown surprisingly fond of it. The following song is a collaboration with fellow noise-jazz band Il Teatro Degli orrori . Conclusion Calling Rome a living, breathing open-air museum is certainly no overstatement. One might wander for hours through the historical center's alleys and side streets and still discover new beautiful sightings and hidden surprises. Italian food is famous across the world and rightly so. Restaurants in the historical center are generally considered tourist traps, but just a few minutes of extra walking took us to great, affordable destinations. Finally, and this is probably accountable for traveling out of season but despite being such a popular destination we've hardly ever felt overcrowded or had shady figures approaching us at touristic viewpoints, as we've experienced in places as London or Paris. All in all, Rome ended up a perfect romantic city-trip destination and highly affordable even for the poor students amongst us! We spent four days, which felt fine enough, but we could've easily stayed longer and still discover plenty more. Gallery Rome Skyline by Stevie Poppe ( https://flic.kr/p/LeGNpM - CC BY-SA 2.0) ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2016/08/travels-romance-in-rome/","loc":"https://steviepoppe.net/blog/2016/08/travels-romance-in-rome/"},{"title":"Pelican Plugin: CSS-only footnote pop-up","text":"Footnotes are a classic staple in any writer's toolbox, allowing them to, in an unobtrusive way, cite sources or display additional information not directly related to the original argument. Nevertheless it is my impression that online footnotes, through the way they are often implemented imitating printed media footnotes, come over quite counterintuitive and force the reader to disrupt his reading flow,failing to make use of today's extensive web design possibilities. As I royally use footnotes in my markdown writing work-flow 1 I figured I'd get acquainted with the Pelican documentation a bit and write a small unobtrusive CSS pop-up plugin to display these on hover for those devices that support it. 2 My solution is pretty rough and based on the way Python-Markdown parses markdown into HTML. As it's a quick ‘n dirty workaround untested with other parsers, I have no intention to further maintain this code so feel free to use it however you'd like. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def content_object_init ( instance ): if instance . _content is not None : content = instance . _content soup = BeautifulSoup ( content , 'html.parser' ) if 'sup' in content : footnotes = soup . find ( class_ = \"footnote\" ) . find_all ( 'p' ) footnoteref = soup . find_all ( class_ = \"footnote-ref\" ) for index , item in enumerate ( footnotes ): footnoteref [ index ] . parent [ 'class' ] = 'popup_footnote' tag = soup . new_tag ( 'span' ) tag . append ( BeautifulSoup ( item . decode_contents (), 'html.parser' )) footnoteref [ index ] . insert_after ( tag ) instance . _content = soup . decode () def register (): signals . content_object_init . connect ( content_object_init ) For simplicity's sake I use the BeautifulSoup library to locate all footnote references and their respective footnotes. Then I loop through them and copy the contents of the footnote in a ‘popup_footnote' span I append to the footnote reference container. Styling is done in CSS. Below is my markup. I use left: 50%; transform: translate(-50%, 0); to center the absolute-positioned pop-up over the footnote reference. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 sup . popup_footnote span { text-align : justify ; z-index : 10 ; display : none ; padding : 5 px ; line-height : 16 px ; opacity : 0.9 ; border-radius : 4 px ; box-shadow : 5 px 5 px 8 px #D4D4D4 ; top : 10 px ; left : 50 % ; transform : translate ( -50 % , 0 ); position : absolute ; width : 250 px ; } sup . popup_footnote : hover span { display : inline ; color : #111 ; border : 1 px solid #eaeaea ; background-color : #fffcfc ; } The downside to this CSS-only implementation (next to having double content in your markup) is that it fails to respond well to responsive design. Fixed-size pop-ups relative to the footnote pointer could result in a potential screen overflow. If this is an issue, the only alternative is using jquery and calculate the position of your reference relative to the window border and use this to calculate an ideal pop-up location. Of course if you're relying on the jquery library either way you might as well use that to copy the footnote on hover instead of above's solution. For such an example, I recommend ignorethecode.net's solution . The difficulty writing web-content with footnotes is distinguishing footnotes or direct links. I generally draw the line between source citation as footnote and useful information as direct link. ↩ Using :active to substitute the lack of proper hover on touch devices wouldn't be effective as these pop-ups display over anchors. A dirty solution would be to use media tags and keep display:none for mobile device widths, but with the blurring of lines between touch and mouse input devices lately this is not foolproof. ↩","tags":"Technical","url":"https://steviepoppe.net/blog/2016/08/pelican-plugin-css-only-footnote-pop-up/","loc":"https://steviepoppe.net/blog/2016/08/pelican-plugin-css-only-footnote-pop-up/"},{"title":"Travels: roadtrip through the Deep South","text":"Earlier this year, me and a couple of friends decided on a summer road-trip throughout the so-called Deep South , the most southern states of the USA. As we have a shared history of playing in bands and writing music together, my friends and I thought it would be cool to purchase some second-hand instruments and travel through some of the more musically well-known states. This short blog covers our three-week itinerary, with plenty of pictures to go along. Brief history The Deep South refers to the seven slave states that originally joined forces in what became known as the Confederate States of America. The historical dependence on slave labor (as well as the proximity to Cuba and Mexico) means that the region has both a large present-day ethnic minority of African-Americans and Hispanic Americans (and a more apparent cultural mix). Therefore, parts of this particular region are also informally called Black Belt (as well as Bible Belt). Simultaneously the region politically tends to be more conservative-leaning; with historic revisionism in regards to the region's role in, and dependence on, trans-Atlantic slavery often neglected in favor of ‘State rights' rhetoric. Itinerary We rented a relatively cheap car that would still be large enough to handle all five of us and our gear. Size-wise The car looked like a limousine, yet paled in comparison to the cars driving around us; among our first cultural shocks right there. As most of us are still students, we left straight after our finals end of June. The summertime in the South is described as being relentlessly tough, so we made sure to have proper air-conditioning and plenty of water available. Florida Miami After a fairly pleasant eight-hour flight, we arrived at Miami International Airport in Florida. Despite arriving late in the evening, it still took us over a whopping three hours of queuing just to get through border control. It was a Kafkaesque experience for sure; having to queue to use check-in machines and manual check-ups serving the exact same task, yet those manual check-ups were so superficial that it felt entirely unnecessary and a waste of time. An antiqued relic of post-911 measures? On our way out we passed about five fast-food chains within a minute of driving, as well as several drive-through gun shops and pharmacies, and big billboards advertising triple-bypasses. What a country. Some other memorable locations to us as foreigners around here were a Waffle House and an enormous Walmart. Everglades We spent some time exploring the everglades, a natural region of tropical wetlands with a large variety of flora and fauna. After passing an alligator crossing the road, we decided to stay in our car for most of the time here. Key West The Key West is the most southern island of the Florida Keys as well as the most southern city in the USA. The area has strong Cuban influence and is generally known as a tropic nightlife paradise. We spent some time at one of the beaches but found the area too crowded to our liking. People were pleasant and talkative enough around here, although the constant witty banter and one-liners (what we had thus-far only experiences in US television shows) became tiring rather quickly. Panama City Beach After a long, intense drive, we halted in panama City Beach and spent the afternoon and evening in a natural park, playing a bit on our instruments and having some drinks. Only at this point did we fully realize cities in the USA often don't have city centers similar to the German city-model popular in Europe. Thus we could never just park our car somewhere and spend the day just walking around or travel by public transport. With the upcoming presidential elections most people we met were incredibly keen on talking politics. In this natural park, we met an elderly man who (after inviting us for a pick-nick with his family) tried to convince us that President Obama was a actually a radical leftist Muslim destroying the United States with his socialist plans and support for Muslim terrorists. The kind of politics you might end up taking for granted when relying on private-owned, heavily sponsored news channels such as Fox News too much. It was for that exact reason, actually, that he would support Trump in the upcoming elections (notwithstanding that isn't actually running). It was a mixed experience; while the family was open and kind (to all colors and races, as he was keen on highlighting by pointing out his daughter's marriage to a Black man, both present at the pick-nick), it was clear that the lack of self-confidence in their Southern identity (these regions are often the butt of the joke) might have further marginalized and radicalized them politically. Alabama After a long drive through the scenic beauty of Alabama, we spent a short night in major port city Mobile . I particularly enjoyed the thick accents spoken here, reminding me heavily of the 1994 movie Forrest Gump , which takes place around this area. Louisiana Louisiana was one of the more interesting states we visited, with a strong Cajun and Creole influence present in both cuisine and culture. It is unfortunate that after eleven years, the scars caused by the notorious Hurricane Katrina are very much visible and unresolved. A blight on a nation that takes pride in its wealth and strong economy. Lafayette Lafayette is a pretty small town with a lively outgoing scene. After some detours we ended up in a local bar filled with burly, tattooed bikers watching several Americana performers. This was a really cool, unplanned experience and the whole thing really felt like a scene straight out of popular Southern Vampire series True Blood. We befriended a young couple, both fresh college graduates and strong Bernie supports. It was a lovely and enriching experience, albeit that my worries on the marginalization of this region were further confirmed through the somewhat condescending tone used in regards to the US south as a whole. New Orleans New Orleans is popularly referred to as the birthplace of jazz music in the USA and thus couldn't possibly be left out of our itinerary. We checked-in close to the famous French Quarter and spent our first night in bars around Bourbon Street . As first impressions go, that one was actually a bit of a downer. Bourbon Street reminded me of a sleazy red-light district overrun by affluent, middle-age tourists, and the smell was unbearably nauseating, reminding us of a mixture of sewer and cannabis. In hindsight this should have made sense; historically parts of the French Quarter did serve as a red-light district, after all, and was popular for drinking, gambling and brothels. The next evenings, however, were spent in nearby streets instead and surely ended up more memorable; having been treated to impromptu jazz concerts and street hip-hop free-styling. New Orleans, as most of Louisiana, is often depicted as having a strong Voodoo culture amongst its creole population (descendants of French or Spanish colonial settlers, often with African or Native American ancestry as well). One of the famous practitioners, Marie Laveau, is buried in New Orleans; her grave becoming a famous tourist attraction. Frankly, the idea of a voodoo culture felt extremely commodified and exploitative; and someone we met of Creole descent did in fact argue that due to gentrification, very little of actual Creole voodoo culture remains. New Orleans has plenty of swamps and bayous and a boat-tour through them was quite lovely. Texas Texas, the second largest state in the USA and bordered by Mexico, has a long and politically colorful history culminating in the narrative of being strong-willed and independent. After days spent driving on gray, asphalt roads, the rough, desert-like setting was quite a change. We thoroughly enjoyed our in this state and extended our stay by almost a week. Houston On our way to the state capital, we passed through Texas' largest city and home of NASA's Mission Control Center . A visit was definitely compulsory. Around this exact time several violent protests broke out following the shooting of an unarmed black man by a police man in Houston, followed later by the shooting of several police officers. Even in our home country we often hear reports of police brutality and racial conflicts in the USA, but actually experiencing the tense atmosphere and intense sadness following these events is an entirely other thing. Austin Having heard that Austin is within Texas known for being a more liberal and alternative city with a large college student population, we decided on spending the fourth of July and following weekend there. Something bizarre: the majority of the bars around there had Jenga games on their terraces. On a particular night, me and two others were on a roll and built a rather large tower, to the point some bystanders started filming and taking pictures. Several time that evening we were approached as \"Oh dude you're the Jenga boys, right?\" - Which was frankly not the reputation we were hoping for, if anything. We befriended a local guy of Mexican descent and had a few more drinks at our place afterwards. Again the topic shifted towards politics; with his strong libertarian mindset ultimately opting to vote for Trump in hopes that it would increase state independence. Wonder how that will play out. One of our best meals was spent at a family-style barbecue restaurant nearby Texas, The Salt Lick BBQ, which we had already heard about before our trip after it went viral on the internet for their specific style of cooking. Another good memory food-wise was a Korean BBQ in the center of Austin. Not exactly very American, but I finally understand the hype. Big Bend National Park Despite apparently being relatively unknown, Big Bend National Park turned out to be the most breathtaking place we've visited in the US. Lying directly on the border of Mexico, the area has a distinct desert-like environment with appropriate flora and fauna. We often passed snakes, lizards and scorpions, as well various spiders and insects I'd rather avoid altogether. Nashville, Tennessee Nashville, the capital of Tennessee is well known as the center of the country music industry in the USA and saw the birth of famous musicians such as Johnny Cash, Willie Nelson and Roy Orbison (and in more recent memory Miley Cyrus). We spent some time on Broadway Street , popular for bar hopping (or \"honky-tonking\") along various country-music venues. It's a pretty touristic place (and we actually passed the notorious far-right Dutch politician Geert Wilders). Some locals drove us to 5-point , the local outgoing district with a vibrant nightlife. Two guys we acquainted at one bar again ended up preaching the necessity of gun-rights and in particular the right to open or concealed carrying. For sure another heavy culture shock right there, given that we have very strict gun laws in Belgium, but they took us for a few rounds at a nearby shooting range the next day, which was an interesting, unplanned experience. Music When traveling, I make it a habit of finding new music by listening to local radio stations or asking recommendations to people I meet in hostels or bars - It's my own personal souvenir and sticks better in my mind than regular touristic souvenirs. This trip's pick is the Nashville-born Hank Williams III , a multi-instrumentalist and grandson of famous honky-tonk / country musician Hank Williams. He successfully blends his country and honky-tonk roots with punk-rock and metal, and we've often played his albums on our trip. Conclusion A first surprise was the enormous influence of American pop-culture and soft-power on our daily lives: upon arrival I felt this sense of nostalgia despite having never visited the place before. Yet even more surprising was the spatial planning in these states: we've had occasions of staying at a certain hotel with a particular lay-out and a range of fixed fast-food chains nearby; getting up at 7AM and driving over 14 hours across concrete roads, only to arrive at the exact same hotel with the exact same fast-food chains nearby again. Genuinely felt like parts were constructed through the usage of copy-paste in a large Sims game. Public transport, especially interstate-wise, is incredibly lacking. The young couple we talked to told us the car-industry successful lobbied against Obama's plan to introduce a European-style train-network. Furthermore, the outrageous amount of advertisements and national flags as signifiers of national identity visible everywhere we went would have been outright bizarre if we were not yet aware of rather nationalistic tendencies in US history and education. Nevertheless it was an enjoyable trip, although not without a variety of culture shocks. We achieved our goals of playing music on the road, and most of our evenings were spent socializing with locals or listening to local bands perform. People were generally incredibly warm and interested and often invited us over for dinner or drinks. Speaking of which, I probably gained over ten pounds eating hamburgers, BBQ and various kinds of fried food. Wonder how I'd look if I had a longer stay. Gallery Desert by Stevie Poppe ( https://flic.kr/p/MbXtHF - CC BY-SA 2.0) ↩","tags":"Personal","url":"https://steviepoppe.net/blog/2016/08/travels-roadtrip-through-the-deep-south/","loc":"https://steviepoppe.net/blog/2016/08/travels-roadtrip-through-the-deep-south/"},{"title":"Hello World - About This Blog","text":"I had been thinking on starting a small, personal blog for a while now and was already in the progress of writing up a small stack of possible articles to post. A long summer-break permitted me to sit down and get started on the technicalities; a process a bit more complicated as initially expected but worth it in the long run. This blog serves not as a tutorial on making one with Pelican (I'll get to that eventually ), but as a brief log on my reasoning and thought process during creation. Blog-platform Deciding Conceptually I tried to steer away from heavy content management systems such as Drupal and WordPress . While easy to work with (and taking quite a bit of technical work of your hands), a server-side database means too heavy a load for what's basically a small-scale personal blog, and several unnecessary security risks on top. 2 The tinkerer in me opted for a DIY approach with full control over both content and design. As far as Static blog frameworks go there were several popular options, but since I'm more accustomed to the Python programming language I ended up choosing for Pelican over more widespread options such as the Ruby -based Jekyll . An added value of managing my blogs client-side and delivering small sized static pages is the ease of finding an appropriate host (I host my pages on GitHub Pages while serving larger files through DropBox) as well as seamless integration with my Markdown -based writing work-flow. 3 Set-up The initial set-up is relatively simple, as installation and basic generation are done through command line, and content written in Markdown , AsciiDoc or reStructuredText 4 gets served as stand-alone pages or articles. There's a wide choice of existing templates , but further customization requires some working knowledge on web-design as well as, preferably, on basic programming concepts (Pelican templates are created using the python-based Jinja2 , allowing for template logic and inheritance). Needless to say, hosting on GitHub Pages requires rudementary knowledge of Git , but that's really not a skill learned in vain as the concept of version control applies well to writing papers and other documents too. Development Getting acquainted with these tools can be time consuming. Luckily Pelican's well documented , and there are plenty of tutorials out there, as well as hundreds of working examples hosted on GitHub available as reference (the backbone of my template is a heavily modified Plumage theme). I prefer a minimalistic layout easy on the eye, and steered clear of unnecessary clutter, but with the advent of visual blogging 5 I figured I'd integrate some graphical elements, such as article banners (that's the extent of it really, I'm not much of an artist), as visual content engages the reader more easily. One easily overlooked design problem is ensuring responsive web-pages for the ever increasing mobile internet traffic 6 , but with easy support for Sass , or just using Twitter's Bootstrap framework, this comes pretty naturally. Finally, implementing additional features is pretty easy too. There's a wide range of existing plugins for features like searching, minifying assets, table of contents and comments (using the third-party Disqus blog hosting service). Hosting GitHub has allowed free hosting for static personal or project-sites on GitHub Pages for a while now. This boils down to creating a new repository with your user-name and GitHub url ( steviepoppe.github.io ) and hosting your pages there. I created an additional source branch to host all my source-files, use ghp-import to place my output files in a separate branch, and force push that to the master branch on GH Pages. 7 1 2 #!python pelican content -o output -s publishconf.py && ghp-import output && git push -f origin gh-pages:master As of writing, GitHub's SSL certificate only covers *.github.io domains; there isn't any support yet for custom domains. Thus as final step after having set up my custom domain, and as an additional security layer, I use CloudFare to secure part of the connection and enroll some further optimizations. Conclusion While further customization requires some technical experience (or at least the proper mindset), I believe the base set-up is easy enough for anyone to learn and allows for more control over your own content and a smoother work-flow as you would have relying on big CMS systems. Any extra skills learned will definitely come in use as digital publishing, web-design and working with Git or other forms of version control become more and more basic skills anyone should possess. In retrospect, Jekyll appears to be an easier option for beginners with it's widespread usage, but both Jekyll and Pelican achieve the same thing, so this is personal choice really. I'll end up writing a full step-by-step beginner's guide on setting up Pelican and hosting on GitHub Pages eventually, but for now I'll post several decent tutorials below. Further reading http://arunrocks.com/moving-blogs-to-pelican/ http://cyrille.rossant.net/pelican-github/ http://duncanlock.net/blog/2013/05/17/how-i-built-this-website-using-pelican-part-1-setup/ http://guizishanren.com/guide-to-set-up-github-page-and-pelican/ Icelandic Sunset by Stevie Poppe ( https://flic.kr/p/M83mKT - CC BY-SA 2.0) ↩ Burnett, Brett. 2015. \"Why Did We Migrate from Drupal to Jekyll?\" Text. The BHW Group. https://thebhwgroup.com/blog/jekyll-drupal-wordpress ↩ O'Nolan, John. 2015. \"The Ultimate Guide to Writing & Publishing with Markdown\". Ghost. https://blog.ghost.org/markdown/ ↩ All three are open, lightweight markup languages with a heavy focus on readability through separation of content and layout layers. ↩ Fanguy, Will. 2016. \"The Amazing Evolution of Visual Storytelling: Blogging, Instagram, Snapchat, and the Future\". Business 2 Community. http://www.business2community.com/trends-news/amazing-evolution-visual-storytelling-blogging-instagram-snapchat-future-01557833 ↩ ‘ the amount of mobile traffic now accounts for more than half of total internet traffic ‘ \"Cisco Visual Networking Index: Global Mobile Data Traffic Forecast Update, 2015–2020 White Paper\". 2016. Cisco. http://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/mobile-white-paper-c11-520862.html ↩ I'm doing this manually through command line for now, but there's several automation options including using Travis-CI for the more hardcore blogger. ↩","tags":"technical","url":"https://steviepoppe.net/blog/2016/08/hello-world-about-this-blog/","loc":"https://steviepoppe.net/blog/2016/08/hello-world-about-this-blog/"}]};